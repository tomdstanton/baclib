{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"baclib","text":"<p>[!WARNING] \ud83d\udea7 This package is currently under construction, proceed with caution \ud83d\udea7</p> <p>High-Performance Python Library for Bacterial Genomics</p> <p><code>baclib</code> is a modern, Numba-accelerated library designed for high-throughput bacterial genomics. Unlike traditional bioinformatics libraries that prioritize flexibility over speed, <code>baclib</code> focuses on raw performance and memory efficiency. It utilizes Structure-of-Arrays (SoA) layouts, binary string processing, and JIT compilation to handle large-scale genomic data structures, alignments, and assembly graphs efficiently.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Binary-First I/O: All file parsing (FASTA, FASTQ, GenBank, GFA, PAF, GFF3) is performed in binary mode. This avoids costly string encoding/decoding overhead and minimizes memory usage.</li> <li>Numba-Accelerated Kernels: Core algorithms for alignment, seeding (Minimizers, Syncmers), and interval arithmetic are JIT-compiled for C-like performance.</li> <li>Vectorized Containers:<ul> <li><code>SeqBatch</code>: Stores thousands of sequences in contiguous memory for SIMD-friendly processing.</li> <li><code>AlignmentBatch</code>: Manages millions of alignment records with NumPy-backed storage.</li> <li><code>IntervalIndex</code>: Fast overlap queries and set operations for genomic features.</li> </ul> </li> <li>Alignment Engine:<ul> <li>Built-in pairwise aligner (Smith-Waterman, Needleman-Wunsch) with affine gap penalties.</li> <li>Fast K-mer indexing (MinHash, Minimizers) for rapid sequence comparison.</li> <li>Seamless integration with Minimap2 via direct process streaming.</li> </ul> </li> <li>Assembly Graph Toolkit: Native support for GFA graphs, including pathfinding, simplification, and topological analysis uses efficient <code>RecordBatch</code> backends.</li> <li>API Clients: Includes a client for the PRODORIC database (<code>ProdoricClient</code>) for retrieving transcriptional regulation data.</li> </ul>"},{"location":"#important-binary-strings","title":"\u26a0\ufe0f Important: Binary Strings","text":"<p>To achieve maximum performance, <code>baclib</code> operates almost exclusively with bytes (<code>b'string'</code>) rather than Python unicode strings (<code>'string'</code>).</p> <ul> <li>Identifiers: <code>record.id</code>, <code>feature.kind</code>, and dictionary keys are <code>bytes</code>.</li> <li>Sequences: DNA/Protein data is stored as <code>uint8</code> numpy arrays but converts to <code>bytes</code>.</li> <li>I/O: Readers expect and return binary data.</li> </ul> <p>Example:</p> <pre><code># Correct\nrecord_id = b\"contig_1\"\nfeature_type = b\"CDS\"\n\n# Incorrect (will likely fail lookups or comparisons)\nrecord_id = \"contig_1\"\n</code></pre> <p>This design choice ensures zero-copy compatibility with low-level parsers and external tools.</p>"},{"location":"#installation","title":"Installation","text":"<p>Requires Python 3.11+.</p> <pre><code>python -m pip install baclib\n</code></pre> <p>For documentation generation support:</p> <pre><code>python -m pip install baclib --group docs\n</code></pre> <p>For developers:</p> <pre><code>python -m pip install baclib -e --group dev\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#reading-sequences","title":"Reading Sequences","text":"<pre><code>from baclib.io import SeqFile\n\n# Read a FASTA file (gzip supported automatically)\n# Use SeqFile.open() as the unified entry point\nwith SeqFile.open(\"genome.fasta.gz\") as reader:\n    for record in reader:\n        # record.id is bytes!\n        print(f\"ID: {record.id.decode()}, Length: {len(record)}\")\n        print(f\"Sequence: {record.seq[:50]}...\")\n\n# Read a GenBank file with features\nwith SeqFile.open(\"annotation.gbk\") as reader:\n    for record in reader:\n        for feature in record.features:\n            if feature.key == b'CDS':\n                # Access qualifiers using bytes keys\n                gene = feature.get(b'gene')\n                print(f\"Gene: {gene}\")\n</code></pre>"},{"location":"#sequence-manipulation","title":"Sequence Manipulation","text":"<pre><code>from baclib.containers.seq import Alphabet, GeneticCode\n\ndna = Alphabet.DNA\n\n# Create a sequence from string\nseq = dna.seq_from(\"ATGCGTAGCTAG\")\n\n# Or generate a random sequence\nseq = dna.random_seq(length=100)\n\n# Reverse complement\nrc_seq = dna.reverse_complement(seq)\nprint(rc_seq)\n\n# Translate to protein using Bacterial code (Table 11)\ngc = GeneticCode.from_code(11)\nprotein = gc.translate(seq)\nprint(protein)\n</code></pre>"},{"location":"#pairwise-alignment","title":"Pairwise Alignment","text":"<p>Perform local, global, or glocal alignment using the built-in high-performance aligner.</p> <pre><code>from baclib.engines.pairwise import Aligner\nfrom baclib.containers.seq import Alphabet, SeqBatch\n\ndna = Alphabet.DNA\n\n# Create random sequences\ntargets = dna.random_batch(n_seqs=5, length=1000)\nqueries = dna.random_batch(n_seqs=2, length=100)\n\n# Initialize aligner (Glocal mode: Global in Query, Local in Target)\naligner = Aligner(mode='glocal', compute_traceback=True)\n\n# Build index on targets\naligner.build(targets)\n\n# Map queries to targets\nhits = aligner.map(queries, min_score=50)\n\nfor hit in hits:\n    print(f\"Query {hit.query} maps to Target {hit.target}\")\n    print(f\"Score: {hit.score}, CIGAR: {hit.cigar}\")\n</code></pre>"},{"location":"#using-minimap2","title":"Using Minimap2","text":"<p>Ensure <code>minimap2</code> is installed and in your PATH.</p> <pre><code>from baclib.lib.external import Minimap2\nfrom baclib.containers.record import RecordBatch\nfrom baclib.containers.seq import Alphabet\n\ndna = Alphabet.DNA\nref = RecordBatch.random(dna, n_seqs=100, length=1000)\nqueries = RecordBatch.random(dna, n_seqs=10, length=100)\n\n# Align using Minimap2 wrapper (handles indexing automatically)\nwith Minimap2(ref) as mapper:\n    for alignment in mapper.align(queries):\n        print(f\"Query: {alignment.queries} -&gt; Target: {alignment.target}\")\n        print(f\"Matches: {alignment.n_matches}\")\n</code></pre>"},{"location":"#genome-assembly-graph","title":"Genome Assembly Graph","text":"<p>Load and analyze assembly graphs (GFA format).</p> <pre><code>from baclib.containers.genome import GenomeAssembly\n\n# Load GFA graph\nassembly = GenomeAssembly.from_file(\"assembly.gfa\")\n\nprint(f\"Edges: {assembly.edges}\")\n</code></pre>"},{"location":"#prodoric-client","title":"PRODORIC Client","text":"<p>Fetch regulatory data directly from the PRODORIC database.</p> <pre><code>from baclib.apis.prodoric import ProdoricClient\n\nclient = ProdoricClient()\n\n# Search for motifs\nmotifs = client.search_motifs(\"LexA\")\nfor m in motifs:\n    print(f\"Found motif: {m.name} ({m.accession})\")\n\n# Get motif details\nmotif = client.get_motif(motifs[0].accession)\nprint(motif.name)\n</code></pre>"},{"location":"#dependencies","title":"Dependencies","text":"<ul> <li><code>numpy</code></li> <li><code>scipy</code></li> <li><code>numba</code> (Optional, but highly recommended for performance)</li> </ul>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the terms of the license found in the <code>LICENSE</code> file.</p>"},{"location":"reference/baclib/","title":"baclib","text":""},{"location":"reference/baclib/#baclib","title":"<code>baclib</code>","text":"<p>A Python library for bacterial genomics: sequence analysis, alignment, and file I/O.</p>"},{"location":"reference/baclib/apis/","title":"apis","text":""},{"location":"reference/baclib/apis/#baclib.apis","title":"<code>baclib.apis</code>","text":"<p>REST API client base class with rate limiting, retries, and parallel request support.</p>"},{"location":"reference/baclib/apis/#baclib.apis.ApiClient","title":"<code>ApiClient</code>","text":"<p>Base class for REST API clients. Handles authentication, retries, rate limiting, and threading. Supports persistent connections (Keep-Alive) when used as a context manager.</p> Source code in <code>baclib/apis/__init__.py</code> <pre><code>class ApiClient:\n    \"\"\"\n    Base class for REST API clients.\n    Handles authentication, retries, rate limiting, and threading.\n    Supports persistent connections (Keep-Alive) when used as a context manager.\n    \"\"\"\n    def __init__(self, base_url: str, api_key: Optional[str] = None, \n                 requests_per_second: float = 3.0, \n                 max_retries: int = 3):\n        self.base_url = base_url.rstrip('/')\n        self._api_key = api_key\n        self._rate_limit_delay = 1.0 / requests_per_second if requests_per_second &gt; 0 else 0\n        self._last_request_time = 0.0\n        self._max_retries = max_retries\n        self._lock = threading.Lock()\n\n        # Persistence state\n        self._keep_alive_active = False\n        self._thread_local = threading.local()\n\n        # Parse base URL for http.client\n        parsed = urllib.parse.urlparse(self.base_url)\n        self._host = parsed.hostname\n        self._port = parsed.port or (443 if parsed.scheme == 'https' else 80)\n        self._scheme = parsed.scheme\n        self._api_root = parsed.path # e.g. /api\n\n    def _get_headers(self) -&gt; Dict[str, str]:\n        headers = {'User-Agent': 'baclib-client/0.1'}\n        if self._api_key:\n            headers['api-key'] = self._api_key\n        return headers\n\n    def _wait_for_rate_limit(self):\n        if self._rate_limit_delay &lt;= 0: return\n\n        with self._lock:\n            now = time.time()\n            next_allowed = self._last_request_time + self._rate_limit_delay\n            wait = next_allowed - now\n\n            if wait &gt; 0:\n                time.sleep(wait)\n                self._last_request_time = time.time()\n            else:\n                self._last_request_time = now\n\n    def run_parallel(self, func: Callable, items: Iterable, max_workers: int = 4) -&gt; List[Any]:\n        \"\"\"\n        executes `func` for each item in `items` in parallel using threads.\n        `func` should accept a single argument (item).\n        \"\"\"\n        results = []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            future_to_item = {executor.submit(func, item): item for item in items}\n            for future in concurrent.futures.as_completed(future_to_item):\n                try:\n                    results.append(future.result())\n                except Exception as e:\n                    # In a real app we might want to capture errors instead of raising\n                    raise e\n        return results\n\n    def _get_connection(self) -&gt; Union[http.client.HTTPSConnection, http.client.HTTPConnection]:\n        \"\"\"Returns a thread-local connection, creating it if necessary.\"\"\"\n        if not hasattr(self._thread_local, 'conn'):\n            context = ssl.create_default_context() if self._scheme == 'https' else None\n            if self._scheme == 'https':\n                self._thread_local.conn = http.client.HTTPSConnection(self._host, self._port, context=context)\n            else:\n                self._thread_local.conn = http.client.HTTPConnection(self._host, self._port)\n        return self._thread_local.conn\n\n    def _close_conn_for_thread(self):\n        \"\"\"Closes the connection for the current thread if it exists.\"\"\"\n        if hasattr(self._thread_local, 'conn'):\n            self._thread_local.conn.close()\n            del self._thread_local.conn\n\n    def request(self, method: str, path: str, params: dict = None, json_data: dict = None, stream: bool = False) -&gt; Any:\n        \"\"\"\n        Performs an HTTP request with rate limiting and error handling.\n        Uses persistent connection if available (inside context manager), otherwise urllib.\n        \"\"\"\n        if self._keep_alive_active:\n            return self._request_persistent(method, path, params, json_data, stream)\n        else:\n            return self._request_urllib(method, path, params, json_data, stream)\n\n    def _request_persistent(self, method: str, path: str, params: dict, json_data: dict, stream: bool) -&gt; Any:\n        # Construct path\n        full_path = f\"{self._api_root}/{path.lstrip('/')}\"\n        if params:\n            params = {k: v for k, v in params.items() if v is not None}\n            query_string = urllib.parse.urlencode(params, doseq=True)\n            full_path = f\"{full_path}?{query_string}\"\n\n        headers = self._get_headers()\n        body = None\n        if json_data:\n            headers['Content-Type'] = 'application/json'\n            body = json.dumps(json_data).encode('utf-8')\n\n        for attempt in range(self._max_retries + 1):\n            self._wait_for_rate_limit()\n            conn = self._get_connection()\n\n            try:\n                conn.request(method, full_path, body=body, headers=headers)\n                response = conn.getresponse()\n\n                # Check for errors status codes\n                if response.status &gt;= 400:\n                    # Raise urllib.error.HTTPError to match existing interface\n                    # We need: url, code, msg, hdrs, fp\n                    raise urllib.error.HTTPError(\n                        url=self.base_url + full_path,\n                        code=response.status,\n                        msg=response.reason,\n                        hdrs=response.headers,\n                        fp=None\n                    )\n\n                if stream:\n                    # For stream, we return the http.client.HTTPResponse object\n                    # It behaves like a file object (read, etc.)\n                    # IMPORTANT: The connection cannot be reused until this is fully read or closed.\n                    return response\n\n                data = response.read()\n                if not data: return None\n                return json.loads(data)\n\n            except (http.client.CannotSendRequest, http.client.ResponseNotReady, BrokenPipeError, ConnectionError):\n                # Connection might have dropped\n                self._close_conn_for_thread()\n                if attempt &lt; self._max_retries:\n                    # Retry with new connection\n                    continue\n                raise\n            except urllib.error.HTTPError as e:\n                # Handle retries for server errors\n                if e.code in (429, 500, 502, 503, 504) and attempt &lt; self._max_retries:\n                    time.sleep(2 ** attempt)\n                    continue\n                raise\n\n    def _request_urllib(self, method: str, path: str, params: dict, json_data: dict, stream: bool) -&gt; Any:\n        \"\"\"Original implementation using urllib.\"\"\"\n        url = f\"{self.base_url}/{path.lstrip('/')}\"\n\n        if params:\n            params = {k: v for k, v in params.items() if v is not None}\n            query_string = urllib.parse.urlencode(params, doseq=True)\n            url = f\"{url}?{query_string}\"\n\n        headers = self._get_headers()\n        data = None\n        if json_data:\n            headers['Content-Type'] = 'application/json'\n            data = json.dumps(json_data).encode('utf-8')\n\n        req = urllib.request.Request(url, data=data, headers=headers, method=method)\n\n        for attempt in range(self._max_retries + 1):\n            self._wait_for_rate_limit()\n            try:\n                response = urllib.request.urlopen(req)\n                if stream:\n                    return response\n\n                content = response.read()\n                if not content: return None\n                return json.loads(content)\n\n            except urllib.error.HTTPError as e:\n                if e.code in (429, 500, 502, 503, 504) and attempt &lt; self._max_retries:\n                    time.sleep(2 ** attempt)\n                    continue\n                else:\n                    raise\n\n            except urllib.error.URLError as e:\n                 if attempt &lt; self._max_retries:\n                    time.sleep(2 ** attempt)\n                    continue\n                 raise e\n\n    def get(self, path: str, params: dict = None, **kwargs) -&gt; Any:\n        return self.request('GET', path, params=params, **kwargs)\n\n    def post(self, path: str, json: dict = None, **kwargs) -&gt; Any:\n        return self.request('POST', path, json_data=json, **kwargs)\n\n    def download(self, path: str, destination: Union[str, Path], params: dict = None, method: str = 'GET', json: dict = None):\n        \"\"\"\n        Downloads a file to the specified destination.\n        \"\"\"\n        dest_path = Path(destination)\n        dest_path.parent.mkdir(parents=True, exist_ok=True)\n\n        import shutil\n        response = self.request(method, path, params=params, json_data=json, stream=True)\n        try:\n             with open(dest_path, 'wb') as f:\n                 shutil.copyfileobj(response, f)\n        finally:\n             # If using persistent connection, we must ensure response is fully consumed/closed\n             if hasattr(response, 'close'):\n                 response.close()\n        return dest_path\n\n    def __enter__(self): \n        self._keep_alive_active = True\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb): \n        self._keep_alive_active = False\n        # Clean up the main thread's connection\n        self._close_conn_for_thread()\n\n    def close(self): \n        \"\"\"Explicit close.\"\"\"\n        self._close_conn_for_thread()\n</code></pre>"},{"location":"reference/baclib/apis/#baclib.apis.ApiClient.close","title":"<code>close()</code>","text":"<p>Explicit close.</p> Source code in <code>baclib/apis/__init__.py</code> <pre><code>def close(self): \n    \"\"\"Explicit close.\"\"\"\n    self._close_conn_for_thread()\n</code></pre>"},{"location":"reference/baclib/apis/#baclib.apis.ApiClient.download","title":"<code>download(path, destination, params=None, method='GET', json=None)</code>","text":"<p>Downloads a file to the specified destination.</p> Source code in <code>baclib/apis/__init__.py</code> <pre><code>def download(self, path: str, destination: Union[str, Path], params: dict = None, method: str = 'GET', json: dict = None):\n    \"\"\"\n    Downloads a file to the specified destination.\n    \"\"\"\n    dest_path = Path(destination)\n    dest_path.parent.mkdir(parents=True, exist_ok=True)\n\n    import shutil\n    response = self.request(method, path, params=params, json_data=json, stream=True)\n    try:\n         with open(dest_path, 'wb') as f:\n             shutil.copyfileobj(response, f)\n    finally:\n         # If using persistent connection, we must ensure response is fully consumed/closed\n         if hasattr(response, 'close'):\n             response.close()\n    return dest_path\n</code></pre>"},{"location":"reference/baclib/apis/#baclib.apis.ApiClient.request","title":"<code>request(method, path, params=None, json_data=None, stream=False)</code>","text":"<p>Performs an HTTP request with rate limiting and error handling. Uses persistent connection if available (inside context manager), otherwise urllib.</p> Source code in <code>baclib/apis/__init__.py</code> <pre><code>def request(self, method: str, path: str, params: dict = None, json_data: dict = None, stream: bool = False) -&gt; Any:\n    \"\"\"\n    Performs an HTTP request with rate limiting and error handling.\n    Uses persistent connection if available (inside context manager), otherwise urllib.\n    \"\"\"\n    if self._keep_alive_active:\n        return self._request_persistent(method, path, params, json_data, stream)\n    else:\n        return self._request_urllib(method, path, params, json_data, stream)\n</code></pre>"},{"location":"reference/baclib/apis/#baclib.apis.ApiClient.run_parallel","title":"<code>run_parallel(func, items, max_workers=4)</code>","text":"<p>executes <code>func</code> for each item in <code>items</code> in parallel using threads. <code>func</code> should accept a single argument (item).</p> Source code in <code>baclib/apis/__init__.py</code> <pre><code>def run_parallel(self, func: Callable, items: Iterable, max_workers: int = 4) -&gt; List[Any]:\n    \"\"\"\n    executes `func` for each item in `items` in parallel using threads.\n    `func` should accept a single argument (item).\n    \"\"\"\n    results = []\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        future_to_item = {executor.submit(func, item): item for item in items}\n        for future in concurrent.futures.as_completed(future_to_item):\n            try:\n                results.append(future.result())\n            except Exception as e:\n                # In a real app we might want to capture errors instead of raising\n                raise e\n    return results\n</code></pre>"},{"location":"reference/baclib/apis/#baclib.apis.Token","title":"<code>Token</code>","text":"<p>               Bases: <code>str</code></p> <p>Base class for API tokens/accessions.</p> <p>Inherits from str to allow direct usage in f-strings and URLs  while providing type safety.</p> Source code in <code>baclib/apis/__init__.py</code> <pre><code>class Token(str):\n    \"\"\"Base class for API tokens/accessions.\n\n    Inherits from str to allow direct usage in f-strings and URLs \n    while providing type safety.\n    \"\"\"\n    def __repr__(self):\n        return f\"{self.__class__.__name__}('{self}')\"\n</code></pre>"},{"location":"reference/baclib/apis/datasets/","title":"datasets","text":""},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets","title":"<code>baclib.apis.datasets</code>","text":"<p>Client for the NCBI Datasets API, supporting genome, gene, and virus data retrieval.</p>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetPackage","title":"<code>DatasetPackage</code>","text":"<p>Helper to access files within a downloaded NCBI Dataset ZIP package.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class DatasetPackage:\n    \"\"\"\n    Helper to access files within a downloaded NCBI Dataset ZIP package.\n    \"\"\"\n    def __init__(self, zip_path: Union[str, Path]):\n        self.zip_path = Path(zip_path)\n        if not self.zip_path.exists():\n            # If it's a temp file created by us, it should exist.\n            raise FileNotFoundError(f\"Package not found: {self.zip_path}\")\n\n    def __repr__(self):\n        return f\"DatasetPackage({self.zip_path})\"\n\n    def extract(self, extract_path: Union[str, Path] = None):\n        \"\"\"Extracts the entire package.\"\"\"\n        with zipfile.ZipFile(self.zip_path, 'r') as z:\n            z.extractall(extract_path)\n\n    def iter_files(self, pattern: str = \"*\") -&gt; Iterator[str]:\n        \"\"\"Yields filenames in the zip matching a pattern.\"\"\"\n        with zipfile.ZipFile(self.zip_path, 'r') as z:\n            for name in z.namelist():\n                if pattern == \"*\" or pattern in name: \n                     yield name\n\n    def sequences(self, fmt: Optional[Union[str, SeqFileFormat]] = None) -&gt; Iterator[Record]:\n        \"\"\"\n        Yields Record objects from files in the package that match supported sequence formats.\n\n        Args:\n            fmt: Optional format to filter by (e.g. SeqFileFormat.FASTA).\n                 If None, tries to detect format for every file in the registry.\n        \"\"\"\n        target_fmt = SeqFileFormat(fmt) if fmt else None\n\n        with zipfile.ZipFile(self.zip_path, 'r') as z:\n            for name in z.namelist():\n                # Skip directories\n                if name.endswith('/'): continue\n\n                detected_fmt = None\n\n                # Check against registry\n                # If target_fmt is provided, only check that.\n                # Else check all.\n\n                candidates = [target_fmt] if target_fmt else SeqFile._REGISTRY.keys()\n\n                for candidate in candidates:\n                    spec = SeqFile._REGISTRY.get(candidate)\n                    if spec and any(name.endswith(ext) for ext in spec.extensions):\n                        detected_fmt = candidate\n                        break\n\n                if detected_fmt:\n                    with z.open(name) as f:\n                         # SeqFile.open with a file-like object and explicit format\n                         reader = SeqFile.open(f, fmt=detected_fmt)\n                         for item in reader:\n                             if isinstance(item, Record):\n                                 yield item\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetPackage.extract","title":"<code>extract(extract_path=None)</code>","text":"<p>Extracts the entire package.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>def extract(self, extract_path: Union[str, Path] = None):\n    \"\"\"Extracts the entire package.\"\"\"\n    with zipfile.ZipFile(self.zip_path, 'r') as z:\n        z.extractall(extract_path)\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetPackage.iter_files","title":"<code>iter_files(pattern='*')</code>","text":"<p>Yields filenames in the zip matching a pattern.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>def iter_files(self, pattern: str = \"*\") -&gt; Iterator[str]:\n    \"\"\"Yields filenames in the zip matching a pattern.\"\"\"\n    with zipfile.ZipFile(self.zip_path, 'r') as z:\n        for name in z.namelist():\n            if pattern == \"*\" or pattern in name: \n                 yield name\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetPackage.sequences","title":"<code>sequences(fmt=None)</code>","text":"<p>Yields Record objects from files in the package that match supported sequence formats.</p> <p>Parameters:</p> Name Type Description Default <code>fmt</code> <code>Optional[Union[str, SeqFileFormat]]</code> <p>Optional format to filter by (e.g. SeqFileFormat.FASTA).  If None, tries to detect format for every file in the registry.</p> <code>None</code> Source code in <code>baclib/apis/datasets.py</code> <pre><code>def sequences(self, fmt: Optional[Union[str, SeqFileFormat]] = None) -&gt; Iterator[Record]:\n    \"\"\"\n    Yields Record objects from files in the package that match supported sequence formats.\n\n    Args:\n        fmt: Optional format to filter by (e.g. SeqFileFormat.FASTA).\n             If None, tries to detect format for every file in the registry.\n    \"\"\"\n    target_fmt = SeqFileFormat(fmt) if fmt else None\n\n    with zipfile.ZipFile(self.zip_path, 'r') as z:\n        for name in z.namelist():\n            # Skip directories\n            if name.endswith('/'): continue\n\n            detected_fmt = None\n\n            # Check against registry\n            # If target_fmt is provided, only check that.\n            # Else check all.\n\n            candidates = [target_fmt] if target_fmt else SeqFile._REGISTRY.keys()\n\n            for candidate in candidates:\n                spec = SeqFile._REGISTRY.get(candidate)\n                if spec and any(name.endswith(ext) for ext in spec.extensions):\n                    detected_fmt = candidate\n                    break\n\n            if detected_fmt:\n                with z.open(name) as f:\n                     # SeqFile.open with a file-like object and explicit format\n                     reader = SeqFile.open(f, fmt=detected_fmt)\n                     for item in reader:\n                         if isinstance(item, Record):\n                             yield item\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetsClient","title":"<code>DatasetsClient</code>","text":"<p>               Bases: <code>ApiClient</code></p> <p>Client for NCBI Datasets v2 API.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class DatasetsClient(ApiClient):\n    \"\"\"\n    Client for NCBI Datasets v2 API.\n    \"\"\"\n    def __init__(self, api_key: str = None):\n        super().__init__(\n            base_url=\"https://api.ncbi.nlm.nih.gov/datasets/v2\",\n            api_key=api_key or os.environ.get(\"NCBI_API_KEY\"),\n            requests_per_second=10 if api_key else 3\n        )\n\n    def download_genome(self, accessions: Union[str, GenomeAccession, List[Union[str, GenomeAccession]]], \n                        output_file: Union[str, Path, None] = None,\n                        include: List[Union[str, GenomeTag]] = None,\n                        chromosomes: List[str] = None,\n                        dehydrated: bool = False\n                        ) -&gt; 'DatasetPackage':\n        \"\"\"\n        Download a genome dataset package.\n\n        Args:\n            accessions: Single accession or list of accessions (GCF_/GCA_).\n            output_file: Path to save the ZIP file. If None, saves to a temporary file.\n            include: List of file types to include.\n            chromosomes: filter by chromosome name.\n            dehydrated: If True, download a dehydrated package (fetch.txt only).\n        \"\"\"\n        if isinstance(accessions, str):\n            accessions = [accessions]\n\n        endpoint = \"/genome/download\"\n\n        # Convert enums to strings\n        include_str = [str(i.value if isinstance(i, Enum) else i) for i in (include or [])]\n\n        payload = {\n            \"accessions\": accessions,\n            \"include_annotation_type\": include_str if include_str else None,\n            \"chromosomes\": chromosomes,\n            \"hydrated\": \"DATA_REPORT_ONLY\" if dehydrated else \"FULLY_HYDRATED\"\n        }\n\n        # Remove None values\n        payload = {k: v for k, v in payload.items() if v is not None}\n\n        if output_file is None:\n            # Create a named temp file that persists\n            tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\")\n            output_file = tf.name\n            tf.close()\n\n        self.download(endpoint, output_file, method='POST', json=payload)\n\n        return DatasetPackage(output_file)\n\n    def get_genome_report(self, accessions: Union[str, GenomeAccession, List[Union[str, GenomeAccession]]]) -&gt; Any:\n        \"\"\"\n        Get genome metadata report.\n        \"\"\"\n        if isinstance(accessions, str):\n            accessions = [accessions]\n\n        endpoint = \"/genome/dataset_report\"\n        payload = {\"accessions\": accessions}\n\n        return self.post(endpoint, json=payload)\n\n\n    def download_gene(self, gene_ids: Union[int, str, GeneId, List[Union[int, str, GeneId]]],\n                      output_file: Union[str, Path, None] = None,\n                      include: List[Union[str, GeneTag]] = None,\n                      filename: str = None) -&gt; 'DatasetPackage':\n        \"\"\"\n        Download a gene dataset package.\n        \"\"\"\n        if isinstance(gene_ids, (int, str)):\n            gene_ids = [str(gene_ids)]\n        else:\n            gene_ids = [str(g) for g in gene_ids]\n\n        endpoint = \"/gene/download\"\n\n        include_str = [str(i.value if isinstance(i, Enum) else i) for i in (include or [])]\n\n        payload = {\n            \"gene_ids\": gene_ids,\n            \"include_annotation_type\": include_str if include_str else None\n        }\n        # Remove None\n        payload = {k: v for k, v in payload.items() if v is not None}\n\n        if output_file is None:\n            tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\")\n            output_file = tf.name\n            tf.close()\n\n        self.download(endpoint, output_file, method='POST', json=payload)\n        return DatasetPackage(output_file)\n\n    def download_virus(self, accessions: Union[str, GenomeAccession, List[Union[str, GenomeAccession]]] = None,\n                       taxon: str = None,\n                       output_file: Union[str, Path, None] = None,\n                       include: List[Union[str, VirusTag]] = None) -&gt; 'DatasetPackage':\n        \"\"\"\n        Download a virus dataset package.\n        \"\"\"\n        if output_file is None:\n            tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\")\n            output_file = tf.name\n            tf.close()\n\n        include_str = [str(i.value if isinstance(i, Enum) else i) for i in (include or [])]\n\n        # Virus API has multiple endpoints depending on query type\n        if accessions:\n            if isinstance(accessions, str): accessions = [accessions]\n            endpoint = \"/virus/genome/download\"\n            payload = {\"accessions\": accessions, \"include_annotation_type\": include_str if include_str else None}\n            payload = {k:v for k,v in payload.items() if v}\n            self.download(endpoint, output_file, method='POST', json=payload)\n        elif taxon:\n            endpoint = f\"/virus/taxon/{taxon}/genome/download\"\n            params = {}\n            if include_str: params['include_annotation_type'] = include_str\n            self.download(endpoint, output_file,  method='GET', params=params)\n        else:\n            raise ValueError(\"Must provide accessions or taxon for virus download.\")\n\n        return DatasetPackage(output_file)\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetsClient.download_gene","title":"<code>download_gene(gene_ids, output_file=None, include=None, filename=None)</code>","text":"<p>Download a gene dataset package.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>def download_gene(self, gene_ids: Union[int, str, GeneId, List[Union[int, str, GeneId]]],\n                  output_file: Union[str, Path, None] = None,\n                  include: List[Union[str, GeneTag]] = None,\n                  filename: str = None) -&gt; 'DatasetPackage':\n    \"\"\"\n    Download a gene dataset package.\n    \"\"\"\n    if isinstance(gene_ids, (int, str)):\n        gene_ids = [str(gene_ids)]\n    else:\n        gene_ids = [str(g) for g in gene_ids]\n\n    endpoint = \"/gene/download\"\n\n    include_str = [str(i.value if isinstance(i, Enum) else i) for i in (include or [])]\n\n    payload = {\n        \"gene_ids\": gene_ids,\n        \"include_annotation_type\": include_str if include_str else None\n    }\n    # Remove None\n    payload = {k: v for k, v in payload.items() if v is not None}\n\n    if output_file is None:\n        tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\")\n        output_file = tf.name\n        tf.close()\n\n    self.download(endpoint, output_file, method='POST', json=payload)\n    return DatasetPackage(output_file)\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetsClient.download_genome","title":"<code>download_genome(accessions, output_file=None, include=None, chromosomes=None, dehydrated=False)</code>","text":"<p>Download a genome dataset package.</p> <p>Parameters:</p> Name Type Description Default <code>accessions</code> <code>Union[str, GenomeAccession, List[Union[str, GenomeAccession]]]</code> <p>Single accession or list of accessions (GCF_/GCA_).</p> required <code>output_file</code> <code>Union[str, Path, None]</code> <p>Path to save the ZIP file. If None, saves to a temporary file.</p> <code>None</code> <code>include</code> <code>List[Union[str, GenomeTag]]</code> <p>List of file types to include.</p> <code>None</code> <code>chromosomes</code> <code>List[str]</code> <p>filter by chromosome name.</p> <code>None</code> <code>dehydrated</code> <code>bool</code> <p>If True, download a dehydrated package (fetch.txt only).</p> <code>False</code> Source code in <code>baclib/apis/datasets.py</code> <pre><code>def download_genome(self, accessions: Union[str, GenomeAccession, List[Union[str, GenomeAccession]]], \n                    output_file: Union[str, Path, None] = None,\n                    include: List[Union[str, GenomeTag]] = None,\n                    chromosomes: List[str] = None,\n                    dehydrated: bool = False\n                    ) -&gt; 'DatasetPackage':\n    \"\"\"\n    Download a genome dataset package.\n\n    Args:\n        accessions: Single accession or list of accessions (GCF_/GCA_).\n        output_file: Path to save the ZIP file. If None, saves to a temporary file.\n        include: List of file types to include.\n        chromosomes: filter by chromosome name.\n        dehydrated: If True, download a dehydrated package (fetch.txt only).\n    \"\"\"\n    if isinstance(accessions, str):\n        accessions = [accessions]\n\n    endpoint = \"/genome/download\"\n\n    # Convert enums to strings\n    include_str = [str(i.value if isinstance(i, Enum) else i) for i in (include or [])]\n\n    payload = {\n        \"accessions\": accessions,\n        \"include_annotation_type\": include_str if include_str else None,\n        \"chromosomes\": chromosomes,\n        \"hydrated\": \"DATA_REPORT_ONLY\" if dehydrated else \"FULLY_HYDRATED\"\n    }\n\n    # Remove None values\n    payload = {k: v for k, v in payload.items() if v is not None}\n\n    if output_file is None:\n        # Create a named temp file that persists\n        tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\")\n        output_file = tf.name\n        tf.close()\n\n    self.download(endpoint, output_file, method='POST', json=payload)\n\n    return DatasetPackage(output_file)\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetsClient.download_virus","title":"<code>download_virus(accessions=None, taxon=None, output_file=None, include=None)</code>","text":"<p>Download a virus dataset package.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>def download_virus(self, accessions: Union[str, GenomeAccession, List[Union[str, GenomeAccession]]] = None,\n                   taxon: str = None,\n                   output_file: Union[str, Path, None] = None,\n                   include: List[Union[str, VirusTag]] = None) -&gt; 'DatasetPackage':\n    \"\"\"\n    Download a virus dataset package.\n    \"\"\"\n    if output_file is None:\n        tf = tempfile.NamedTemporaryFile(delete=False, suffix=\".zip\")\n        output_file = tf.name\n        tf.close()\n\n    include_str = [str(i.value if isinstance(i, Enum) else i) for i in (include or [])]\n\n    # Virus API has multiple endpoints depending on query type\n    if accessions:\n        if isinstance(accessions, str): accessions = [accessions]\n        endpoint = \"/virus/genome/download\"\n        payload = {\"accessions\": accessions, \"include_annotation_type\": include_str if include_str else None}\n        payload = {k:v for k,v in payload.items() if v}\n        self.download(endpoint, output_file, method='POST', json=payload)\n    elif taxon:\n        endpoint = f\"/virus/taxon/{taxon}/genome/download\"\n        params = {}\n        if include_str: params['include_annotation_type'] = include_str\n        self.download(endpoint, output_file,  method='GET', params=params)\n    else:\n        raise ValueError(\"Must provide accessions or taxon for virus download.\")\n\n    return DatasetPackage(output_file)\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.DatasetsClient.get_genome_report","title":"<code>get_genome_report(accessions)</code>","text":"<p>Get genome metadata report.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>def get_genome_report(self, accessions: Union[str, GenomeAccession, List[Union[str, GenomeAccession]]]) -&gt; Any:\n    \"\"\"\n    Get genome metadata report.\n    \"\"\"\n    if isinstance(accessions, str):\n        accessions = [accessions]\n\n    endpoint = \"/genome/dataset_report\"\n    payload = {\"accessions\": accessions}\n\n    return self.post(endpoint, json=payload)\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.GeneId","title":"<code>GeneId</code>","text":"<p>               Bases: <code>NcbiToken</code></p> <p>NCBI Gene ID.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class GeneId(NcbiToken):\n    \"\"\"NCBI Gene ID.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.GeneTag","title":"<code>GeneTag</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>File types available in an NCBI gene dataset package.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class GeneTag(str, Enum):\n    \"\"\"File types available in an NCBI gene dataset package.\"\"\"\n    GENE_FASTA = \"GENE_FASTA\"\n    PROTEIN_FASTA = \"PROTEIN_FASTA\"\n    CDS_FASTA = \"CDS_FASTA\"\n    GENE_FLANKING_FASTA = \"GENE_FLANKING_FASTA\"\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.GenomeAccession","title":"<code>GenomeAccession</code>","text":"<p>               Bases: <code>NcbiToken</code></p> <p>NCBI Genome Accession (e.g. GCF_000005845.2).</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class GenomeAccession(NcbiToken):\n    \"\"\"NCBI Genome Accession (e.g. GCF_000005845.2).\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.GenomeTag","title":"<code>GenomeTag</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>File types available in an NCBI genome dataset package.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class GenomeTag(str, Enum):\n    \"\"\"File types available in an NCBI genome dataset package.\"\"\"\n    GENOME_FASTA = \"GENOME_FASTA\"\n    GENOME_GFF = \"GENOME_GFF\"\n    GENOME_GB = \"GENOME_GB\"\n    RNA_FASTA = \"RNA_FASTA\"\n    PROT_FASTA = \"PROT_FASTA\"\n    CDS_FASTA = \"CDS_FASTA\"\n    SEQUENCE_REPORT = \"SEQUENCE_REPORT\"\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.NcbiToken","title":"<code>NcbiToken</code>","text":"<p>               Bases: <code>Token</code></p> <p>Base class for NCBI tokens.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class NcbiToken(Token):\n    \"\"\"Base class for NCBI tokens.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/apis/datasets/#baclib.apis.datasets.VirusTag","title":"<code>VirusTag</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>File types available in an NCBI virus dataset package.</p> Source code in <code>baclib/apis/datasets.py</code> <pre><code>class VirusTag(str, Enum):\n    \"\"\"File types available in an NCBI virus dataset package.\"\"\"\n    GENOME = \"GENOME\"\n    PROTEIN = \"PROTEIN\"\n    CDS = \"CDS\"\n    ANNOTATION = \"ANNOTATION\"\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/","title":"prodoric","text":""},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric","title":"<code>baclib.apis.prodoric</code>","text":"<p>Client for the PRODORIC API, providing access to transcription factor binding site matrices and regulatory networks.</p>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.MatrixAccession","title":"<code>MatrixAccession</code>","text":"<p>               Bases: <code>ProdoricToken</code></p> <p>PRODORIC Matrix Accession (e.g. MX000001).</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>class MatrixAccession(ProdoricToken):\n    \"\"\"PRODORIC Matrix Accession (e.g. MX000001).\"\"\"\n\n    @property\n    def id(self) -&gt; int:\n        \"\"\"Returns the numeric ID part of the accession.\"\"\"\n        if self.startswith(\"MX\"):\n            return int(self[2:])\n        if self.isdigit():\n             return int(self)\n        raise ValueError(f\"Invalid matrix accession format: {self}\")\n\n    @classmethod\n    def from_id(cls, mx_id: Union[int, str]) -&gt; 'MatrixAccession':\n        \"\"\"Create a MatrixAccession from an integer ID or string.\"\"\"\n        if isinstance(mx_id, int):\n             return cls(f\"MX{mx_id:06d}\")\n        # If string is just digits, prepend MX\n        s = str(mx_id)\n        if s.isdigit():\n             return cls(f\"MX{s.zfill(6)}\") \n        if not s.startswith(\"MX\"):\n             return cls(f\"MX{s}\")\n        return cls(s)\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.MatrixAccession.id","title":"<code>id</code>  <code>property</code>","text":"<p>Returns the numeric ID part of the accession.</p>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.MatrixAccession.from_id","title":"<code>from_id(mx_id)</code>  <code>classmethod</code>","text":"<p>Create a MatrixAccession from an integer ID or string.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>@classmethod\ndef from_id(cls, mx_id: Union[int, str]) -&gt; 'MatrixAccession':\n    \"\"\"Create a MatrixAccession from an integer ID or string.\"\"\"\n    if isinstance(mx_id, int):\n         return cls(f\"MX{mx_id:06d}\")\n    # If string is just digits, prepend MX\n    s = str(mx_id)\n    if s.isdigit():\n         return cls(f\"MX{s.zfill(6)}\") \n    if not s.startswith(\"MX\"):\n         return cls(f\"MX{s}\")\n    return cls(s)\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.OrganismAccession","title":"<code>OrganismAccession</code>","text":"<p>               Bases: <code>ProdoricToken</code></p> <p>PRODORIC Organism Accession.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>class OrganismAccession(ProdoricToken):\n    \"\"\"PRODORIC Organism Accession.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.ProdoricClient","title":"<code>ProdoricClient</code>","text":"<p>               Bases: <code>ApiClient</code></p> <p>Client for the PRODORIC API. Provides access to transcription factor binding sites and matrices.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>class ProdoricClient(ApiClient):\n    \"\"\"\n    Client for the PRODORIC API.\n    Provides access to transcription factor binding sites and matrices.\n    \"\"\"\n    def __init__(self, api_key: str = None):\n        super().__init__(\n            base_url=\"https://www.prodoric.de/api\",\n            api_key=None, \n            requests_per_second=3\n        )\n\n    def search_motifs(self, term: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Search for matrices by term (accession, name, organism).\n        Returns a list of MatrixSearchResult objects (dicts).\n        \"\"\"\n        encoded_term = urllib.parse.quote(term)\n        endpoint = f\"matrix/search/{encoded_term}\"\n\n        try:\n            return self.get(endpoint)\n        except Exception:\n            return []\n\n    def get_organisms(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"\n        Retrieve a list of all available organisms.\n        Returns a list of dicts with keys: name, short, accession.\n        Accessions are converted to OrganismAccession tokens.\n        \"\"\"\n        try:\n            data = self.get(\"organism\")\n            # Convert accessions to tokens\n            if isinstance(data, list):\n                for item in data:\n                    if \"accession\" in item:\n                        item[\"accession\"] = OrganismAccession(item[\"accession\"])\n            return data\n        except Exception:\n            # Return empty list on failure or log?\n            return []\n\n    def get_motif(self, accession: Union[str, MatrixAccession]) -&gt; Optional[Motif]:\n        \"\"\"\n        Retrieve a matrix by accession (e.g. MX000001) and return a Motif object.\n        \"\"\"\n        if isinstance(accession, MatrixAccession):\n            # Use the ID property from our token class if available, \n            # but since endpoint expects MX + id, we can just use string representation \n            # if we trust the token (which is MX+ID).\n            # But the endpoint code in `get_motif` manually strips 'MX'.\n            # Let's rely on string representation first.\n            accession_str = str(accession)\n        else:\n            accession_str = accession\n\n        clean_id = accession_str\n        if accession_str.startswith(\"MX\"):\n            clean_id = accession_str[2:]\n\n        # Validation: check if clean_id is integer-like\n        if not clean_id.isdigit():\n             raise ValueError(f\"Invalid accession format: {accession}. Expected MX followed by integers.\")\n\n        endpoint = f\"matrix/MX{clean_id}\"\n\n        try:\n            data = self.get(endpoint)\n            # data could be None if 404\n            if not data: return None\n            return self._parse_motif(data, accession)\n        except Exception:\n            # We might want to log or re-raise, but for now return None on failure\n            return None\n\n    def get_regulon(self, matrix_id: str) -&gt; Optional[Graph]:\n        \"\"\"\n        Retrieve the regulon network for a specific matrix (TF).\n        Returns a generic Graph object.\n        \"\"\"\n        clean_id = matrix_id\n        if matrix_id.startswith(\"MX\"):\n            clean_id = matrix_id[2:]\n\n        endpoint = f\"prodonet/MX{clean_id}\"\n        return self._fetch_graph(endpoint)\n\n    def get_network(self, organism_acc: Union[str, OrganismAccession]) -&gt; Optional[Graph]:\n        \"\"\"\n        Retrieve the regulatory network for an organism.\n        \"\"\"\n        endpoint = f\"prodonet/{organism_acc}\"\n        return self._fetch_graph(endpoint)\n\n    def _fetch_graph(self, endpoint: str) -&gt; Optional[Graph]:\n        try:\n            data = self.get(endpoint)\n            if not data: return None\n            return self._parse_network(data)\n        except Exception:\n            return None\n\n    def _parse_motif(self, data: Dict[str, Any], accession: str) -&gt; Motif:\n        \"\"\"\n        Convert PRODORIC matrix JSON to baclib Motif.\n        \"\"\"\n        # Data contains \"pwm\" key with \"A\", \"C\", \"G\", \"T\" lists.\n        # Although called \"pwm\", the values look like counts (integers like 0, 2, 0.5?)\n        # 2 and 0.5? Maybe weighted counts?\n        # Let's assume they are frequencies or weights that need to be normalized.\n        # But wait, Motif.from_counts expects integers conventionally, but floats work nicely too.\n        # If they sum to significantly more than 1 per column, they are counts/weights.\n        # If they sum to 1, they are probabilities.\n\n        pwm = data.get(\"pwm\", {})\n        if not pwm:\n             raise ValueError(\"No PWM data found in response\")\n\n        # Order: A, C, G, T? BacLib DNA default is often lexicographical or specific.\n        # Alphabet.DNA default order is typically A, C, G, T or similar.\n        # Let's check BacLib alphabet.\n        # Alphabet.DNA is usually A, C, G, T.\n        # We need to constructing a matrix of shape (4, L).\n\n        # Keys in JSON: \"A\", \"C\", \"G\", \"T\"\n        # We need to map these to the Alphabet indices.\n\n        # Alphabet.DNA is defined as b'TCAG' (Lines 511 in core/alphabet.py)\n        # We must construct the matrix with rows in this order: T, C, A, G\n\n        alphabet = Alphabet.DNA\n\n        # Check length from 'A' (or any key present)\n        if 'A' in pwm:\n             length = len(pwm['A'])\n        elif 'T' in pwm:\n             length = len(pwm['T'])\n        else:\n             raise ValueError(\"PWM must contain at least 'A' or 'T'\")\n\n        if length == 0: raise ValueError(\"Empty matrix\")\n\n        # Explicit construction in TCAG order:\n        # Row 0: T\n        # Row 1: C\n        # Row 2: A\n        # Row 3: G\n        zeros = [0.0] * length\n        rows = [\n            pwm.get('T', zeros),\n            pwm.get('C', zeros),\n            pwm.get('A', zeros),\n            pwm.get('G', zeros)\n        ]\n\n        matrix = np.array(rows, dtype=np.float32)\n\n        # The data in example has 0, 2, 0.5.\n        # If we treat them as counts/weights, we can use from_counts \n        # (which normalizes to freqs) or from_weights directly?\n        # from_counts does: freqs = counts / sum; then from_freqs\n        # from_freqs does: weights = freqs / bg; then from_weights\n\n        # If the input IS weights (odds), using from_counts is wrong.\n        # If the input IS counts, using from_weights is wrong.\n        # 0.5 looks like a count (maybe 0.5 sequences? weighted sequence?).\n        # \"max_score\": 24 implies a scoring range.\n\n        # Let's blindly try from_counts as a safe default for now unless we know it's log-odds.\n        # (Raw PSSM values would be negative and positive floats).\n        # These are all non-negative. Likely counts or frequencies.\n\n        bs_name = data.get(\"name\", \"Unknown\").encode('ascii', errors='ignore')\n\n        bg = Background.uniform(alphabet)\n        return Motif.from_counts(bs_name, matrix, bg)\n\n    def _parse_network(self, data: Dict[str, Any]) -&gt; Graph:\n        \"\"\"\n        Convert ProdoNet JSON to baclib Graph.\n        \"\"\"\n        g = Graph(directed=True, multi=True)\n\n        # 1. Add Nodes\n        nodes = data.get(\"nodes\", [])\n        # Each node has \"id\" (int), \"label\", \"type\", \"mx\" (optional), \"group\"\n        # We should use \"id\" as the internal ID (converted to bytes) for the Graph structure?\n        # Or should we use labels? IDs are safer.\n        # Edge \"from\" and \"to\" use these integer IDs.\n\n        node_map = {} # int id -&gt; bytes id\n\n        for n in nodes:\n            nid = n.get(\"id\")\n            label = n.get(\"label\", str(nid))\n            # Create a unique bytes ID. Maybe just str(nid)?\n            bid = str(nid).encode('ascii')\n            node_map[nid] = bid\n\n            # Attributes\n            attrs = {\n                b'label': label,\n                b'type': n.get('type'),\n                b'group': n.get('group'),\n                b'mx': n.get('mx')\n            }\n            g.add_node(bid, attributes=attrs)\n\n        # 2. Add Edges\n        edges = data.get(\"edges\", [])\n        for e in edges:\n            u_int = e.get(\"from\")\n            v_int = e.get(\"to\")\n            etype = e.get(\"type\") # \"-\", \"op\", \"+\" ?, \"repression\"?\n\n            if u_int in node_map and v_int in node_map:\n                u = node_map[u_int]\n                v = node_map[v_int]\n\n                attrs = {}\n                if etype: attrs[b'type'] = etype\n\n                # Create Edge object\n                # Default strands to forward as graph is abstract regulation\n                edge = Edge(u, v, attributes=attrs)\n                g.add_edges([edge])\n\n        return g\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.ProdoricClient.get_motif","title":"<code>get_motif(accession)</code>","text":"<p>Retrieve a matrix by accession (e.g. MX000001) and return a Motif object.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>def get_motif(self, accession: Union[str, MatrixAccession]) -&gt; Optional[Motif]:\n    \"\"\"\n    Retrieve a matrix by accession (e.g. MX000001) and return a Motif object.\n    \"\"\"\n    if isinstance(accession, MatrixAccession):\n        # Use the ID property from our token class if available, \n        # but since endpoint expects MX + id, we can just use string representation \n        # if we trust the token (which is MX+ID).\n        # But the endpoint code in `get_motif` manually strips 'MX'.\n        # Let's rely on string representation first.\n        accession_str = str(accession)\n    else:\n        accession_str = accession\n\n    clean_id = accession_str\n    if accession_str.startswith(\"MX\"):\n        clean_id = accession_str[2:]\n\n    # Validation: check if clean_id is integer-like\n    if not clean_id.isdigit():\n         raise ValueError(f\"Invalid accession format: {accession}. Expected MX followed by integers.\")\n\n    endpoint = f\"matrix/MX{clean_id}\"\n\n    try:\n        data = self.get(endpoint)\n        # data could be None if 404\n        if not data: return None\n        return self._parse_motif(data, accession)\n    except Exception:\n        # We might want to log or re-raise, but for now return None on failure\n        return None\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.ProdoricClient.get_network","title":"<code>get_network(organism_acc)</code>","text":"<p>Retrieve the regulatory network for an organism.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>def get_network(self, organism_acc: Union[str, OrganismAccession]) -&gt; Optional[Graph]:\n    \"\"\"\n    Retrieve the regulatory network for an organism.\n    \"\"\"\n    endpoint = f\"prodonet/{organism_acc}\"\n    return self._fetch_graph(endpoint)\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.ProdoricClient.get_organisms","title":"<code>get_organisms()</code>","text":"<p>Retrieve a list of all available organisms. Returns a list of dicts with keys: name, short, accession. Accessions are converted to OrganismAccession tokens.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>def get_organisms(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Retrieve a list of all available organisms.\n    Returns a list of dicts with keys: name, short, accession.\n    Accessions are converted to OrganismAccession tokens.\n    \"\"\"\n    try:\n        data = self.get(\"organism\")\n        # Convert accessions to tokens\n        if isinstance(data, list):\n            for item in data:\n                if \"accession\" in item:\n                    item[\"accession\"] = OrganismAccession(item[\"accession\"])\n        return data\n    except Exception:\n        # Return empty list on failure or log?\n        return []\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.ProdoricClient.get_regulon","title":"<code>get_regulon(matrix_id)</code>","text":"<p>Retrieve the regulon network for a specific matrix (TF). Returns a generic Graph object.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>def get_regulon(self, matrix_id: str) -&gt; Optional[Graph]:\n    \"\"\"\n    Retrieve the regulon network for a specific matrix (TF).\n    Returns a generic Graph object.\n    \"\"\"\n    clean_id = matrix_id\n    if matrix_id.startswith(\"MX\"):\n        clean_id = matrix_id[2:]\n\n    endpoint = f\"prodonet/MX{clean_id}\"\n    return self._fetch_graph(endpoint)\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.ProdoricClient.search_motifs","title":"<code>search_motifs(term)</code>","text":"<p>Search for matrices by term (accession, name, organism). Returns a list of MatrixSearchResult objects (dicts).</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>def search_motifs(self, term: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Search for matrices by term (accession, name, organism).\n    Returns a list of MatrixSearchResult objects (dicts).\n    \"\"\"\n    encoded_term = urllib.parse.quote(term)\n    endpoint = f\"matrix/search/{encoded_term}\"\n\n    try:\n        return self.get(endpoint)\n    except Exception:\n        return []\n</code></pre>"},{"location":"reference/baclib/apis/prodoric/#baclib.apis.prodoric.ProdoricToken","title":"<code>ProdoricToken</code>","text":"<p>               Bases: <code>Token</code></p> <p>Base class for PRODORIC tokens.</p> Source code in <code>baclib/apis/prodoric.py</code> <pre><code>class ProdoricToken(Token):\n    \"\"\"Base class for PRODORIC tokens.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/containers/","title":"containers","text":""},{"location":"reference/baclib/containers/#baclib.containers","title":"<code>baclib.containers</code>","text":"<p>This module contains abstract containers for various components in bacterial genomics. Each component may have a batched counterpart for efficient batch processing.</p>"},{"location":"reference/baclib/containers/#baclib.containers.Batch","title":"<code>Batch</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all batch containers.</p> <p>Batches are columnar containers that store multiple instances of a component efficiently (usually using SoA layout with NumPy arrays). They enforce the Sequence protocol (len, getitem, iter).</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>class Batch(ABC):\n    \"\"\"\n    Abstract base class for all batch containers.\n\n    Batches are columnar containers that store multiple instances of a component\n    efficiently (usually using SoA layout with NumPy arrays). They enforce the\n    Sequence protocol (len, getitem, iter).\n    \"\"\"\n    __slots__ = ()\n    @abstractmethod\n    def __len__(self) -&gt; int: ...\n    @classmethod\n    @abstractmethod\n    def empty(cls) -&gt; 'Batch':\n        \"\"\"Creates an empty batch.\"\"\"\n        ...\n    @property\n    @abstractmethod\n    def component(self):\n        \"\"\"Returns the component class stored in this batch.\"\"\"\n        ...\n    @classmethod\n    @abstractmethod\n    def build(cls, components: Iterable[object]) -&gt; 'Batch':\n        \"\"\"Constructs a batch from an iterable of components.\"\"\"\n        ...\n    @classmethod\n    @abstractmethod\n    def concat(cls, batches: Iterable['Batch']) -&gt; 'Batch':\n        \"\"\"Concatenates multiple batches into one.\"\"\"\n        ...\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'Batch':\n        \"\"\"\n        Creates a batch of *n* filled with \"empty\" or default components.\n\n        For RaggedBatches, this means *n* empty lists.\n        For FixedBatches, this means *n* default/null entries.\n\n        Args:\n            n: Size of the batch.\n\n        Returns:\n            A new batch of size *n*.\n\n        Raises:\n            NotImplementedError: If the subclass does not support zero-initialization.\n        \"\"\"\n        if n == 0: return cls.empty()\n        raise NotImplementedError(f\"{cls.__name__} does not support zeros(n&gt;0)\")\n    @abstractmethod\n    def __getitem__(self, item): ...\n    def __iter__(self):\n        for i in range(len(self)): yield self[i]\n    def __bool__(self):\n        return len(self) &gt; 0\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the approximate memory usage of the batch in bytes.\"\"\"\n        return 0\n    def copy(self) -&gt; 'Batch':\n        \"\"\"Returns a deep copy of the batch.\"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.Batch.component","title":"<code>component</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the component class stored in this batch.</p>"},{"location":"reference/baclib/containers/#baclib.containers.Batch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the approximate memory usage of the batch in bytes.</p>"},{"location":"reference/baclib/containers/#baclib.containers.Batch.build","title":"<code>build(components)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Constructs a batch from an iterable of components.</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>@classmethod\n@abstractmethod\ndef build(cls, components: Iterable[object]) -&gt; 'Batch':\n    \"\"\"Constructs a batch from an iterable of components.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.Batch.concat","title":"<code>concat(batches)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Concatenates multiple batches into one.</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>@classmethod\n@abstractmethod\ndef concat(cls, batches: Iterable['Batch']) -&gt; 'Batch':\n    \"\"\"Concatenates multiple batches into one.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.Batch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of the batch.</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>def copy(self) -&gt; 'Batch':\n    \"\"\"Returns a deep copy of the batch.\"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.Batch.empty","title":"<code>empty()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Creates an empty batch.</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>@classmethod\n@abstractmethod\ndef empty(cls) -&gt; 'Batch':\n    \"\"\"Creates an empty batch.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.Batch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n filled with \"empty\" or default components.</p> <p>For RaggedBatches, this means n empty lists. For FixedBatches, this means n default/null entries.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Size of the batch.</p> required <p>Returns:</p> Type Description <code>Batch</code> <p>A new batch of size n.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the subclass does not support zero-initialization.</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'Batch':\n    \"\"\"\n    Creates a batch of *n* filled with \"empty\" or default components.\n\n    For RaggedBatches, this means *n* empty lists.\n    For FixedBatches, this means *n* default/null entries.\n\n    Args:\n        n: Size of the batch.\n\n    Returns:\n        A new batch of size *n*.\n\n    Raises:\n        NotImplementedError: If the subclass does not support zero-initialization.\n    \"\"\"\n    if n == 0: return cls.empty()\n    raise NotImplementedError(f\"{cls.__name__} does not support zeros(n&gt;0)\")\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.Batchable","title":"<code>Batchable</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Interface for components that can be batched.</p> <p>Classes implementing this interface must define a <code>batch</code> property that returns their corresponding <code>Batch</code> class.</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>class Batchable(ABC):\n    \"\"\"\n    Interface for components that can be batched.\n\n    Classes implementing this interface must define a ``batch`` property\n    that returns their corresponding ``Batch`` class.\n    \"\"\"\n    __slots__ = ()\n    @property\n    @abstractmethod\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the Batch class associated with this component type.\"\"\"\n        ...\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.Batchable.batch","title":"<code>batch</code>  <code>abstractmethod</code> <code>property</code>","text":"<p>Returns the Batch class associated with this component type.</p>"},{"location":"reference/baclib/containers/#baclib.containers.RaggedBatch","title":"<code>RaggedBatch</code>","text":"<p>               Bases: <code>Batch</code></p> <p>Base class for batches that store variable-length items in a flattened format (CSR-like).</p> <p>Manages the offsets array and length calculation. Subclasses typically add flat data arrays.</p> <p>Parameters:</p> Name Type Description Default <code>offsets</code> <code>ndarray</code> <p>Array of offsets (size N + 1).</p> required <code>validate</code> <code>bool</code> <p>If <code>True</code>, checks offset validity.</p> <code>False</code> Source code in <code>baclib/containers/__init__.py</code> <pre><code>class RaggedBatch(Batch):\n    \"\"\"\n    Base class for batches that store variable-length items in a flattened format (CSR-like).\n\n    Manages the offsets array and length calculation. Subclasses typically add\n    flat data arrays.\n\n    Args:\n        offsets: Array of offsets (size *N* + 1).\n        validate: If ``True``, checks offset validity.\n    \"\"\"\n    __slots__ = ('_offsets', '_length')\n    def __init__(self, offsets: np.ndarray, validate: bool = False):\n        self._offsets = offsets\n        self._length = len(offsets) - 1\n        if validate: self._validate()\n\n    def _validate(self):\n        if self._length &lt; 0:\n            raise ValueError(\"Offsets array must contain at least one element (usually [0]).\")\n        if self._offsets[0] != 0:\n            raise ValueError(\"Offsets must start with 0.\")\n        if not np.all(self._offsets[:-1] &lt;= self._offsets[1:]):\n            raise ValueError(\"Offsets must be non-decreasing.\")\n\n    @property\n    def total_size(self) -&gt; int:\n        \"\"\"Returns the total number of elements flattened across all components.\"\"\"\n        return self._offsets[-1] if self._length &gt;= 0 else 0\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'RaggedBatch':\n        \"\"\"Creates a batch of *n* empty components (length 0).\"\"\"\n        # [0, 0, ..., 0] -&gt; all lengths are 0\n        offsets = np.zeros(n + 1, dtype=np.int32)\n        return cls(offsets)\n\n    @classmethod\n    def empty(cls) -&gt; 'RaggedBatch':\n        \"\"\"Creates an empty batch.\"\"\"\n        return cls.zeros(0)\n\n    @property\n    def lengths(self) -&gt; np.ndarray:\n        \"\"\"Returns the lengths of the components as a numpy array.\"\"\"\n        return np.diff(self._offsets)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the memory usage of the offset array.\"\"\"\n        return self._offsets.nbytes\n\n    def __len__(self) -&gt; int: return self._length\n\n    def _get_slice_info(self, item: slice) -&gt; tuple[np.ndarray, int, int]:\n        \"\"\"Helper to calculate new offsets and data slice indices for slicing.\"\"\"\n        start, stop, step = item.indices(len(self))\n        if step != 1: raise NotImplementedError(\"Batch slicing with step != 1 not supported\")\n        val_start = self._offsets[start]\n        val_end = self._offsets[stop]\n        new_offsets = self._offsets[start:stop+1] - val_start\n        return new_offsets, val_start, val_end\n\n    @staticmethod\n    def _stack_offsets(batches: Iterable['RaggedBatch']) -&gt; np.ndarray:\n        \"\"\"Helper to concatenate offset arrays from multiple batches.\"\"\"\n        batches = list(batches)\n        offsets_list = [b._offsets for b in batches]\n        if not offsets_list: return np.zeros(1, dtype=np.int32)\n\n        # We must shift subsequent offsets by the total count of previous batches\n        # offsets[0] is always 0, so we slice [1:] for subsequent appends\n        shifts = np.cumsum([0] + [b._offsets[-1] for b in batches[:-1]])\n        if len(batches) == 1:\n            return offsets_list[0]\n\n        parts = [offsets_list[0]]\n        for i in range(1, len(batches)):\n             parts.append(offsets_list[i][1:] + shifts[i])\n\n        return np.concatenate(parts)\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.RaggedBatch.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>Returns the lengths of the components as a numpy array.</p>"},{"location":"reference/baclib/containers/#baclib.containers.RaggedBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the memory usage of the offset array.</p>"},{"location":"reference/baclib/containers/#baclib.containers.RaggedBatch.total_size","title":"<code>total_size</code>  <code>property</code>","text":"<p>Returns the total number of elements flattened across all components.</p>"},{"location":"reference/baclib/containers/#baclib.containers.RaggedBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty batch.</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'RaggedBatch':\n    \"\"\"Creates an empty batch.\"\"\"\n    return cls.zeros(0)\n</code></pre>"},{"location":"reference/baclib/containers/#baclib.containers.RaggedBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n empty components (length 0).</p> Source code in <code>baclib/containers/__init__.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'RaggedBatch':\n    \"\"\"Creates a batch of *n* empty components (length 0).\"\"\"\n    # [0, 0, ..., 0] -&gt; all lengths are 0\n    offsets = np.zeros(n + 1, dtype=np.int32)\n    return cls(offsets)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/","title":"alignment","text":""},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment","title":"<code>baclib.containers.alignment</code>","text":"<p>Module for managing alignments and alignment batches using Structure-of-Arrays layout.</p>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment","title":"<code>Alignment</code>","text":"<p>               Bases: <code>Feature</code></p> <p>Represents a pairwise sequence alignment between a query and a target.</p> <p>Inherits from <code>Feature</code> to represent the alignment interval on the target sequence.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>bytes</code> <p>Query sequence ID (bytes).</p> required <code>query_interval</code> <code>Interval</code> <p>Interval on the query sequence.</p> required <code>target</code> <code>bytes</code> <p>Target sequence ID (bytes).</p> required <code>interval</code> <code>Interval</code> <p>Interval on the target sequence.</p> required <code>query_length</code> <code>int</code> <p>Total length of the query sequence.</p> <code>0</code> <code>target_length</code> <code>int</code> <p>Total length of the target sequence.</p> <code>0</code> <code>length</code> <code>int</code> <p>Alignment length (including gaps).</p> <code>0</code> <code>cigar</code> <code>bytes</code> <p>CIGAR string (bytes).</p> <code>None</code> <code>n_matches</code> <code>int</code> <p>Number of matching bases.</p> <code>0</code> <code>quality</code> <code>int</code> <p>Mapping quality (Phred).</p> <code>0</code> <code>qualifiers</code> <code>Iterable[tuple[bytes, Any]]</code> <p>Optional qualifiers/tags.</p> <code>None</code> <code>score</code> <code>float</code> <p>Alignment score.</p> <code>0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; aln = Alignment(b'read1', Interval(0, 100), b'ref', Interval(500, 600), score=50.0)\n&gt;&gt;&gt; aln.query\nb'read1'\n</code></pre> Source code in <code>baclib/containers/alignment.py</code> <pre><code>class Alignment(Feature):\n    \"\"\"\n    Represents a pairwise sequence alignment between a query and a target.\n\n    Inherits from ``Feature`` to represent the alignment interval on the target sequence.\n\n    Args:\n        query: Query sequence ID (bytes).\n        query_interval: Interval on the query sequence.\n        target: Target sequence ID (bytes).\n        interval: Interval on the target sequence.\n        query_length: Total length of the query sequence.\n        target_length: Total length of the target sequence.\n        length: Alignment length (including gaps).\n        cigar: CIGAR string (bytes).\n        n_matches: Number of matching bases.\n        quality: Mapping quality (Phred).\n        qualifiers: Optional qualifiers/tags.\n        score: Alignment score.\n\n    Examples:\n        &gt;&gt;&gt; aln = Alignment(b'read1', Interval(0, 100), b'ref', Interval(500, 600), score=50.0)\n        &gt;&gt;&gt; aln.query\n        b'read1'\n    \"\"\"\n    __slots__ = (\n        'query', 'query_interval', 'query_length', 'target', 'target_length', 'length', 'cigar', 'score',\n        'n_matches', 'quality'\n    )\n\n    def __init__(self, query: bytes, query_interval: 'Interval', target: bytes, interval: 'Interval',\n                 query_length: int = 0, target_length: int = 0, length: int = 0, cigar: bytes = None,\n                 n_matches: int = 0, quality: int = 0, qualifiers: Iterable[tuple[bytes, Any]] = None, score: float = 0):\n        super().__init__(interval, key=FeatureKey.MISC_FEATURE, qualifiers=qualifiers)\n        self.query = query\n        self.target = target\n        self.query_interval = query_interval\n        self.query_length = query_length\n        self.target_length = target_length\n        self.length = length\n        self.cigar = cigar\n        self.score = score\n        self.n_matches = n_matches\n        self.quality = quality\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``AlignmentBatch`` class.\n        \"\"\"\n        return AlignmentBatch\n\n    def __repr__(self):\n        return (f\"Alignment({self.query.decode(Alphabet.ENCODING, 'ignore')}-&gt;\"\n                f\"{self.target.decode(Alphabet.ENCODING, 'ignore')}, score={self.score})\")\n\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return (super().__eq__(other) and\n                    self.query == other.query and\n                    self.query_interval == other.query_interval and\n                    self.score == other.score and\n                    self.cigar == other.cigar)\n        return False\n\n    def copy(self) -&gt; 'Alignment':\n        \"\"\"Returns a deep copy of the alignment.\"\"\"\n        return Alignment(\n            query=self.query, query_interval=self.query_interval, target=self.target,\n            interval=self.interval, query_length=self.query_length, target_length=self.target_length,\n            length=self.length, cigar=self.cigar, n_matches=self.n_matches, quality=self.quality,\n            qualifiers=list(self.qualifiers), score=self.score\n        )\n\n    def shift(self, x: int, y: int = None) -&gt; 'Alignment':\n        \"\"\"Shifts the alignment coordinates on the target sequence.\n\n        Args:\n            x: Amount to shift the start.\n            y: Amount to shift the end (defaults to *x*).\n\n        Returns:\n            A new shifted ``Alignment``.\n        \"\"\"\n        return Alignment(\n            query=self.query, query_interval=self.query_interval, target=self.target,\n            interval=self.interval.shift(x, y),\n            query_length=self.query_length, target_length=self.target_length, length=self.length,\n            cigar=self.cigar, n_matches=self.n_matches, quality=self.quality,\n            qualifiers=list(self.qualifiers), score=self.score\n        )\n\n    def reverse_complement(self, parent_length: int) -&gt; 'Alignment':\n        \"\"\"Reverse complements the alignment relative to a parent sequence of given length.\n\n        Args:\n            parent_length: Length of the target sequence.\n\n        Returns:\n            A new ``Alignment`` with transformed coordinates and strand.\n        \"\"\"\n        return Alignment(\n            query=self.query, query_interval=self.query_interval, target=self.target,\n            interval=self.interval.reverse_complement(parent_length),\n            query_length=self.query_length, target_length=self.target_length, length=self.length,\n            cigar=self.cigar, n_matches=self.n_matches, quality=self.quality,\n            qualifiers=list(self.qualifiers), score=self.score\n        )\n\n    def query_coverage(self) -&gt; float:\n        \"\"\"Returns the fraction of the query sequence covered by the alignment.\"\"\"\n        return len(self.query_interval) / self.query_length if self.query_length &gt; 0 else 0.0\n\n    def target_coverage(self) -&gt; float:\n        \"\"\"Returns the fraction of the target sequence covered by the alignment.\"\"\"\n        return len(self.interval) / self.target_length if self.target_length &gt; 0 else 0.0\n\n    def identity(self) -&gt; float:\n        \"\"\"Returns the sequence identity (matches / alignment length).\"\"\"\n        return self.n_matches / self.length if self.length &gt; 0 else 0.0\n\n    def flip(self) -&gt; 'Alignment':\n        \"\"\"Swaps query and target roles.\n\n        Returns:\n            A new ``Alignment`` where query becomes target and vice versa.\n        \"\"\"\n        return Alignment(\n            query=self.target, query_interval=self.interval, query_length=self.target_length,\n            target=self.query, interval=self.query_interval, target_length=self.query_length,\n            length=self.length, cigar=self.cigar, score=self.score, n_matches=self.n_matches,\n            quality=self.quality, qualifiers=list(self.qualifiers) if self.qualifiers else None\n        )\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>AlignmentBatch</code> class.</p>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of the alignment.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def copy(self) -&gt; 'Alignment':\n    \"\"\"Returns a deep copy of the alignment.\"\"\"\n    return Alignment(\n        query=self.query, query_interval=self.query_interval, target=self.target,\n        interval=self.interval, query_length=self.query_length, target_length=self.target_length,\n        length=self.length, cigar=self.cigar, n_matches=self.n_matches, quality=self.quality,\n        qualifiers=list(self.qualifiers), score=self.score\n    )\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.flip","title":"<code>flip()</code>","text":"<p>Swaps query and target roles.</p> <p>Returns:</p> Type Description <code>Alignment</code> <p>A new <code>Alignment</code> where query becomes target and vice versa.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def flip(self) -&gt; 'Alignment':\n    \"\"\"Swaps query and target roles.\n\n    Returns:\n        A new ``Alignment`` where query becomes target and vice versa.\n    \"\"\"\n    return Alignment(\n        query=self.target, query_interval=self.interval, query_length=self.target_length,\n        target=self.query, interval=self.query_interval, target_length=self.query_length,\n        length=self.length, cigar=self.cigar, score=self.score, n_matches=self.n_matches,\n        quality=self.quality, qualifiers=list(self.qualifiers) if self.qualifiers else None\n    )\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.identity","title":"<code>identity()</code>","text":"<p>Returns the sequence identity (matches / alignment length).</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def identity(self) -&gt; float:\n    \"\"\"Returns the sequence identity (matches / alignment length).\"\"\"\n    return self.n_matches / self.length if self.length &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.query_coverage","title":"<code>query_coverage()</code>","text":"<p>Returns the fraction of the query sequence covered by the alignment.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def query_coverage(self) -&gt; float:\n    \"\"\"Returns the fraction of the query sequence covered by the alignment.\"\"\"\n    return len(self.query_interval) / self.query_length if self.query_length &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.reverse_complement","title":"<code>reverse_complement(parent_length)</code>","text":"<p>Reverse complements the alignment relative to a parent sequence of given length.</p> <p>Parameters:</p> Name Type Description Default <code>parent_length</code> <code>int</code> <p>Length of the target sequence.</p> required <p>Returns:</p> Type Description <code>Alignment</code> <p>A new <code>Alignment</code> with transformed coordinates and strand.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def reverse_complement(self, parent_length: int) -&gt; 'Alignment':\n    \"\"\"Reverse complements the alignment relative to a parent sequence of given length.\n\n    Args:\n        parent_length: Length of the target sequence.\n\n    Returns:\n        A new ``Alignment`` with transformed coordinates and strand.\n    \"\"\"\n    return Alignment(\n        query=self.query, query_interval=self.query_interval, target=self.target,\n        interval=self.interval.reverse_complement(parent_length),\n        query_length=self.query_length, target_length=self.target_length, length=self.length,\n        cigar=self.cigar, n_matches=self.n_matches, quality=self.quality,\n        qualifiers=list(self.qualifiers), score=self.score\n    )\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.shift","title":"<code>shift(x, y=None)</code>","text":"<p>Shifts the alignment coordinates on the target sequence.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>Amount to shift the start.</p> required <code>y</code> <code>int</code> <p>Amount to shift the end (defaults to x).</p> <code>None</code> <p>Returns:</p> Type Description <code>Alignment</code> <p>A new shifted <code>Alignment</code>.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def shift(self, x: int, y: int = None) -&gt; 'Alignment':\n    \"\"\"Shifts the alignment coordinates on the target sequence.\n\n    Args:\n        x: Amount to shift the start.\n        y: Amount to shift the end (defaults to *x*).\n\n    Returns:\n        A new shifted ``Alignment``.\n    \"\"\"\n    return Alignment(\n        query=self.query, query_interval=self.query_interval, target=self.target,\n        interval=self.interval.shift(x, y),\n        query_length=self.query_length, target_length=self.target_length, length=self.length,\n        cigar=self.cigar, n_matches=self.n_matches, quality=self.quality,\n        qualifiers=list(self.qualifiers), score=self.score\n    )\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Alignment.target_coverage","title":"<code>target_coverage()</code>","text":"<p>Returns the fraction of the target sequence covered by the alignment.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def target_coverage(self) -&gt; float:\n    \"\"\"Returns the fraction of the target sequence covered by the alignment.\"\"\"\n    return len(self.interval) / self.target_length if self.target_length &gt; 0 else 0.0\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch","title":"<code>AlignmentBatch</code>","text":"<p>               Bases: <code>Batch</code>, <code>HasIntervals</code></p> <p>High-performance columnar container for batches of pairwise alignments.</p> <p>Stores data in Structure-of-Arrays (SoA) layout for efficiency. Handles query/target IDs using integer indices and lookup arrays.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Structured numpy array containing all alignment fields.</p> <code>None</code> <code>cigars</code> <code>ndarray</code> <p>Object array of CIGAR strings.</p> <code>None</code> <code>qualifiers</code> <code>ndarray</code> <p>Object array of qualifier lists.</p> <code>None</code> <code>query_ids</code> <code>ndarray</code> <p>Array of unique query IDs.</p> <code>None</code> <code>target_ids</code> <code>ndarray</code> <p>Array of unique target IDs.</p> <code>None</code> Source code in <code>baclib/containers/alignment.py</code> <pre><code>class AlignmentBatch(Batch, HasIntervals):\n    \"\"\"\n    High-performance columnar container for batches of pairwise alignments.\n\n    Stores data in Structure-of-Arrays (SoA) layout for efficiency. Handles\n    query/target IDs using integer indices and lookup arrays.\n\n    Args:\n        data: Structured numpy array containing all alignment fields.\n        cigars: Object array of CIGAR strings.\n        qualifiers: Object array of qualifier lists.\n        query_ids: Array of unique query IDs.\n        target_ids: Array of unique target IDs.\n    \"\"\"\n\n    class Field(str, Enum):\n        Q_IDX = 'q_idx'\n        T_IDX = 't_idx'\n        SCORE = 'score'\n        Q_START = 'q_start'\n        Q_END = 'q_end'\n        Q_LEN = 'q_len'\n        Q_STRAND = 'q_strand'\n        T_START = 't_start'\n        T_END = 't_end'\n        T_LEN = 't_len'\n        T_STRAND = 't_strand'\n        MATCHES = 'matches'\n        QUALITY = 'quality'\n        ALN_LEN = 'aln_len'\n\n    _DTYPE = np.dtype([\n        (Field.Q_IDX, np.int32), (Field.T_IDX, np.int32), (Field.SCORE, np.float32),\n        (Field.Q_START, np.int32), (Field.Q_END, np.int32), (Field.Q_LEN, np.int32), (Field.Q_STRAND, np.int8),\n        (Field.T_START, np.int32), (Field.T_END, np.int32), (Field.T_LEN, np.int32), (Field.T_STRAND, np.int8),\n        (Field.MATCHES, np.int32), (Field.QUALITY, np.uint8), (Field.ALN_LEN, np.int32)\n    ])\n\n    __slots__ = ('_data', '_cigars', '_qualifiers', 'query_ids', 'target_ids', '_spatial_indices')\n\n    def __init__(self, data: np.ndarray = None, cigars: np.ndarray = None, qualifiers: np.ndarray = None,\n                 query_ids: np.ndarray = None, target_ids: np.ndarray = None):\n        \"\"\"\n        Direct initialization. Prefer factory methods (from_hits, etc.) for ease of use.\n        \"\"\"\n        if data is None:\n            self._data = np.zeros(0, dtype=self._DTYPE)\n            self._cigars = np.zeros(0, dtype=object)\n            self._qualifiers = np.zeros(0, dtype=object)\n            # Default empty IDs arrays should be empty\n            if query_ids is None: self.query_ids = np.empty(0, dtype='S1')\n            if target_ids is None: self.target_ids = np.empty(0, dtype='S1')\n        else:\n            self._data = data\n            n = len(data)\n            self._cigars = cigars if cigars is not None else np.full(n, None, dtype=object)\n            self._qualifiers = qualifiers if qualifiers is not None else np.full(n, None, dtype=object)\n            self.query_ids = query_ids\n            self.target_ids = target_ids\n\n        self._spatial_indices: dict[int, IntervalBatch] = {}\n\n    @classmethod\n    def from_data(cls,\n                  q_idx: np.ndarray, t_idx: np.ndarray, score: np.ndarray,\n                  q_coords: np.ndarray, t_coords: np.ndarray,\n                  q_lens: np.ndarray, t_lens: np.ndarray,\n                  cigars: np.ndarray = None,\n                  q_strands: np.ndarray = None, t_strands: np.ndarray = None):\n        \"\"\"Zero-copy construction from raw Aligner output arrays.\"\"\"\n        n = len(q_idx)\n        data = np.zeros(n, dtype=cls._DTYPE)\n\n        data[cls.Field.Q_IDX] = q_idx\n        data[cls.Field.T_IDX] = t_idx\n        data[cls.Field.SCORE] = score\n        data[cls.Field.Q_START] = q_coords[:, 0]\n        data[cls.Field.Q_END] = q_coords[:, 1]\n        data[cls.Field.T_START] = t_coords[:, 0]\n        data[cls.Field.T_END] = t_coords[:, 1]\n        data[cls.Field.Q_LEN] = q_lens\n        data[cls.Field.T_LEN] = t_lens\n\n        data[cls.Field.Q_STRAND] = q_strands if q_strands is not None else 1\n        data[cls.Field.T_STRAND] = t_strands if t_strands is not None else 1\n        data[cls.Field.ALN_LEN] = np.maximum(\n            data[cls.Field.Q_END] - data[cls.Field.Q_START],\n            data[cls.Field.T_END] - data[cls.Field.T_START]\n        )\n        data[cls.Field.MATCHES] = (data[cls.Field.ALN_LEN] * 0.9).astype(np.int32)\n        data[cls.Field.QUALITY] = 60\n\n        return cls(data=data, cigars=cigars)\n\n    @classmethod\n    def build(cls, alignments: Iterable[Alignment]) -&gt; 'AlignmentBatch':\n        \"\"\"\n        Creates an AlignmentBatch from an iterable of Alignment objects.\n\n        Args:\n            alignments: An iterable of ``Alignment`` objects.\n\n        Returns:\n            A new ``AlignmentBatch``.\n        \"\"\"\n        # Avoid copy if already a list/tuple\n        if isinstance(alignments, (list, tuple)):\n            items = alignments\n        else:\n            items = list(alignments)\n\n        n = len(items)\n        if n == 0: return cls()\n\n        # Handle identifiers vs indices\n        first = items[0]\n\n        # Pass 1: Collect columns (Faster than row-by-row tuple construction)\n        q_raw = []\n        t_raw = []\n\n        # Pre-allocate lists for speed\n        q_starts, q_ends, q_lens, q_strands = [], [], [], []\n        t_starts, t_ends, t_lens, t_strands = [], [], [], []\n        scores, matches, qualities, aln_lens = [], [], [], []\n        cigars_list, qualifiers_list = [], []\n\n        for x in items:\n            q_raw.append(x.query)\n            t_raw.append(x.target)\n            cigars_list.append(x.cigar)\n            qualifiers_list.append(x.qualifiers)\n\n            q_starts.append(x.query_interval.start)\n            q_ends.append(x.query_interval.end)\n            q_lens.append(x.query_length)\n            q_strands.append(x.query_interval.strand)\n\n            t_starts.append(x.interval.start)\n            t_ends.append(x.interval.end)\n            t_lens.append(x.target_length)\n            t_strands.append(x.interval.strand)\n\n            scores.append(x.score)\n            matches.append(x.n_matches)\n            qualities.append(x.quality)\n            aln_lens.append(x.length)\n\n        # Pass 2: Build structured array\n        data = np.zeros(n, dtype=cls._DTYPE)\n\n        # Resolve IDs &amp; Indices\n        q_ids = None\n        if isinstance(first.query, (int, np.integer)): data[cls.Field.Q_IDX] = q_raw\n        else: q_ids, data[cls.Field.Q_IDX] = np.unique(q_raw, return_inverse=True)\n\n        t_ids = None\n        if isinstance(first.target, (int, np.integer)): data[cls.Field.T_IDX] = t_raw\n        else: t_ids, data[cls.Field.T_IDX] = np.unique(t_raw, return_inverse=True)\n\n        # Bulk fill (NumPy handles list-&gt;array conversion efficiently)\n        data[cls.Field.Q_START] = q_starts; data[cls.Field.Q_END] = q_ends; data[cls.Field.Q_LEN] = q_lens; data[cls.Field.Q_STRAND] = q_strands\n        data[cls.Field.T_START] = t_starts; data[cls.Field.T_END] = t_ends; data[cls.Field.T_LEN] = t_lens; data[cls.Field.T_STRAND] = t_strands\n        data[cls.Field.SCORE] = scores; data[cls.Field.MATCHES] = matches; data[cls.Field.QUALITY] = qualities; data[cls.Field.ALN_LEN] = aln_lens\n\n        cigars = np.array(cigars_list, dtype=object)\n        qualifiers = np.array(qualifiers_list, dtype=object)\n\n        return cls(data=data, cigars=cigars, qualifiers=qualifiers, query_ids=q_ids, target_ids=t_ids)\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'AlignmentBatch':\n        \"\"\"Creates a batch of *n* empty placeholder alignments.\"\"\"\n        return cls(\n            data=np.zeros(n, dtype=cls._DTYPE),\n            cigars=np.full(n, None, dtype=object),\n            qualifiers=np.full(n, None, dtype=object),\n            query_ids=np.empty(0, dtype='S1'),  # No ID mapping by default\n            target_ids=np.empty(0, dtype='S1')\n        )\n\n    @classmethod\n    def empty(cls) -&gt; 'AlignmentBatch':\n        \"\"\"Creates an empty AlignmentBatch.\"\"\"\n        return cls.zeros(0)\n\n    @property\n    def component(self): return Alignment\n\n    @classmethod\n    def concat(cls, batches: Iterable['AlignmentBatch']) -&gt; 'AlignmentBatch':\n        \"\"\"Concatenates multiple alignment batches.\"\"\"\n        batches = list(batches)\n        if not batches: return cls.zeros(0)\n\n        # 1. Structure of Arrays Concatenation (Fast)\n        data = np.concatenate([b._data for b in batches])\n        cigars = np.concatenate([b._cigars for b in batches])\n        qualifiers = np.concatenate([b._qualifiers for b in batches])\n\n        # 2. ID Mapping Resolution\n        # We need to unify the ID maps if they differ.\n        first = batches[0]\n        q_ids = first.query_ids\n        t_ids = first.target_ids\n\n        return cls(data, cigars, qualifiers, q_ids, t_ids)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\"\"\"\n        return self._data.nbytes + self._cigars.nbytes + self._qualifiers.nbytes\n\n    def copy(self) -&gt; 'AlignmentBatch':\n        \"\"\"Returns a deep copy of the batch.\"\"\"\n        return self.__class__(self._data.copy(), self._cigars.copy(), self._qualifiers.copy(), \n                              self.query_ids, self.target_ids)\n\n    def __len__(self): return len(self._data)\n    def __repr__(self): return f\"&lt;AlignmentBatch: {len(self)} alignments&gt;\"\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)): return self._make_alignment(item)\n        elif isinstance(item, (slice, np.ndarray, list)):\n            return AlignmentBatch(self._data[item], self._cigars[item], self._qualifiers[item],\n                                  query_ids=self.query_ids, target_ids=self.target_ids)\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n    @property\n    def scores(self) -&gt; np.ndarray: return self._data[self.Field.SCORE]\n    @property\n    def q_indices(self) -&gt; np.ndarray: return self._data[self.Field.Q_IDX]\n    @property\n    def t_indices(self) -&gt; np.ndarray: return self._data[self.Field.T_IDX]\n    @property\n    def q_coords(self) -&gt; np.ndarray:\n        return np.stack((self._data[self.Field.Q_START], self._data[self.Field.Q_END]), axis=1)\n    @property\n    def q_starts(self) -&gt; np.ndarray: return self._data[self.Field.Q_START]\n    @property\n    def q_ends(self) -&gt; np.ndarray: return self._data[self.Field.Q_END]\n    @property\n    def t_starts(self) -&gt; np.ndarray: return self._data[self.Field.T_START]\n    @property\n    def t_ends(self) -&gt; np.ndarray: return self._data[self.Field.T_END]\n    @property\n    def matches(self) -&gt; np.ndarray: return self._data[self.Field.MATCHES]\n    @property\n    def aln_lens(self) -&gt; np.ndarray: return self._data[self.Field.ALN_LEN]\n    @property\n    def q_lens(self) -&gt; np.ndarray: return self._data[self.Field.Q_LEN]\n    @property\n    def t_lens(self) -&gt; np.ndarray: return self._data[self.Field.T_LEN]\n    @property\n    def q_strands(self) -&gt; np.ndarray: return self._data[self.Field.Q_STRAND]\n    @property\n    def t_strands(self) -&gt; np.ndarray: return self._data[self.Field.T_STRAND]\n    @property\n    def cigars(self) -&gt; np.ndarray: return self._cigars\n\n    @property\n    def intervals(self) -&gt; IntervalBatch:\n        \"\"\"\n        Returns the target intervals as an IntervalBatch.\n        Enables efficient interoperability with IntervalBatch.from_features().\n        \"\"\"\n        return self.target.to_intervals(sort=False)\n\n    @property\n    def query(self) -&gt; AlignmentSide:\n        \"\"\"Returns a view of the Query side of the alignments.\"\"\"\n        ids = self.query_ids[self.q_indices] if self.query_ids is not None else self.q_indices\n        return AlignmentSide(\n            self.q_indices, self.q_starts, self.q_ends, \n            self.q_lens, self.q_strands, ids\n        )\n\n    @property\n    def target(self) -&gt; AlignmentSide:\n        \"\"\"Returns a view of the Target side of the alignments.\"\"\"\n        ids = self.target_ids[self.t_indices] if self.target_ids is not None else self.t_indices\n        return AlignmentSide(\n            self.t_indices, self.t_starts, self.t_ends, \n            self.t_lens, self.t_strands, ids\n        )\n\n    def filter(self, query_idxs: Iterable[int] = None, target_idxs: Iterable[int] = None) -&gt; 'AlignmentBatch':\n        \"\"\"\n        Filters alignments by query or target indices.\n\n        Args:\n            query_idxs: Iterable of internal query integer indices to keep.\n            target_idxs: Iterable of internal target integer indices to keep.\n\n        Returns:\n            A new filtered AlignmentBatch.\n        \"\"\"\n        mask = None\n        if query_idxs is not None:\n            # Create a boolean lookup table if range is reasonable, else hash set\n            # Assuming dense indices from SeqBatch:\n            q_set = np.array(list(query_idxs), dtype=np.int32)\n            mask = np.isin(self.q_indices, q_set)  # O(N) or O(N log M)\n\n        if target_idxs is not None:\n            t_set = np.array(list(target_idxs), dtype=np.int32)\n            t_mask = np.isin(self.t_indices, t_set)\n            mask = t_mask if mask is None else (mask &amp; t_mask)\n\n        return self[mask] if mask is not None else self[:]\n\n    def best(self, by_target: bool = False) -&gt; 'AlignmentBatch':\n        \"\"\"\n        Keeps only the best scoring alignment for each query (or target).\n\n        Args:\n            by_target: If True, keeps best per target.\n\n        Returns:\n            A new AlignmentBatch.\n        \"\"\"\n        indices = self.t_indices if by_target else self.q_indices\n        # To find max ID for array sizing, we scan once.\n        # This is safe because indices correspond to SeqBatch,\n        # so max(indices) &lt; len(batch).\n        max_id = np.max(indices) if len(indices) &gt; 0 else 0\n        best_indices = _best_hit_kernel(indices, self.scores, max_id + 1)\n        return self[np.sort(best_indices)]\n\n    def sort(self, by: str = 'score', ascending: bool = False) -&gt; 'AlignmentBatch':\n        \"\"\"\n        Sorts the batch.\n\n        Args:\n            by: Field to sort by ('score', 'query', 'target').\n            ascending: Sort order.\n\n        Returns:\n            A new sorted AlignmentBatch.\n        \"\"\"\n        # Now we map the 'by' string to our internal fields safely\n        try:\n            perm = np.argsort(self._data[self.Field(by)])\n        except ValueError: # Fallback for unknown fields or properties\n             raise ValueError(f\"Unknown sort key: {by}\")\n\n        if not ascending: perm = perm[::-1]\n        return self[perm]\n\n    def _make_alignment(self, idx: int) -&gt; Alignment:\n        r = self._data[idx]\n        q_idx = r[self.Field.Q_IDX]\n        t_idx = r[self.Field.T_IDX]\n\n        return Alignment(\n            query=self.query_ids[q_idx] if self.query_ids is not None else q_idx,\n            query_interval=Interval(r[self.Field.Q_START], r[self.Field.Q_END], r[self.Field.Q_STRAND]),\n            target=self.target_ids[t_idx] if self.target_ids is not None else t_idx,\n            interval=Interval(r[self.Field.T_START], r[self.Field.T_END], r[self.Field.T_STRAND]),\n            query_length=r[self.Field.Q_LEN], target_length=r[self.Field.T_LEN], length=r[self.Field.ALN_LEN],\n            cigar=self._cigars[idx], score=r[self.Field.SCORE], n_matches=r[self.Field.MATCHES],\n            quality=r[self.Field.QUALITY], qualifiers=self._qualifiers[idx]\n        )\n\n    def _get_spatial_index(self, target_idx: int) -&gt; IntervalBatch:\n        \"\"\"Retrieves or builds the spatial index for a target.\"\"\"\n        if (target_int_idx := self._spatial_indices.get(target_idx)) is None:\n            mask = self.t_indices == target_idx\n            # Optimization: Direct IntervalBatch construction from arrays\n            self._spatial_indices[target_idx] = IntervalBatch(\n                self.t_starts[mask], self.t_ends[mask], sort=True\n            )\n            return self._spatial_indices[target_idx]\n        return target_int_idx\n\n    def swap_sides(self) -&gt; 'AlignmentBatch':\n        \"\"\"\n        Returns a new collection with queries and targets swapped.\n        Vectorized operation.\n        \"\"\"\n        new_coll = copy.copy(self)\n\n        # Swap data columns\n        new_data = self._data.copy()\n        new_data[self.Field.Q_IDX] = self.t_indices\n        new_data[self.Field.T_IDX] = self.q_indices\n        new_data[self.Field.Q_START] = self.t_starts\n        new_data[self.Field.Q_END] = self.t_ends\n        new_data[self.Field.Q_LEN] = self.t_lens\n        new_data[self.Field.T_START] = self.q_starts\n        new_data[self.Field.T_END] = self.q_ends\n        new_data[self.Field.T_LEN] = self.q_lens\n        new_data[self.Field.Q_STRAND] = 1 # Reset query to forward reference\n        new_data[self.Field.T_STRAND] = self.q_strands * self.t_strands # Preserve relative orientation\n\n        new_coll._data = new_data\n        new_coll.query_ids = self.target_ids\n        new_coll.target_ids = self.query_ids\n        new_coll._graph = None  # Invalidate graph\n        new_coll._t_id_map = None\n        new_coll._spatial_indices = {}  # Invalidate indices\n        return new_coll\n\n    def merge_overlaps(self, target_idx: int, tolerance: int = 0) -&gt; IntervalBatch:\n        \"\"\"\n        Merges overlapping intervals on a target.\n\n        Args:\n            target_idx (int): The target sequence ID.\n            tolerance (int, optional): Gap tolerance for merging. Defaults to 0.\n\n        Returns:\n            IntervalBatch: An IntervalBatch of merged intervals.\n        \"\"\"\n        return self._get_spatial_index(target_idx).merge(tolerance=tolerance)\n\n    def group_by(self, by_target: bool = False) -&gt; Generator[tuple[int, np.ndarray], None, None]:\n        \"\"\"\n        Efficiently groups alignments by query or target.\n\n        Yields:\n            (id, indices): Tuple of the query/target ID and a numpy array of indices into the collection.\n        \"\"\"\n        indices = self.t_indices if by_target else self.q_indices\n        # Sort indices by the key column (stable sort)\n        perm = np.argsort(indices, kind='mergesort')\n        sorted_keys = indices[perm]\n        # Find unique keys and their start positions\n        unique_keys, start_indices = np.unique(sorted_keys, return_index=True)\n\n        for i, start in enumerate(start_indices):\n            end = start_indices[i + 1] if i + 1 &lt; len(start_indices) else len(perm)\n            yield unique_keys[i], perm[start:end]\n\n    def cull_overlaps(self, max_overlap_fraction: float = 0.1, key: str = 'score') -&gt; 'AlignmentBatch':\n        \"\"\"\n        Greedily removes alignments that overlap with higher-scoring alignments on the same target.\n\n        Args:\n            max_overlap_fraction (float, optional): Maximum allowed overlap fraction. Defaults to 0.1.\n            key (str, optional): Attribute to use for sorting. Defaults to 'score'.\n\n        Returns:\n            AlignmentBatch: A new filtered AlignmentCollection.\n        \"\"\"\n        col = self.Field(key)\n        kept_indices_list = []\n\n        # OPTIMIZATION: Global Sort -&gt; Chunking\n        if col in self._data.dtype.names and np.issubdtype(self._data[col].dtype, np.number):\n            # Global Sort: Target (Primary), Key Descending (Secondary)\n            # np.lexsort keys are (secondary, primary)\n            vals = self._data[col]\n            targets = self.t_indices\n            # Negate vals for descending sort\n            perm = np.lexsort((-vals, targets))\n\n            # Find groups on sorted array\n            t_sorted = targets[perm]\n            _, start_indices = np.unique(t_sorted, return_index=True)\n\n            # Pre-fetch arrays\n            s_arr = self.t_starts\n            e_arr = self.t_ends\n            l_arr = self.aln_lens\n\n            for i in range(len(start_indices)):\n                start = start_indices[i]\n                end = start_indices[i+1] if i + 1 &lt; len(start_indices) else len(perm)\n                group_indices = perm[start:end]\n\n                # Kernel\n                s = s_arr[group_indices]\n                e = e_arr[group_indices]\n                l = l_arr[group_indices]\n\n                keep_mask = _greedy_overlap_kernel(s, e, l, max_overlap_fraction)\n                kept_indices_list.append(group_indices[keep_mask])\n        else:\n            # Fallback: Object-based sort\n            for target_id, group_indices in self.group_by(by_target=True):\n                temp = [(self._make_alignment(i), i) for i in group_indices]\n                temp.sort(key=lambda x: getattr(x[0], key), reverse=True)\n                sorted_indices = [x[1] for x in temp]\n\n                s_coords = self.t_starts[sorted_indices]\n                e_coords = self.t_ends[sorted_indices]\n                lengths = self.aln_lens[sorted_indices]\n                keep_mask = _greedy_overlap_kernel(s_coords, e_coords, lengths, max_overlap_fraction)\n                kept_indices_list.append(sorted_indices[keep_mask])\n\n        # Return a new filtered collection\n        if not kept_indices_list: return self[np.array([], dtype=np.int32)]\n\n        # We sort indices to maintain stable order relative to input\n        return self[np.sort(np.concatenate(kept_indices_list))]\n\n    def chain(self, max_gap: int = 1000) -&gt; 'AlignmentBatch':\n        \"\"\"\n        Performs Synteny Chaining (Collinear Chaining) to reconstruct fragmented alignments.\n\n        For each (Query, Target) pair, it finds the optimal chain of local alignments \n        that maximizes the total score while maintaining collinearity.\n\n        Args:\n            max_gap (int): Maximum allowed gap between chained alignments.\n\n        Returns:\n            AlignmentBatch: A new collection containing only the alignments that form the best chains.\n        \"\"\"\n        # OPTIMIZATION: Global Sort: Target (Primary), Query (Secondary), Q_Start (Tertiary)\n        # lexsort keys: (tertiary, secondary, primary)\n        # We also need to group by Strand to handle reverse chains correctly\n        perm = np.lexsort((self.q_starts, self.t_strands, self.q_indices, self.t_indices))\n\n        t_sorted = self.t_indices[perm]\n        q_sorted = self.q_indices[perm]\n        st_sorted = self.t_strands[perm]\n\n        # Pack for fast grouping\n        # We detect boundaries where T, Q, or Strand changes\n        diff = (t_sorted[:-1] != t_sorted[1:]) | (q_sorted[:-1] != q_sorted[1:]) | (st_sorted[:-1] != st_sorted[1:])\n        start_indices = np.concatenate(([0], np.flatnonzero(diff) + 1))\n\n        kept_indices = []\n\n        # Pre-fetch arrays\n        q_s_arr = self.q_starts\n        q_e_arr = self.q_ends\n        t_s_arr = self.t_starts\n        t_e_arr = self.t_ends\n        scores_arr = self.scores\n\n        for i in range(len(start_indices)):\n            start = start_indices[i]\n            end = start_indices[i+1] if i + 1 &lt; len(start_indices) else len(perm)\n\n            group_indices = perm[start:end]\n            if len(group_indices) == 1: \n                kept_indices.append(group_indices)\n                continue\n\n            # Extract data (already sorted by q_start due to global sort)\n            q_s = q_s_arr[group_indices]\n            q_e = q_e_arr[group_indices]\n            t_s = t_s_arr[group_indices]\n            t_e = t_e_arr[group_indices]\n            scores = scores_arr[group_indices]\n\n            # Run DP Kernel (Replaces Graph/Bellman-Ford)\n            # Solves Longest Path in DAG directly on arrays\n            t_st = st_sorted[start]\n\n            if t_st == -1:\n                # Reverse strand chaining: Invert coordinates to enforce collinearity\n                # Gap = t_s'[i] - t_e'[j] = (-t_e[i]) - (-t_s[j]) = t_s[j] - t_e[i]\n                pred, best_idx = _chain_dp_kernel(q_s, q_e, -t_e, -t_s, scores, max_gap)\n            else:\n                pred, best_idx = _chain_dp_kernel(q_s, q_e, t_s, t_e, scores, max_gap)\n\n            if best_idx == -1:\n                # Fallback: keep best single alignment\n                best_idx = np.argmax(scores)\n                kept_indices.append(group_indices[best_idx:best_idx+1])\n            else:\n                # Backtrack\n                local_idxs = _backtrack_kernel(pred, best_idx)\n                kept_indices.append(group_indices[local_idxs])\n\n        if not kept_indices: return self[np.array([], dtype=np.int32)]\n        return self[np.sort(np.concatenate(kept_indices))]\n\n    def find_dovetails(self, by_target: bool = False, return_ids: bool = False, tolerance: int = 0) -&gt; Generator[\n        tuple[Union[int, bytes], list[tuple[Union[int, bytes], int]], list[tuple[Union[int, bytes], int]]], None, None]:\n        \"\"\"\n        Identifies potential dovetail connections based on alignment boundary conditions.\n\n        Args:\n            by_target (bool): If True, uses Targets as the pivot.\n            return_ids (bool): If True, returns identifiers (bytes) instead of integer indices.\n            tolerance (int): Tolerance for boundary checks (default 0).\n\n        Yields:\n            (pivot_id, starts_list, ends_list)\n            starts_list: List of (node_id, strand) where the alignment extends LEFT from the Pivot.\n            ends_list: List of (node_id, strand) where the alignment extends RIGHT from the Pivot.\n        \"\"\"\n        # 1. Define Pivot and Node columns\n        pivot = self.target if by_target else self.query\n        node = self.query if by_target else self.target\n\n        pivot_indices = pivot.indices\n        pivot_starts = pivot.starts\n        pivot_ends = pivot.ends\n        pivot_lens = pivot.lengths\n\n        # Relative strand is essential for correct graph orientation\n        # (q_strand * t_strand) gives the relative orientation regardless of which side is pivot\n        rel_strands = self.q_strands * self.t_strands\n\n        node_indices = node.indices\n\n        pivot_lookup = self.target_ids if by_target else self.query_ids\n        node_lookup = self.query_ids if by_target else self.target_ids\n\n        # 2. Group by Pivot\n        perm = np.argsort(pivot_indices, kind='mergesort')\n        p_sorted = pivot_indices[perm]\n        _, start_indices = np.unique(p_sorted, return_index=True)\n\n        # 3. Iterate Groups\n        for i in range(len(start_indices)):\n            start = start_indices[i]\n            end = start_indices[i + 1] if i + 1 &lt; len(start_indices) else len(perm)\n            group_indices = perm[start:end]\n\n            p_s = pivot_starts[group_indices]\n            p_e = pivot_ends[group_indices]\n            p_l = pivot_lens[group_indices]\n\n            # Get relative strands for this group\n            r_st = rel_strands[group_indices]\n\n            # Boundaries with tolerance\n            p_at_start = p_s &lt;= tolerance\n            p_at_end = p_e &gt;= (p_l - tolerance)\n\n            # Starts (Left Extension): Pivot Start connects to Node\n            starts_mask = p_at_start\n\n            # Ends (Right Extension): Pivot End connects to Node\n            ends_mask = p_at_end\n\n            if not np.any(starts_mask) and not np.any(ends_mask):\n                continue\n\n            # Get Pivot ID\n            p_idx = pivot_indices[group_indices[0]]\n            p_out = pivot_lookup[p_idx] if return_ids and pivot_lookup is not None else p_idx\n\n            # Helper to extract Node IDs and Strands\n            def get_nodes(mask):\n                if not np.any(mask): return []\n                idxs = group_indices[mask]\n                n_idxs = node_indices[idxs]\n                n_st = r_st[mask] # Use relative strand\n\n                if return_ids and node_lookup is not None:\n                    ids = [node_lookup[x] for x in n_idxs]\n                    return list(zip(ids, n_st.tolist()))\n                return list(zip(n_idxs.tolist(), n_st.tolist()))\n\n            yield p_out, get_nodes(starts_mask), get_nodes(ends_mask)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.intervals","title":"<code>intervals</code>  <code>property</code>","text":"<p>Returns the target intervals as an IntervalBatch. Enables efficient interoperability with IntervalBatch.from_features().</p>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.query","title":"<code>query</code>  <code>property</code>","text":"<p>Returns a view of the Query side of the alignments.</p>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.target","title":"<code>target</code>  <code>property</code>","text":"<p>Returns a view of the Target side of the alignments.</p>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.__init__","title":"<code>__init__(data=None, cigars=None, qualifiers=None, query_ids=None, target_ids=None)</code>","text":"<p>Direct initialization. Prefer factory methods (from_hits, etc.) for ease of use.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def __init__(self, data: np.ndarray = None, cigars: np.ndarray = None, qualifiers: np.ndarray = None,\n             query_ids: np.ndarray = None, target_ids: np.ndarray = None):\n    \"\"\"\n    Direct initialization. Prefer factory methods (from_hits, etc.) for ease of use.\n    \"\"\"\n    if data is None:\n        self._data = np.zeros(0, dtype=self._DTYPE)\n        self._cigars = np.zeros(0, dtype=object)\n        self._qualifiers = np.zeros(0, dtype=object)\n        # Default empty IDs arrays should be empty\n        if query_ids is None: self.query_ids = np.empty(0, dtype='S1')\n        if target_ids is None: self.target_ids = np.empty(0, dtype='S1')\n    else:\n        self._data = data\n        n = len(data)\n        self._cigars = cigars if cigars is not None else np.full(n, None, dtype=object)\n        self._qualifiers = qualifiers if qualifiers is not None else np.full(n, None, dtype=object)\n        self.query_ids = query_ids\n        self.target_ids = target_ids\n\n    self._spatial_indices: dict[int, IntervalBatch] = {}\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.best","title":"<code>best(by_target=False)</code>","text":"<p>Keeps only the best scoring alignment for each query (or target).</p> <p>Parameters:</p> Name Type Description Default <code>by_target</code> <code>bool</code> <p>If True, keeps best per target.</p> <code>False</code> <p>Returns:</p> Type Description <code>AlignmentBatch</code> <p>A new AlignmentBatch.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def best(self, by_target: bool = False) -&gt; 'AlignmentBatch':\n    \"\"\"\n    Keeps only the best scoring alignment for each query (or target).\n\n    Args:\n        by_target: If True, keeps best per target.\n\n    Returns:\n        A new AlignmentBatch.\n    \"\"\"\n    indices = self.t_indices if by_target else self.q_indices\n    # To find max ID for array sizing, we scan once.\n    # This is safe because indices correspond to SeqBatch,\n    # so max(indices) &lt; len(batch).\n    max_id = np.max(indices) if len(indices) &gt; 0 else 0\n    best_indices = _best_hit_kernel(indices, self.scores, max_id + 1)\n    return self[np.sort(best_indices)]\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.build","title":"<code>build(alignments)</code>  <code>classmethod</code>","text":"<p>Creates an AlignmentBatch from an iterable of Alignment objects.</p> <p>Parameters:</p> Name Type Description Default <code>alignments</code> <code>Iterable[Alignment]</code> <p>An iterable of <code>Alignment</code> objects.</p> required <p>Returns:</p> Type Description <code>AlignmentBatch</code> <p>A new <code>AlignmentBatch</code>.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@classmethod\ndef build(cls, alignments: Iterable[Alignment]) -&gt; 'AlignmentBatch':\n    \"\"\"\n    Creates an AlignmentBatch from an iterable of Alignment objects.\n\n    Args:\n        alignments: An iterable of ``Alignment`` objects.\n\n    Returns:\n        A new ``AlignmentBatch``.\n    \"\"\"\n    # Avoid copy if already a list/tuple\n    if isinstance(alignments, (list, tuple)):\n        items = alignments\n    else:\n        items = list(alignments)\n\n    n = len(items)\n    if n == 0: return cls()\n\n    # Handle identifiers vs indices\n    first = items[0]\n\n    # Pass 1: Collect columns (Faster than row-by-row tuple construction)\n    q_raw = []\n    t_raw = []\n\n    # Pre-allocate lists for speed\n    q_starts, q_ends, q_lens, q_strands = [], [], [], []\n    t_starts, t_ends, t_lens, t_strands = [], [], [], []\n    scores, matches, qualities, aln_lens = [], [], [], []\n    cigars_list, qualifiers_list = [], []\n\n    for x in items:\n        q_raw.append(x.query)\n        t_raw.append(x.target)\n        cigars_list.append(x.cigar)\n        qualifiers_list.append(x.qualifiers)\n\n        q_starts.append(x.query_interval.start)\n        q_ends.append(x.query_interval.end)\n        q_lens.append(x.query_length)\n        q_strands.append(x.query_interval.strand)\n\n        t_starts.append(x.interval.start)\n        t_ends.append(x.interval.end)\n        t_lens.append(x.target_length)\n        t_strands.append(x.interval.strand)\n\n        scores.append(x.score)\n        matches.append(x.n_matches)\n        qualities.append(x.quality)\n        aln_lens.append(x.length)\n\n    # Pass 2: Build structured array\n    data = np.zeros(n, dtype=cls._DTYPE)\n\n    # Resolve IDs &amp; Indices\n    q_ids = None\n    if isinstance(first.query, (int, np.integer)): data[cls.Field.Q_IDX] = q_raw\n    else: q_ids, data[cls.Field.Q_IDX] = np.unique(q_raw, return_inverse=True)\n\n    t_ids = None\n    if isinstance(first.target, (int, np.integer)): data[cls.Field.T_IDX] = t_raw\n    else: t_ids, data[cls.Field.T_IDX] = np.unique(t_raw, return_inverse=True)\n\n    # Bulk fill (NumPy handles list-&gt;array conversion efficiently)\n    data[cls.Field.Q_START] = q_starts; data[cls.Field.Q_END] = q_ends; data[cls.Field.Q_LEN] = q_lens; data[cls.Field.Q_STRAND] = q_strands\n    data[cls.Field.T_START] = t_starts; data[cls.Field.T_END] = t_ends; data[cls.Field.T_LEN] = t_lens; data[cls.Field.T_STRAND] = t_strands\n    data[cls.Field.SCORE] = scores; data[cls.Field.MATCHES] = matches; data[cls.Field.QUALITY] = qualities; data[cls.Field.ALN_LEN] = aln_lens\n\n    cigars = np.array(cigars_list, dtype=object)\n    qualifiers = np.array(qualifiers_list, dtype=object)\n\n    return cls(data=data, cigars=cigars, qualifiers=qualifiers, query_ids=q_ids, target_ids=t_ids)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.chain","title":"<code>chain(max_gap=1000)</code>","text":"<p>Performs Synteny Chaining (Collinear Chaining) to reconstruct fragmented alignments.</p> <p>For each (Query, Target) pair, it finds the optimal chain of local alignments  that maximizes the total score while maintaining collinearity.</p> <p>Parameters:</p> Name Type Description Default <code>max_gap</code> <code>int</code> <p>Maximum allowed gap between chained alignments.</p> <code>1000</code> <p>Returns:</p> Name Type Description <code>AlignmentBatch</code> <code>AlignmentBatch</code> <p>A new collection containing only the alignments that form the best chains.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def chain(self, max_gap: int = 1000) -&gt; 'AlignmentBatch':\n    \"\"\"\n    Performs Synteny Chaining (Collinear Chaining) to reconstruct fragmented alignments.\n\n    For each (Query, Target) pair, it finds the optimal chain of local alignments \n    that maximizes the total score while maintaining collinearity.\n\n    Args:\n        max_gap (int): Maximum allowed gap between chained alignments.\n\n    Returns:\n        AlignmentBatch: A new collection containing only the alignments that form the best chains.\n    \"\"\"\n    # OPTIMIZATION: Global Sort: Target (Primary), Query (Secondary), Q_Start (Tertiary)\n    # lexsort keys: (tertiary, secondary, primary)\n    # We also need to group by Strand to handle reverse chains correctly\n    perm = np.lexsort((self.q_starts, self.t_strands, self.q_indices, self.t_indices))\n\n    t_sorted = self.t_indices[perm]\n    q_sorted = self.q_indices[perm]\n    st_sorted = self.t_strands[perm]\n\n    # Pack for fast grouping\n    # We detect boundaries where T, Q, or Strand changes\n    diff = (t_sorted[:-1] != t_sorted[1:]) | (q_sorted[:-1] != q_sorted[1:]) | (st_sorted[:-1] != st_sorted[1:])\n    start_indices = np.concatenate(([0], np.flatnonzero(diff) + 1))\n\n    kept_indices = []\n\n    # Pre-fetch arrays\n    q_s_arr = self.q_starts\n    q_e_arr = self.q_ends\n    t_s_arr = self.t_starts\n    t_e_arr = self.t_ends\n    scores_arr = self.scores\n\n    for i in range(len(start_indices)):\n        start = start_indices[i]\n        end = start_indices[i+1] if i + 1 &lt; len(start_indices) else len(perm)\n\n        group_indices = perm[start:end]\n        if len(group_indices) == 1: \n            kept_indices.append(group_indices)\n            continue\n\n        # Extract data (already sorted by q_start due to global sort)\n        q_s = q_s_arr[group_indices]\n        q_e = q_e_arr[group_indices]\n        t_s = t_s_arr[group_indices]\n        t_e = t_e_arr[group_indices]\n        scores = scores_arr[group_indices]\n\n        # Run DP Kernel (Replaces Graph/Bellman-Ford)\n        # Solves Longest Path in DAG directly on arrays\n        t_st = st_sorted[start]\n\n        if t_st == -1:\n            # Reverse strand chaining: Invert coordinates to enforce collinearity\n            # Gap = t_s'[i] - t_e'[j] = (-t_e[i]) - (-t_s[j]) = t_s[j] - t_e[i]\n            pred, best_idx = _chain_dp_kernel(q_s, q_e, -t_e, -t_s, scores, max_gap)\n        else:\n            pred, best_idx = _chain_dp_kernel(q_s, q_e, t_s, t_e, scores, max_gap)\n\n        if best_idx == -1:\n            # Fallback: keep best single alignment\n            best_idx = np.argmax(scores)\n            kept_indices.append(group_indices[best_idx:best_idx+1])\n        else:\n            # Backtrack\n            local_idxs = _backtrack_kernel(pred, best_idx)\n            kept_indices.append(group_indices[local_idxs])\n\n    if not kept_indices: return self[np.array([], dtype=np.int32)]\n    return self[np.sort(np.concatenate(kept_indices))]\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple alignment batches.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['AlignmentBatch']) -&gt; 'AlignmentBatch':\n    \"\"\"Concatenates multiple alignment batches.\"\"\"\n    batches = list(batches)\n    if not batches: return cls.zeros(0)\n\n    # 1. Structure of Arrays Concatenation (Fast)\n    data = np.concatenate([b._data for b in batches])\n    cigars = np.concatenate([b._cigars for b in batches])\n    qualifiers = np.concatenate([b._qualifiers for b in batches])\n\n    # 2. ID Mapping Resolution\n    # We need to unify the ID maps if they differ.\n    first = batches[0]\n    q_ids = first.query_ids\n    t_ids = first.target_ids\n\n    return cls(data, cigars, qualifiers, q_ids, t_ids)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of the batch.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def copy(self) -&gt; 'AlignmentBatch':\n    \"\"\"Returns a deep copy of the batch.\"\"\"\n    return self.__class__(self._data.copy(), self._cigars.copy(), self._qualifiers.copy(), \n                          self.query_ids, self.target_ids)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.cull_overlaps","title":"<code>cull_overlaps(max_overlap_fraction=0.1, key='score')</code>","text":"<p>Greedily removes alignments that overlap with higher-scoring alignments on the same target.</p> <p>Parameters:</p> Name Type Description Default <code>max_overlap_fraction</code> <code>float</code> <p>Maximum allowed overlap fraction. Defaults to 0.1.</p> <code>0.1</code> <code>key</code> <code>str</code> <p>Attribute to use for sorting. Defaults to 'score'.</p> <code>'score'</code> <p>Returns:</p> Name Type Description <code>AlignmentBatch</code> <code>AlignmentBatch</code> <p>A new filtered AlignmentCollection.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def cull_overlaps(self, max_overlap_fraction: float = 0.1, key: str = 'score') -&gt; 'AlignmentBatch':\n    \"\"\"\n    Greedily removes alignments that overlap with higher-scoring alignments on the same target.\n\n    Args:\n        max_overlap_fraction (float, optional): Maximum allowed overlap fraction. Defaults to 0.1.\n        key (str, optional): Attribute to use for sorting. Defaults to 'score'.\n\n    Returns:\n        AlignmentBatch: A new filtered AlignmentCollection.\n    \"\"\"\n    col = self.Field(key)\n    kept_indices_list = []\n\n    # OPTIMIZATION: Global Sort -&gt; Chunking\n    if col in self._data.dtype.names and np.issubdtype(self._data[col].dtype, np.number):\n        # Global Sort: Target (Primary), Key Descending (Secondary)\n        # np.lexsort keys are (secondary, primary)\n        vals = self._data[col]\n        targets = self.t_indices\n        # Negate vals for descending sort\n        perm = np.lexsort((-vals, targets))\n\n        # Find groups on sorted array\n        t_sorted = targets[perm]\n        _, start_indices = np.unique(t_sorted, return_index=True)\n\n        # Pre-fetch arrays\n        s_arr = self.t_starts\n        e_arr = self.t_ends\n        l_arr = self.aln_lens\n\n        for i in range(len(start_indices)):\n            start = start_indices[i]\n            end = start_indices[i+1] if i + 1 &lt; len(start_indices) else len(perm)\n            group_indices = perm[start:end]\n\n            # Kernel\n            s = s_arr[group_indices]\n            e = e_arr[group_indices]\n            l = l_arr[group_indices]\n\n            keep_mask = _greedy_overlap_kernel(s, e, l, max_overlap_fraction)\n            kept_indices_list.append(group_indices[keep_mask])\n    else:\n        # Fallback: Object-based sort\n        for target_id, group_indices in self.group_by(by_target=True):\n            temp = [(self._make_alignment(i), i) for i in group_indices]\n            temp.sort(key=lambda x: getattr(x[0], key), reverse=True)\n            sorted_indices = [x[1] for x in temp]\n\n            s_coords = self.t_starts[sorted_indices]\n            e_coords = self.t_ends[sorted_indices]\n            lengths = self.aln_lens[sorted_indices]\n            keep_mask = _greedy_overlap_kernel(s_coords, e_coords, lengths, max_overlap_fraction)\n            kept_indices_list.append(sorted_indices[keep_mask])\n\n    # Return a new filtered collection\n    if not kept_indices_list: return self[np.array([], dtype=np.int32)]\n\n    # We sort indices to maintain stable order relative to input\n    return self[np.sort(np.concatenate(kept_indices_list))]\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty AlignmentBatch.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'AlignmentBatch':\n    \"\"\"Creates an empty AlignmentBatch.\"\"\"\n    return cls.zeros(0)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.filter","title":"<code>filter(query_idxs=None, target_idxs=None)</code>","text":"<p>Filters alignments by query or target indices.</p> <p>Parameters:</p> Name Type Description Default <code>query_idxs</code> <code>Iterable[int]</code> <p>Iterable of internal query integer indices to keep.</p> <code>None</code> <code>target_idxs</code> <code>Iterable[int]</code> <p>Iterable of internal target integer indices to keep.</p> <code>None</code> <p>Returns:</p> Type Description <code>AlignmentBatch</code> <p>A new filtered AlignmentBatch.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def filter(self, query_idxs: Iterable[int] = None, target_idxs: Iterable[int] = None) -&gt; 'AlignmentBatch':\n    \"\"\"\n    Filters alignments by query or target indices.\n\n    Args:\n        query_idxs: Iterable of internal query integer indices to keep.\n        target_idxs: Iterable of internal target integer indices to keep.\n\n    Returns:\n        A new filtered AlignmentBatch.\n    \"\"\"\n    mask = None\n    if query_idxs is not None:\n        # Create a boolean lookup table if range is reasonable, else hash set\n        # Assuming dense indices from SeqBatch:\n        q_set = np.array(list(query_idxs), dtype=np.int32)\n        mask = np.isin(self.q_indices, q_set)  # O(N) or O(N log M)\n\n    if target_idxs is not None:\n        t_set = np.array(list(target_idxs), dtype=np.int32)\n        t_mask = np.isin(self.t_indices, t_set)\n        mask = t_mask if mask is None else (mask &amp; t_mask)\n\n    return self[mask] if mask is not None else self[:]\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.find_dovetails","title":"<code>find_dovetails(by_target=False, return_ids=False, tolerance=0)</code>","text":"<p>Identifies potential dovetail connections based on alignment boundary conditions.</p> <p>Parameters:</p> Name Type Description Default <code>by_target</code> <code>bool</code> <p>If True, uses Targets as the pivot.</p> <code>False</code> <code>return_ids</code> <code>bool</code> <p>If True, returns identifiers (bytes) instead of integer indices.</p> <code>False</code> <code>tolerance</code> <code>int</code> <p>Tolerance for boundary checks (default 0).</p> <code>0</code> <p>Yields:</p> Name Type Description <code>Union[int, bytes]</code> <p>(pivot_id, starts_list, ends_list)</p> <code>starts_list</code> <code>list[tuple[Union[int, bytes], int]]</code> <p>List of (node_id, strand) where the alignment extends LEFT from the Pivot.</p> <code>ends_list</code> <code>list[tuple[Union[int, bytes], int]]</code> <p>List of (node_id, strand) where the alignment extends RIGHT from the Pivot.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def find_dovetails(self, by_target: bool = False, return_ids: bool = False, tolerance: int = 0) -&gt; Generator[\n    tuple[Union[int, bytes], list[tuple[Union[int, bytes], int]], list[tuple[Union[int, bytes], int]]], None, None]:\n    \"\"\"\n    Identifies potential dovetail connections based on alignment boundary conditions.\n\n    Args:\n        by_target (bool): If True, uses Targets as the pivot.\n        return_ids (bool): If True, returns identifiers (bytes) instead of integer indices.\n        tolerance (int): Tolerance for boundary checks (default 0).\n\n    Yields:\n        (pivot_id, starts_list, ends_list)\n        starts_list: List of (node_id, strand) where the alignment extends LEFT from the Pivot.\n        ends_list: List of (node_id, strand) where the alignment extends RIGHT from the Pivot.\n    \"\"\"\n    # 1. Define Pivot and Node columns\n    pivot = self.target if by_target else self.query\n    node = self.query if by_target else self.target\n\n    pivot_indices = pivot.indices\n    pivot_starts = pivot.starts\n    pivot_ends = pivot.ends\n    pivot_lens = pivot.lengths\n\n    # Relative strand is essential for correct graph orientation\n    # (q_strand * t_strand) gives the relative orientation regardless of which side is pivot\n    rel_strands = self.q_strands * self.t_strands\n\n    node_indices = node.indices\n\n    pivot_lookup = self.target_ids if by_target else self.query_ids\n    node_lookup = self.query_ids if by_target else self.target_ids\n\n    # 2. Group by Pivot\n    perm = np.argsort(pivot_indices, kind='mergesort')\n    p_sorted = pivot_indices[perm]\n    _, start_indices = np.unique(p_sorted, return_index=True)\n\n    # 3. Iterate Groups\n    for i in range(len(start_indices)):\n        start = start_indices[i]\n        end = start_indices[i + 1] if i + 1 &lt; len(start_indices) else len(perm)\n        group_indices = perm[start:end]\n\n        p_s = pivot_starts[group_indices]\n        p_e = pivot_ends[group_indices]\n        p_l = pivot_lens[group_indices]\n\n        # Get relative strands for this group\n        r_st = rel_strands[group_indices]\n\n        # Boundaries with tolerance\n        p_at_start = p_s &lt;= tolerance\n        p_at_end = p_e &gt;= (p_l - tolerance)\n\n        # Starts (Left Extension): Pivot Start connects to Node\n        starts_mask = p_at_start\n\n        # Ends (Right Extension): Pivot End connects to Node\n        ends_mask = p_at_end\n\n        if not np.any(starts_mask) and not np.any(ends_mask):\n            continue\n\n        # Get Pivot ID\n        p_idx = pivot_indices[group_indices[0]]\n        p_out = pivot_lookup[p_idx] if return_ids and pivot_lookup is not None else p_idx\n\n        # Helper to extract Node IDs and Strands\n        def get_nodes(mask):\n            if not np.any(mask): return []\n            idxs = group_indices[mask]\n            n_idxs = node_indices[idxs]\n            n_st = r_st[mask] # Use relative strand\n\n            if return_ids and node_lookup is not None:\n                ids = [node_lookup[x] for x in n_idxs]\n                return list(zip(ids, n_st.tolist()))\n            return list(zip(n_idxs.tolist(), n_st.tolist()))\n\n        yield p_out, get_nodes(starts_mask), get_nodes(ends_mask)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.from_data","title":"<code>from_data(q_idx, t_idx, score, q_coords, t_coords, q_lens, t_lens, cigars=None, q_strands=None, t_strands=None)</code>  <code>classmethod</code>","text":"<p>Zero-copy construction from raw Aligner output arrays.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@classmethod\ndef from_data(cls,\n              q_idx: np.ndarray, t_idx: np.ndarray, score: np.ndarray,\n              q_coords: np.ndarray, t_coords: np.ndarray,\n              q_lens: np.ndarray, t_lens: np.ndarray,\n              cigars: np.ndarray = None,\n              q_strands: np.ndarray = None, t_strands: np.ndarray = None):\n    \"\"\"Zero-copy construction from raw Aligner output arrays.\"\"\"\n    n = len(q_idx)\n    data = np.zeros(n, dtype=cls._DTYPE)\n\n    data[cls.Field.Q_IDX] = q_idx\n    data[cls.Field.T_IDX] = t_idx\n    data[cls.Field.SCORE] = score\n    data[cls.Field.Q_START] = q_coords[:, 0]\n    data[cls.Field.Q_END] = q_coords[:, 1]\n    data[cls.Field.T_START] = t_coords[:, 0]\n    data[cls.Field.T_END] = t_coords[:, 1]\n    data[cls.Field.Q_LEN] = q_lens\n    data[cls.Field.T_LEN] = t_lens\n\n    data[cls.Field.Q_STRAND] = q_strands if q_strands is not None else 1\n    data[cls.Field.T_STRAND] = t_strands if t_strands is not None else 1\n    data[cls.Field.ALN_LEN] = np.maximum(\n        data[cls.Field.Q_END] - data[cls.Field.Q_START],\n        data[cls.Field.T_END] - data[cls.Field.T_START]\n    )\n    data[cls.Field.MATCHES] = (data[cls.Field.ALN_LEN] * 0.9).astype(np.int32)\n    data[cls.Field.QUALITY] = 60\n\n    return cls(data=data, cigars=cigars)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.group_by","title":"<code>group_by(by_target=False)</code>","text":"<p>Efficiently groups alignments by query or target.</p> <p>Yields:</p> Type Description <code>(id, indices)</code> <p>Tuple of the query/target ID and a numpy array of indices into the collection.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def group_by(self, by_target: bool = False) -&gt; Generator[tuple[int, np.ndarray], None, None]:\n    \"\"\"\n    Efficiently groups alignments by query or target.\n\n    Yields:\n        (id, indices): Tuple of the query/target ID and a numpy array of indices into the collection.\n    \"\"\"\n    indices = self.t_indices if by_target else self.q_indices\n    # Sort indices by the key column (stable sort)\n    perm = np.argsort(indices, kind='mergesort')\n    sorted_keys = indices[perm]\n    # Find unique keys and their start positions\n    unique_keys, start_indices = np.unique(sorted_keys, return_index=True)\n\n    for i, start in enumerate(start_indices):\n        end = start_indices[i + 1] if i + 1 &lt; len(start_indices) else len(perm)\n        yield unique_keys[i], perm[start:end]\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.merge_overlaps","title":"<code>merge_overlaps(target_idx, tolerance=0)</code>","text":"<p>Merges overlapping intervals on a target.</p> <p>Parameters:</p> Name Type Description Default <code>target_idx</code> <code>int</code> <p>The target sequence ID.</p> required <code>tolerance</code> <code>int</code> <p>Gap tolerance for merging. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>IntervalBatch</code> <code>IntervalBatch</code> <p>An IntervalBatch of merged intervals.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def merge_overlaps(self, target_idx: int, tolerance: int = 0) -&gt; IntervalBatch:\n    \"\"\"\n    Merges overlapping intervals on a target.\n\n    Args:\n        target_idx (int): The target sequence ID.\n        tolerance (int, optional): Gap tolerance for merging. Defaults to 0.\n\n    Returns:\n        IntervalBatch: An IntervalBatch of merged intervals.\n    \"\"\"\n    return self._get_spatial_index(target_idx).merge(tolerance=tolerance)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.sort","title":"<code>sort(by='score', ascending=False)</code>","text":"<p>Sorts the batch.</p> <p>Parameters:</p> Name Type Description Default <code>by</code> <code>str</code> <p>Field to sort by ('score', 'query', 'target').</p> <code>'score'</code> <code>ascending</code> <code>bool</code> <p>Sort order.</p> <code>False</code> <p>Returns:</p> Type Description <code>AlignmentBatch</code> <p>A new sorted AlignmentBatch.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def sort(self, by: str = 'score', ascending: bool = False) -&gt; 'AlignmentBatch':\n    \"\"\"\n    Sorts the batch.\n\n    Args:\n        by: Field to sort by ('score', 'query', 'target').\n        ascending: Sort order.\n\n    Returns:\n        A new sorted AlignmentBatch.\n    \"\"\"\n    # Now we map the 'by' string to our internal fields safely\n    try:\n        perm = np.argsort(self._data[self.Field(by)])\n    except ValueError: # Fallback for unknown fields or properties\n         raise ValueError(f\"Unknown sort key: {by}\")\n\n    if not ascending: perm = perm[::-1]\n    return self[perm]\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.swap_sides","title":"<code>swap_sides()</code>","text":"<p>Returns a new collection with queries and targets swapped. Vectorized operation.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def swap_sides(self) -&gt; 'AlignmentBatch':\n    \"\"\"\n    Returns a new collection with queries and targets swapped.\n    Vectorized operation.\n    \"\"\"\n    new_coll = copy.copy(self)\n\n    # Swap data columns\n    new_data = self._data.copy()\n    new_data[self.Field.Q_IDX] = self.t_indices\n    new_data[self.Field.T_IDX] = self.q_indices\n    new_data[self.Field.Q_START] = self.t_starts\n    new_data[self.Field.Q_END] = self.t_ends\n    new_data[self.Field.Q_LEN] = self.t_lens\n    new_data[self.Field.T_START] = self.q_starts\n    new_data[self.Field.T_END] = self.q_ends\n    new_data[self.Field.T_LEN] = self.q_lens\n    new_data[self.Field.Q_STRAND] = 1 # Reset query to forward reference\n    new_data[self.Field.T_STRAND] = self.q_strands * self.t_strands # Preserve relative orientation\n\n    new_coll._data = new_data\n    new_coll.query_ids = self.target_ids\n    new_coll.target_ids = self.query_ids\n    new_coll._graph = None  # Invalidate graph\n    new_coll._t_id_map = None\n    new_coll._spatial_indices = {}  # Invalidate indices\n    return new_coll\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n empty placeholder alignments.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'AlignmentBatch':\n    \"\"\"Creates a batch of *n* empty placeholder alignments.\"\"\"\n    return cls(\n        data=np.zeros(n, dtype=cls._DTYPE),\n        cigars=np.full(n, None, dtype=object),\n        qualifiers=np.full(n, None, dtype=object),\n        query_ids=np.empty(0, dtype='S1'),  # No ID mapping by default\n        target_ids=np.empty(0, dtype='S1')\n    )\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentSide","title":"<code>AlignmentSide</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>A view of one side (Query or Target) of an AlignmentBatch.</p> <p>Attributes:</p> Name Type Description <code>indices</code> <code>ndarray</code> <p>Indices into the ID array.</p> <code>starts</code> <code>ndarray</code> <p>Start coordinates.</p> <code>ends</code> <code>ndarray</code> <p>End coordinates.</p> <code>lengths</code> <code>ndarray</code> <p>Sequence lengths.</p> <code>strands</code> <code>ndarray</code> <p>Strand orientations.</p> <code>ids</code> <code>ndarray</code> <p>Resolved ID strings (bytes).</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>class AlignmentSide(NamedTuple):\n    \"\"\"\n    A view of one side (Query or Target) of an AlignmentBatch.\n\n    Attributes:\n        indices: Indices into the ID array.\n        starts: Start coordinates.\n        ends: End coordinates.\n        lengths: Sequence lengths.\n        strands: Strand orientations.\n        ids: Resolved ID strings (bytes).\n    \"\"\"\n    indices: np.ndarray\n    starts: np.ndarray\n    ends: np.ndarray\n    lengths: np.ndarray\n    strands: np.ndarray\n    ids: np.ndarray\n\n    @property\n    def coords(self) -&gt; np.ndarray:\n        \"\"\"Returns a (N, 2) array of [start, end] coordinates.\"\"\"\n        return np.stack((self.starts, self.ends), axis=1)\n\n    def to_intervals(self, sort: bool = True) -&gt; 'IntervalBatch':\n        \"\"\"Returns an IntervalBatch representing this side's intervals.\"\"\"\n        return IntervalBatch(self.starts, self.ends, self.strands, sort=sort)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentSide.coords","title":"<code>coords</code>  <code>property</code>","text":"<p>Returns a (N, 2) array of [start, end] coordinates.</p>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.AlignmentSide.to_intervals","title":"<code>to_intervals(sort=True)</code>","text":"<p>Returns an IntervalBatch representing this side's intervals.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>def to_intervals(self, sort: bool = True) -&gt; 'IntervalBatch':\n    \"\"\"Returns an IntervalBatch representing this side's intervals.\"\"\"\n    return IntervalBatch(self.starts, self.ends, self.strands, sort=sort)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Cigar","title":"<code>Cigar</code>","text":"<p>High-performance CIGAR string parser and builder.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>class Cigar:\n    \"\"\"\n    High-performance CIGAR string parser and builder.\n    \"\"\"\n    # Fast lookup for bytes -&gt; integer op codes\n    _OP_BYTES_LOOKUP = [b'M', b'I', b'D', b'N', b'S', b'H', b'P', b'=', b'X', b'B']\n\n    _BYTE_TO_OP = np.full(256, 255, dtype=np.uint8)\n    for op, sym in enumerate(_OP_BYTES_LOOKUP):\n        _BYTE_TO_OP[ord(sym)] = op\n\n    # Consumption logic\n    _QUERY_CONSUMERS = np.array([True, True, False, False, True, False, False, True, True, False], dtype=bool)\n    _TARGET_CONSUMERS = np.array([True, False, True, True, False, False, False, True, True, False], dtype=bool)\n    _ALN_CONSUMERS = _QUERY_CONSUMERS | _TARGET_CONSUMERS\n\n    @classmethod\n    def parse(cls, cigar: bytes) -&gt; Generator[tuple[bytes, int, int, int, int], None, None]:\n        \"\"\"\n        Parses a CIGAR string.\n        Yields: (op_char, count, query_consumed, target_consumed, aln_consumed).\n        \"\"\"\n        ops, counts = _parse_cigar_kernel(cigar, cls._BYTE_TO_OP)\n        q_len, t_len, aln_len = 0, 0, 0\n        lookup = cls._OP_BYTES_LOOKUP\n\n        for i in range(len(ops)):\n            op = ops[i]\n            n = counts[i]\n            if cls._QUERY_CONSUMERS[op]: q_len += n\n            if cls._TARGET_CONSUMERS[op]: t_len += n\n            if cls._ALN_CONSUMERS[op]: aln_len += n\n            yield lookup[op], n, q_len, t_len, aln_len\n\n    @classmethod\n    def parse_into_arrays(cls, cigar: bytes) -&gt; tuple[np.ndarray, np.ndarray]:\n        \"\"\"Parses CIGAR string directly into (ops, counts) arrays.\"\"\"\n        return _parse_cigar_kernel(cigar, cls._BYTE_TO_OP)\n\n    @staticmethod\n    def make(query: np.ndarray, target: np.ndarray, gap_code: int = 255, extended: bool = False) -&gt; bytes:\n        \"\"\"\n        Constructs a CIGAR string from aligned sequences.\n        \"\"\"\n        if len(query) == 0: return b\"\"\n        counts, ops = _cigar_rle_kernel(query, target, gap_code, extended)\n        return b\"\".join([b\"%d\" % c + Cigar._OP_BYTES_LOOKUP[o] for c, o in zip(counts, ops)])\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Cigar.make","title":"<code>make(query, target, gap_code=255, extended=False)</code>  <code>staticmethod</code>","text":"<p>Constructs a CIGAR string from aligned sequences.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@staticmethod\ndef make(query: np.ndarray, target: np.ndarray, gap_code: int = 255, extended: bool = False) -&gt; bytes:\n    \"\"\"\n    Constructs a CIGAR string from aligned sequences.\n    \"\"\"\n    if len(query) == 0: return b\"\"\n    counts, ops = _cigar_rle_kernel(query, target, gap_code, extended)\n    return b\"\".join([b\"%d\" % c + Cigar._OP_BYTES_LOOKUP[o] for c, o in zip(counts, ops)])\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Cigar.parse","title":"<code>parse(cigar)</code>  <code>classmethod</code>","text":"<p>Parses a CIGAR string. Yields: (op_char, count, query_consumed, target_consumed, aln_consumed).</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@classmethod\ndef parse(cls, cigar: bytes) -&gt; Generator[tuple[bytes, int, int, int, int], None, None]:\n    \"\"\"\n    Parses a CIGAR string.\n    Yields: (op_char, count, query_consumed, target_consumed, aln_consumed).\n    \"\"\"\n    ops, counts = _parse_cigar_kernel(cigar, cls._BYTE_TO_OP)\n    q_len, t_len, aln_len = 0, 0, 0\n    lookup = cls._OP_BYTES_LOOKUP\n\n    for i in range(len(ops)):\n        op = ops[i]\n        n = counts[i]\n        if cls._QUERY_CONSUMERS[op]: q_len += n\n        if cls._TARGET_CONSUMERS[op]: t_len += n\n        if cls._ALN_CONSUMERS[op]: aln_len += n\n        yield lookup[op], n, q_len, t_len, aln_len\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.Cigar.parse_into_arrays","title":"<code>parse_into_arrays(cigar)</code>  <code>classmethod</code>","text":"<p>Parses CIGAR string directly into (ops, counts) arrays.</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>@classmethod\ndef parse_into_arrays(cls, cigar: bytes) -&gt; tuple[np.ndarray, np.ndarray]:\n    \"\"\"Parses CIGAR string directly into (ops, counts) arrays.\"\"\"\n    return _parse_cigar_kernel(cigar, cls._BYTE_TO_OP)\n</code></pre>"},{"location":"reference/baclib/containers/alignment/#baclib.containers.alignment.CigarOp","title":"<code>CigarOp</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>CIGAR alignment operation codes (SAM specification).</p> Source code in <code>baclib/containers/alignment.py</code> <pre><code>class CigarOp(IntEnum):\n    \"\"\"CIGAR alignment operation codes (SAM specification).\"\"\"\n    M = 0\n    I = 1\n    D = 2\n    N = 3\n    S = 4\n    H = 5\n    P = 6\n    EQ = 7\n    X = 8\n    B = 9\n</code></pre>"},{"location":"reference/baclib/containers/cluster/","title":"cluster","text":""},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster","title":"<code>baclib.containers.cluster</code>","text":"<p>Container for sequence clusters and their batched representation.</p>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.Cluster","title":"<code>Cluster</code>","text":"<p>A group of related nodes (e.g. connected component, clique, or community).</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Iterable</code> <p>An iterable of member node IDs.</p> required <code>id_</code> <code>bytes</code> <p>Optional cluster identifier.</p> <code>None</code> <code>score</code> <code>float</code> <p>Optional cluster quality score (default <code>0.0</code>).</p> <code>0.0</code> <code>representative</code> <code>bytes</code> <p>Optional ID of the representative member.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; c = Cluster([b'seq1', b'seq2', b'seq3'], id_=b'cluster_0')\n&gt;&gt;&gt; len(c)\n3\n</code></pre> Source code in <code>baclib/containers/cluster.py</code> <pre><code>class Cluster:\n    \"\"\"\n    A group of related nodes (e.g. connected component, clique, or community).\n\n    Args:\n        nodes: An iterable of member node IDs.\n        id_: Optional cluster identifier.\n        score: Optional cluster quality score (default ``0.0``).\n        representative: Optional ID of the representative member.\n\n    Examples:\n        &gt;&gt;&gt; c = Cluster([b'seq1', b'seq2', b'seq3'], id_=b'cluster_0')\n        &gt;&gt;&gt; len(c)\n        3\n    \"\"\"\n    __slots__ = ('_members', 'id', 'score', 'representative')\n\n    def __init__(self, nodes: Iterable, id_: bytes = None, score: float = 0.0, representative: bytes = None):\n        self._members = list(nodes)\n        self.id = id_\n        self.score = score\n        self.representative = representative\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``ClusterBatch`` class.\n        \"\"\"\n        return ClusterBatch\n\n    def __len__(self): return len(self._members)\n\n    def __iter__(self): return iter(self._members)\n\n    def __getitem__(self, item): return self._members[item]\n\n    def __repr__(self):\n        rep = f\", rep={self.representative.decode()}\" if self.representative else \"\"\n        return f\"Cluster(size={len(self._members)}{rep})\"\n</code></pre>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.Cluster.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>ClusterBatch</code> class.</p>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch","title":"<code>ClusterBatch</code>","text":"<p>               Bases: <code>RaggedBatch</code></p> <p>Columnar batch of clusters using ragged array storage.</p> <p>Stores all member node IDs in a flat array with per-cluster offsets, scores, representative IDs, and cluster IDs.</p> <p>Parameters:</p> Name Type Description Default <code>flat_nodes</code> <code>ndarray</code> <p>Flat object array of all node IDs concatenated.</p> required <code>offsets</code> <code>ndarray</code> <p><code>int32</code> offset array (length = <code>n_clusters + 1</code>).</p> required <code>scores</code> <code>ndarray</code> <p>Optional <code>float32</code> array of per-cluster scores.</p> <code>None</code> <code>representatives</code> <code>ndarray</code> <p>Optional object array of representative node IDs.</p> <code>None</code> <code>ids</code> <code>ndarray</code> <p>Optional object array of cluster IDs.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = ClusterBatch.build([cluster1, cluster2])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/cluster.py</code> <pre><code>class ClusterBatch(RaggedBatch):\n    \"\"\"\n    Columnar batch of clusters using ragged array storage.\n\n    Stores all member node IDs in a flat array with per-cluster offsets,\n    scores, representative IDs, and cluster IDs.\n\n    Args:\n        flat_nodes: Flat object array of all node IDs concatenated.\n        offsets: ``int32`` offset array (length = ``n_clusters + 1``).\n        scores: Optional ``float32`` array of per-cluster scores.\n        representatives: Optional object array of representative node IDs.\n        ids: Optional object array of cluster IDs.\n\n    Examples:\n        &gt;&gt;&gt; batch = ClusterBatch.build([cluster1, cluster2])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    __slots__ = ('_flat_nodes', '_scores', '_representatives', '_ids')\n\n    def __init__(self, flat_nodes: np.ndarray, offsets: np.ndarray, scores: np.ndarray = None,\n                 representatives: np.ndarray = None, ids: np.ndarray = None):\n        super().__init__(offsets)\n        self._flat_nodes = flat_nodes\n        n = len(offsets) - 1\n        self._scores = scores if scores is not None else np.zeros(n, dtype=np.float32)\n        self._representatives = representatives if representatives is not None else np.full(n, b'', dtype='S1')\n        self._ids = ids if ids is not None else np.full(n, b'', dtype='S1')\n\n    @classmethod\n    def empty(cls) -&gt; 'ClusterBatch':\n        \"\"\"Creates an empty ClusterBatch with zero clusters.\n\n        Returns:\n            An empty ``ClusterBatch``.\n        \"\"\"\n        return cls.zeros(0)\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'ClusterBatch':\n        \"\"\"Creates a ClusterBatch with *n* empty placeholder clusters.\n\n        Args:\n            n: Number of placeholder cluster slots.\n\n        Returns:\n            A ``ClusterBatch`` with zero-score, empty clusters.\n        \"\"\"\n        return cls(\n            np.empty(0, dtype='S1'),\n            np.zeros(n + 1, dtype=np.int32)\n        )\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``Cluster`` class.\n        \"\"\"\n        return Cluster\n\n    @classmethod\n    def concat(cls, batches: Iterable['ClusterBatch']) -&gt; 'ClusterBatch':\n        \"\"\"Concatenates multiple ClusterBatch objects into one.\n\n        Args:\n            batches: An iterable of ``ClusterBatch`` objects.\n\n        Returns:\n            A single concatenated ``ClusterBatch``.\n        \"\"\"\n        batches = list(batches)\n        if not batches: return cls.empty()\n        flat_nodes = np.concatenate([b._flat_nodes for b in batches])\n        scores = np.concatenate([b._scores for b in batches])\n        reps = np.concatenate([b._representatives for b in batches])\n        ids = np.concatenate([b._ids for b in batches])\n        offsets = cls._stack_offsets(batches)\n        return cls(flat_nodes, offsets, scores, reps, ids)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by all internal arrays.\n        \"\"\"\n        return super().nbytes + self._flat_nodes.nbytes + self._scores.nbytes + self._representatives.nbytes + self._ids.nbytes\n\n    def copy(self) -&gt; 'ClusterBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``ClusterBatch`` with copied arrays.\n        \"\"\"\n        return self.__class__(self._flat_nodes.copy(), self._offsets.copy(), self._scores.copy(), self._representatives.copy(), self._ids.copy())\n\n    def __repr__(self):\n        return f\"&lt;ClusterBatch: {len(self)} clusters&gt;\"\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            start = self._offsets[item]\n            end = self._offsets[item + 1]\n            nodes = self._flat_nodes[start:end]\n            return Cluster(nodes, id_=self._ids[item], score=self._scores[item],\n                           representative=self._representatives[item])\n\n        if isinstance(item, slice):\n            new_offsets, val_start, val_end = self._get_slice_info(item)\n\n            return ClusterBatch(\n                self._flat_nodes[val_start:val_end],\n                new_offsets,\n                self._scores[item],\n                self._representatives[item],\n                self._ids[item]\n            )\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    @classmethod\n    def build(cls, components: Iterable['Cluster']) -&gt; 'ClusterBatch':\n        \"\"\"Constructs a ClusterBatch from an iterable of Cluster objects.\n\n        Args:\n            components: An iterable of ``Cluster`` objects.\n\n        Returns:\n            A new ``ClusterBatch``.\n\n        Examples:\n            &gt;&gt;&gt; batch = ClusterBatch.build([cluster1, cluster2])\n        \"\"\"\n        clusters = list(components)\n        n = len(clusters)\n        if n == 0:\n            return cls(np.empty(0, dtype='S1'), np.array([0], dtype=np.int32))\n\n        flat_nodes = []\n        offsets = [0]\n        curr = 0\n        for c in clusters:\n            flat_nodes.extend(c._members)\n            curr += len(c._members)\n            offsets.append(curr)\n\n        return cls(\n            np.array(flat_nodes),\n            np.array(offsets, dtype=np.int32),\n            np.array([c.score for c in clusters], dtype=np.float32),\n            np.array([c.representative or b'' for c in clusters]),\n            np.array([c.id or b'' for c in clusters])\n        )\n</code></pre>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>Cluster</code> class.</p>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by all internal arrays.</p>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Constructs a ClusterBatch from an iterable of Cluster objects.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>Iterable[Cluster]</code> <p>An iterable of <code>Cluster</code> objects.</p> required <p>Returns:</p> Type Description <code>ClusterBatch</code> <p>A new <code>ClusterBatch</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = ClusterBatch.build([cluster1, cluster2])\n</code></pre> Source code in <code>baclib/containers/cluster.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable['Cluster']) -&gt; 'ClusterBatch':\n    \"\"\"Constructs a ClusterBatch from an iterable of Cluster objects.\n\n    Args:\n        components: An iterable of ``Cluster`` objects.\n\n    Returns:\n        A new ``ClusterBatch``.\n\n    Examples:\n        &gt;&gt;&gt; batch = ClusterBatch.build([cluster1, cluster2])\n    \"\"\"\n    clusters = list(components)\n    n = len(clusters)\n    if n == 0:\n        return cls(np.empty(0, dtype='S1'), np.array([0], dtype=np.int32))\n\n    flat_nodes = []\n    offsets = [0]\n    curr = 0\n    for c in clusters:\n        flat_nodes.extend(c._members)\n        curr += len(c._members)\n        offsets.append(curr)\n\n    return cls(\n        np.array(flat_nodes),\n        np.array(offsets, dtype=np.int32),\n        np.array([c.score for c in clusters], dtype=np.float32),\n        np.array([c.representative or b'' for c in clusters]),\n        np.array([c.id or b'' for c in clusters])\n    )\n</code></pre>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple ClusterBatch objects into one.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[ClusterBatch]</code> <p>An iterable of <code>ClusterBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>ClusterBatch</code> <p>A single concatenated <code>ClusterBatch</code>.</p> Source code in <code>baclib/containers/cluster.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['ClusterBatch']) -&gt; 'ClusterBatch':\n    \"\"\"Concatenates multiple ClusterBatch objects into one.\n\n    Args:\n        batches: An iterable of ``ClusterBatch`` objects.\n\n    Returns:\n        A single concatenated ``ClusterBatch``.\n    \"\"\"\n    batches = list(batches)\n    if not batches: return cls.empty()\n    flat_nodes = np.concatenate([b._flat_nodes for b in batches])\n    scores = np.concatenate([b._scores for b in batches])\n    reps = np.concatenate([b._representatives for b in batches])\n    ids = np.concatenate([b._ids for b in batches])\n    offsets = cls._stack_offsets(batches)\n    return cls(flat_nodes, offsets, scores, reps, ids)\n</code></pre>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>ClusterBatch</code> <p>A new <code>ClusterBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/cluster.py</code> <pre><code>def copy(self) -&gt; 'ClusterBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``ClusterBatch`` with copied arrays.\n    \"\"\"\n    return self.__class__(self._flat_nodes.copy(), self._offsets.copy(), self._scores.copy(), self._representatives.copy(), self._ids.copy())\n</code></pre>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty ClusterBatch with zero clusters.</p> <p>Returns:</p> Type Description <code>ClusterBatch</code> <p>An empty <code>ClusterBatch</code>.</p> Source code in <code>baclib/containers/cluster.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'ClusterBatch':\n    \"\"\"Creates an empty ClusterBatch with zero clusters.\n\n    Returns:\n        An empty ``ClusterBatch``.\n    \"\"\"\n    return cls.zeros(0)\n</code></pre>"},{"location":"reference/baclib/containers/cluster/#baclib.containers.cluster.ClusterBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a ClusterBatch with n empty placeholder clusters.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of placeholder cluster slots.</p> required <p>Returns:</p> Type Description <code>ClusterBatch</code> <p>A <code>ClusterBatch</code> with zero-score, empty clusters.</p> Source code in <code>baclib/containers/cluster.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'ClusterBatch':\n    \"\"\"Creates a ClusterBatch with *n* empty placeholder clusters.\n\n    Args:\n        n: Number of placeholder cluster slots.\n\n    Returns:\n        A ``ClusterBatch`` with zero-score, empty clusters.\n    \"\"\"\n    return cls(\n        np.empty(0, dtype='S1'),\n        np.zeros(n + 1, dtype=np.int32)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/feature/","title":"feature","text":""},{"location":"reference/baclib/containers/feature/#baclib.containers.feature","title":"<code>baclib.containers.feature</code>","text":"<p>Containers for genomic features (genes, CDS, etc.) with interval-based coordinates and batch support.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature","title":"<code>Feature</code>","text":"<p>               Bases: <code>HasInterval</code>, <code>Batchable</code></p> <p>A single genomic feature with an interval, type key, and qualifiers.</p> <p>Features represent annotated regions of a sequence (genes, CDS, regulatory elements, etc.) and carry metadata as key-value qualifier pairs.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Interval</code> <p>Genomic coordinates and strand.</p> required <code>key</code> <code>Union[FeatureKey, bytes]</code> <p>The INSDC feature type. Accepts a <code>FeatureKey</code> enum or raw bytes.</p> <code>MISC_FEATURE</code> <code>qualifiers</code> <code>Iterable[tuple[bytes, QualifierType]]</code> <p>Optional iterable of <code>(key, value)</code> qualifier tuples.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feat = Feature(Interval(100, 500, 1), FeatureKey.CDS,\n...                [(b'gene', b'dnaA'), (b'product', b'initiator')])\n&gt;&gt;&gt; feat.key\n&lt;FeatureKey.CDS: 2&gt;\n&gt;&gt;&gt; feat[b'gene']\nb'dnaA'\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>class Feature(HasInterval, Batchable):\n    \"\"\"\n    A single genomic feature with an interval, type key, and qualifiers.\n\n    Features represent annotated regions of a sequence (genes, CDS, regulatory\n    elements, etc.) and carry metadata as key-value qualifier pairs.\n\n    Args:\n        interval: Genomic coordinates and strand.\n        key: The INSDC feature type. Accepts a ``FeatureKey`` enum or raw bytes.\n        qualifiers: Optional iterable of ``(key, value)`` qualifier tuples.\n\n    Examples:\n        &gt;&gt;&gt; feat = Feature(Interval(100, 500, 1), FeatureKey.CDS,\n        ...                [(b'gene', b'dnaA'), (b'product', b'initiator')])\n        &gt;&gt;&gt; feat.key\n        &lt;FeatureKey.CDS: 2&gt;\n        &gt;&gt;&gt; feat[b'gene']\n        b'dnaA'\n    \"\"\"\n    Key = FeatureKey  # Alias for convenience (e.g. Feature.Key.CDS)\n    __slots__ = ('_interval', '_key', '_qualifiers')\n    def __init__(self, interval: 'Interval', key: Union[FeatureKey, bytes] = FeatureKey.MISC_FEATURE,\n                 qualifiers: Iterable[tuple[bytes, QualifierType]] = None):\n        self._interval = interval\n        self._key = FeatureKey.from_bytes(key) if isinstance(key, bytes) else key\n        self._qualifiers = QualifierList(qualifiers)\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``FeatureBatch`` class.\n        \"\"\"\n        return FeatureBatch\n\n    @property\n    def interval(self) -&gt; 'Interval':\n        \"\"\"Returns the genomic interval (start, end, strand).\n\n        Returns:\n            The feature's ``Interval``.\n        \"\"\"\n        return self._interval\n\n    @property\n    def key(self) -&gt; FeatureKey:\n        \"\"\"Returns the INSDC feature type key.\n\n        Returns:\n            A ``FeatureKey`` enum member.\n        \"\"\"\n        return self._key\n\n    @property\n    def qualifiers(self) -&gt; QualifierList:\n        \"\"\"Returns the qualifier list for this feature.\n\n        Returns:\n            A ``QualifierList`` of ``(key, value)`` tuples.\n        \"\"\"\n        return self._qualifiers\n\n    def __len__(self) -&gt; int: return len(self.interval)\n    def __iter__(self): return self.interval.__iter__()\n    def __contains__(self, item) -&gt; bool: return self.interval.__contains__(item)\n    def __eq__(self, other):\n        if isinstance(other, self.__class__):\n            return (self.interval == other.interval and self.key == other.key and\n                    self.qualifiers == other.qualifiers)\n        return False\n    def __repr__(self): return f\"Feature({self.key.name}, {self.interval})\"\n    # Delegate dict-like access to qualifiers, but ensure we use .get() for lookups\n    # to avoid ambiguity with integer indices if Feature were to support them.\n    def __getitem__(self, item): return self.qualifiers.get(item)\n    def __setitem__(self, key: bytes, value: QualifierType): self.qualifiers[key] = value\n\n    def overlap(self, other) -&gt; int:\n        \"\"\"Returns the number of overlapping bases with another interval.\n\n        Args:\n            other: An ``Interval``, ``Feature``, or other ``HasInterval``.\n\n        Returns:\n            Number of overlapping bases (0 if none).\n        \"\"\"\n        return self.interval.overlap(other)\n\n    def get(self, item: bytes, default=None) -&gt; QualifierType:\n        \"\"\"Returns the first qualifier value for a key, or *default*.\n\n        Args:\n            item: The qualifier key to look up.\n            default: Value to return if key is absent.\n\n        Returns:\n            The first matching value, or *default*.\n\n        Examples:\n            &gt;&gt;&gt; feat.get(b'gene')\n            b'dnaA'\n        \"\"\"\n        return self.qualifiers.get(item, default)\n\n    def get_all(self, key: bytes) -&gt; list[QualifierType]:\n        \"\"\"Returns all qualifier values for a key.\n\n        Args:\n            key: The qualifier key to look up.\n\n        Returns:\n            A list of all matching values.\n        \"\"\"\n        return self.qualifiers.get_all(key)\n\n    def add_qualifier(self, key: bytes, value: QualifierType = True):\n        \"\"\"Appends a qualifier to this feature.\n\n        Args:\n            key: The qualifier key.\n            value: The qualifier value (defaults to ``True`` for flag qualifiers).\n\n        Examples:\n            &gt;&gt;&gt; feat.add_qualifier(b'pseudo')\n            &gt;&gt;&gt; feat[b'pseudo']\n            True\n        \"\"\"\n        self.qualifiers.add(key, value)\n\n    def shift(self, x: int, y: int = None) -&gt; 'Feature':\n        \"\"\"Returns a new Feature with shifted coordinates.\n\n        Args:\n            x: Offset to add to the start (and end, unless *y* is given).\n            y: Optional separate offset for the end.\n\n        Returns:\n            A new ``Feature`` with adjusted interval.\n        \"\"\"\n        return Feature(self.interval.shift(x, y), self.key, list(self.qualifiers))\n\n    def reverse_complement(self, parent_length: int) -&gt; 'Feature':\n        \"\"\"Returns a new Feature with reverse-complemented coordinates.\n\n        Args:\n            parent_length: Length of the parent sequence.\n\n        Returns:\n            A new ``Feature`` on the opposite strand.\n        \"\"\"\n        return Feature(self.interval.reverse_complement(parent_length), self.key, list(self.qualifiers))\n\n    def extract(self, parent_seq: Seq) -&gt; Seq:\n        \"\"\"Extracts the feature's sequence from the parent, respecting strand.\n\n        Args:\n            parent_seq: The full parent ``Seq`` to slice from.\n\n        Returns:\n            The extracted subsequence (reverse-complemented if on minus strand).\n\n        Examples:\n            &gt;&gt;&gt; seq = Alphabet.DNA.from_bytes(b'ATGCGATCGA')\n            &gt;&gt;&gt; feat = Feature(Interval(0, 3, 1), FeatureKey.CDS)\n            &gt;&gt;&gt; feat.extract(seq)  # returns Seq('ATG')\n        \"\"\"\n        sub = parent_seq[self.interval.start:self.interval.end]\n        if self.interval.strand == -1: return parent_seq.alphabet.reverse_complement(sub)\n        return sub\n\n    def copy(self) -&gt; 'Feature':\n        \"\"\"Creates a shallow copy of the feature (qualifiers are copied).\n\n        Returns:\n            A new ``Feature`` with the same interval, key, and copied qualifiers.\n        \"\"\"\n        return Feature(self.interval, self.key, list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>FeatureBatch</code> class.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.interval","title":"<code>interval</code>  <code>property</code>","text":"<p>Returns the genomic interval (start, end, strand).</p> <p>Returns:</p> Type Description <code>Interval</code> <p>The feature's <code>Interval</code>.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.key","title":"<code>key</code>  <code>property</code>","text":"<p>Returns the INSDC feature type key.</p> <p>Returns:</p> Type Description <code>FeatureKey</code> <p>A <code>FeatureKey</code> enum member.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.qualifiers","title":"<code>qualifiers</code>  <code>property</code>","text":"<p>Returns the qualifier list for this feature.</p> <p>Returns:</p> Type Description <code>QualifierList</code> <p>A <code>QualifierList</code> of <code>(key, value)</code> tuples.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.add_qualifier","title":"<code>add_qualifier(key, value=True)</code>","text":"<p>Appends a qualifier to this feature.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>bytes</code> <p>The qualifier key.</p> required <code>value</code> <code>QualifierType</code> <p>The qualifier value (defaults to <code>True</code> for flag qualifiers).</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feat.add_qualifier(b'pseudo')\n&gt;&gt;&gt; feat[b'pseudo']\nTrue\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>def add_qualifier(self, key: bytes, value: QualifierType = True):\n    \"\"\"Appends a qualifier to this feature.\n\n    Args:\n        key: The qualifier key.\n        value: The qualifier value (defaults to ``True`` for flag qualifiers).\n\n    Examples:\n        &gt;&gt;&gt; feat.add_qualifier(b'pseudo')\n        &gt;&gt;&gt; feat[b'pseudo']\n        True\n    \"\"\"\n    self.qualifiers.add(key, value)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.copy","title":"<code>copy()</code>","text":"<p>Creates a shallow copy of the feature (qualifiers are copied).</p> <p>Returns:</p> Type Description <code>Feature</code> <p>A new <code>Feature</code> with the same interval, key, and copied qualifiers.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def copy(self) -&gt; 'Feature':\n    \"\"\"Creates a shallow copy of the feature (qualifiers are copied).\n\n    Returns:\n        A new ``Feature`` with the same interval, key, and copied qualifiers.\n    \"\"\"\n    return Feature(self.interval, self.key, list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.extract","title":"<code>extract(parent_seq)</code>","text":"<p>Extracts the feature's sequence from the parent, respecting strand.</p> <p>Parameters:</p> Name Type Description Default <code>parent_seq</code> <code>Seq</code> <p>The full parent <code>Seq</code> to slice from.</p> required <p>Returns:</p> Type Description <code>Seq</code> <p>The extracted subsequence (reverse-complemented if on minus strand).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = Alphabet.DNA.from_bytes(b'ATGCGATCGA')\n&gt;&gt;&gt; feat = Feature(Interval(0, 3, 1), FeatureKey.CDS)\n&gt;&gt;&gt; feat.extract(seq)  # returns Seq('ATG')\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>def extract(self, parent_seq: Seq) -&gt; Seq:\n    \"\"\"Extracts the feature's sequence from the parent, respecting strand.\n\n    Args:\n        parent_seq: The full parent ``Seq`` to slice from.\n\n    Returns:\n        The extracted subsequence (reverse-complemented if on minus strand).\n\n    Examples:\n        &gt;&gt;&gt; seq = Alphabet.DNA.from_bytes(b'ATGCGATCGA')\n        &gt;&gt;&gt; feat = Feature(Interval(0, 3, 1), FeatureKey.CDS)\n        &gt;&gt;&gt; feat.extract(seq)  # returns Seq('ATG')\n    \"\"\"\n    sub = parent_seq[self.interval.start:self.interval.end]\n    if self.interval.strand == -1: return parent_seq.alphabet.reverse_complement(sub)\n    return sub\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.get","title":"<code>get(item, default=None)</code>","text":"<p>Returns the first qualifier value for a key, or default.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>bytes</code> <p>The qualifier key to look up.</p> required <code>default</code> <p>Value to return if key is absent.</p> <code>None</code> <p>Returns:</p> Type Description <code>QualifierType</code> <p>The first matching value, or default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; feat.get(b'gene')\nb'dnaA'\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>def get(self, item: bytes, default=None) -&gt; QualifierType:\n    \"\"\"Returns the first qualifier value for a key, or *default*.\n\n    Args:\n        item: The qualifier key to look up.\n        default: Value to return if key is absent.\n\n    Returns:\n        The first matching value, or *default*.\n\n    Examples:\n        &gt;&gt;&gt; feat.get(b'gene')\n        b'dnaA'\n    \"\"\"\n    return self.qualifiers.get(item, default)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.get_all","title":"<code>get_all(key)</code>","text":"<p>Returns all qualifier values for a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>bytes</code> <p>The qualifier key to look up.</p> required <p>Returns:</p> Type Description <code>list[QualifierType]</code> <p>A list of all matching values.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def get_all(self, key: bytes) -&gt; list[QualifierType]:\n    \"\"\"Returns all qualifier values for a key.\n\n    Args:\n        key: The qualifier key to look up.\n\n    Returns:\n        A list of all matching values.\n    \"\"\"\n    return self.qualifiers.get_all(key)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.overlap","title":"<code>overlap(other)</code>","text":"<p>Returns the number of overlapping bases with another interval.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <p>An <code>Interval</code>, <code>Feature</code>, or other <code>HasInterval</code>.</p> required <p>Returns:</p> Type Description <code>int</code> <p>Number of overlapping bases (0 if none).</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def overlap(self, other) -&gt; int:\n    \"\"\"Returns the number of overlapping bases with another interval.\n\n    Args:\n        other: An ``Interval``, ``Feature``, or other ``HasInterval``.\n\n    Returns:\n        Number of overlapping bases (0 if none).\n    \"\"\"\n    return self.interval.overlap(other)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.reverse_complement","title":"<code>reverse_complement(parent_length)</code>","text":"<p>Returns a new Feature with reverse-complemented coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>parent_length</code> <code>int</code> <p>Length of the parent sequence.</p> required <p>Returns:</p> Type Description <code>Feature</code> <p>A new <code>Feature</code> on the opposite strand.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def reverse_complement(self, parent_length: int) -&gt; 'Feature':\n    \"\"\"Returns a new Feature with reverse-complemented coordinates.\n\n    Args:\n        parent_length: Length of the parent sequence.\n\n    Returns:\n        A new ``Feature`` on the opposite strand.\n    \"\"\"\n    return Feature(self.interval.reverse_complement(parent_length), self.key, list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.Feature.shift","title":"<code>shift(x, y=None)</code>","text":"<p>Returns a new Feature with shifted coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>Offset to add to the start (and end, unless y is given).</p> required <code>y</code> <code>int</code> <p>Optional separate offset for the end.</p> <code>None</code> <p>Returns:</p> Type Description <code>Feature</code> <p>A new <code>Feature</code> with adjusted interval.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def shift(self, x: int, y: int = None) -&gt; 'Feature':\n    \"\"\"Returns a new Feature with shifted coordinates.\n\n    Args:\n        x: Offset to add to the start (and end, unless *y* is given).\n        y: Optional separate offset for the end.\n\n    Returns:\n        A new ``Feature`` with adjusted interval.\n    \"\"\"\n    return Feature(self.interval.shift(x, y), self.key, list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch","title":"<code>FeatureBatch</code>","text":"<p>               Bases: <code>Batch</code>, <code>HasIntervals</code></p> <p>A columnar batch of features for efficient bulk operations.</p> <p>Stores intervals, keys, and qualifiers in separate numpy arrays, enabling vectorized filtering and slicing.</p> <p>Parameters:</p> Name Type Description Default <code>intervals</code> <code>IntervalBatch</code> <p>An <code>IntervalBatch</code> of feature coordinates.</p> required <code>keys</code> <code>ndarray</code> <p>An <code>int16</code> numpy array of <code>FeatureKey</code> values.</p> required <code>qualifiers</code> <code>QualifierBatch</code> <p>A <code>QualifierBatch</code> of per-feature qualifiers.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = FeatureBatch.build([feat1, feat2, feat3])\n&gt;&gt;&gt; len(batch)\n3\n&gt;&gt;&gt; batch[0]  # reconstructs a Feature\nFeature(CDS, Interval(100, 500, +))\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>class FeatureBatch(Batch, HasIntervals):\n    \"\"\"\n    A columnar batch of features for efficient bulk operations.\n\n    Stores intervals, keys, and qualifiers in separate numpy arrays,\n    enabling vectorized filtering and slicing.\n\n    Args:\n        intervals: An ``IntervalBatch`` of feature coordinates.\n        keys: An ``int16`` numpy array of ``FeatureKey`` values.\n        qualifiers: A ``QualifierBatch`` of per-feature qualifiers.\n\n    Examples:\n        &gt;&gt;&gt; batch = FeatureBatch.build([feat1, feat2, feat3])\n        &gt;&gt;&gt; len(batch)\n        3\n        &gt;&gt;&gt; batch[0]  # reconstructs a Feature\n        Feature(CDS, Interval(100, 500, +))\n    \"\"\"\n    __slots__ = ('_intervals', '_keys', '_qualifiers')\n\n    def __init__(self, intervals: IntervalBatch, keys: np.ndarray, qualifiers: QualifierBatch):\n        self._intervals = intervals\n        self._keys = keys\n        self._qualifiers = qualifiers\n\n    @classmethod\n    def empty(cls) -&gt; 'FeatureBatch':\n        \"\"\"Creates an empty FeatureBatch with zero features.\n\n        Returns:\n            An empty ``FeatureBatch``.\n        \"\"\"\n        return cls(IntervalBatch.empty(), np.empty(0, dtype=np.int16), QualifierBatch.empty())\n\n    @classmethod\n    def build(cls, features: list[Feature]) -&gt; 'FeatureBatch':\n        \"\"\"Constructs a FeatureBatch from a list of Feature objects.\n\n        Args:\n            features: List of ``Feature`` objects to batch.\n\n        Returns:\n            A new ``FeatureBatch``.\n\n        Examples:\n            &gt;&gt;&gt; batch = FeatureBatch.build([feat1, feat2])\n            &gt;&gt;&gt; len(batch)\n            2\n        \"\"\"\n        n = len(features)\n\n        # Manual extraction to ensure sort=False (IntervalBatch.from_features sorts by default)\n        starts = np.empty(n, dtype=np.int32)\n        ends = np.empty(n, dtype=np.int32)\n        strands = np.empty(n, dtype=np.int32)\n        keys = np.empty(n, dtype=np.int16)\n\n        for i, f in enumerate(features):\n            iv = f.interval\n            starts[i] = iv.start\n            ends[i] = iv.end\n            strands[i] = iv.strand\n            keys[i] = f.key.value\n\n        intervals = IntervalBatch(starts, ends, strands, sort=False)\n\n        # Generator to avoid creating intermediate list of qualifier lists\n        qualifiers = QualifierBatch.build(f.qualifiers for f in features)\n        return cls(intervals, keys, qualifiers)\n\n    @classmethod\n    def concat(cls, batches: Iterable['FeatureBatch']) -&gt; 'FeatureBatch':\n        \"\"\"Concatenates multiple FeatureBatch objects into one.\n\n        Args:\n            batches: An iterable of ``FeatureBatch`` objects.\n\n        Returns:\n            A single concatenated ``FeatureBatch``.\n\n        Examples:\n            &gt;&gt;&gt; combined = FeatureBatch.concat([batch_a, batch_b])\n        \"\"\"\n        batches = list(batches)\n        if not batches: return cls.empty()\n\n        intervals = IntervalBatch.concat([b.intervals for b in batches])\n        keys = np.concatenate([b.keys for b in batches])\n        qualifiers = QualifierBatch.concat([b._qualifiers for b in batches])\n\n        return cls(intervals, keys, qualifiers)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by intervals, keys, and qualifiers.\n        \"\"\"\n        return self._intervals.nbytes + self._keys.nbytes + self._qualifiers.nbytes\n\n    def copy(self) -&gt; 'FeatureBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``FeatureBatch`` with copied arrays.\n        \"\"\"\n        return self.__class__(self._intervals.copy(), self._keys.copy(), self._qualifiers.copy())\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``Feature`` class.\n        \"\"\"\n        return Feature\n\n    @property\n    def intervals(self) -&gt; IntervalBatch:\n        \"\"\"Returns the interval array for all features.\n\n        Returns:\n            An ``IntervalBatch`` of feature coordinates.\n        \"\"\"\n        return self._intervals\n\n    @property\n    def keys(self) -&gt; np.ndarray:\n        \"\"\"Returns the raw ``int16`` array of feature key values.\n\n        Returns:\n            A numpy array of ``FeatureKey`` integer values.\n        \"\"\"\n        return self._keys\n\n    def get_key(self, idx: int) -&gt; FeatureKey:\n        \"\"\"Returns the FeatureKey for the feature at *idx*.\n\n        Args:\n            idx: Feature index.\n\n        Returns:\n            The ``FeatureKey`` enum member.\n\n        Examples:\n            &gt;&gt;&gt; batch.get_key(0)\n            &lt;FeatureKey.CDS: 2&gt;\n        \"\"\"\n        return FeatureKey(self._keys[idx])\n\n    def get_qualifiers(self, idx: int) -&gt; list[tuple]:\n        \"\"\"Returns the qualifier list for the feature at *idx*.\n\n        Args:\n            idx: Feature index.\n\n        Returns:\n            A list of ``(key, value)`` tuples.\n\n        Examples:\n            &gt;&gt;&gt; batch.get_qualifiers(0)\n            [(b'gene', b'dnaA'), (b'product', b'initiator')]\n        \"\"\"\n        return self._qualifiers[idx]\n\n    def __repr__(self): return f\"&lt;FeatureBatch: {len(self)} features&gt;\"\n    def __len__(self): return len(self._intervals)\n    def __getitem__(self, item) -&gt; Union['Feature', 'FeatureBatch']:\n        \"\"\"Reconstructs a Feature or slices a sub-batch.\n\n        Args:\n            item: Integer index (returns ``Feature``) or slice (returns ``FeatureBatch``).\n\n        Returns:\n            A single ``Feature`` or a sliced ``FeatureBatch``.\n        \"\"\"\n        if isinstance(item, (int, np.integer)):\n            interval = Interval(\n                self._intervals.starts[item],\n                self._intervals.ends[item],\n                self._intervals.strands[item]\n            )\n            return Feature(interval, self.get_key(item), self._qualifiers[item])\n\n        if isinstance(item, slice):\n            # Slice components\n            new_intervals = self._intervals[item]\n            new_keys = self._keys[item]\n            new_quals = self._qualifiers[item]\n\n            obj = object.__new__(FeatureBatch)\n            obj._intervals = new_intervals\n            obj._keys = new_keys\n            obj._qualifiers = new_quals\n            return obj\n\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    @classmethod\n    def random(cls, n: int, rng: np.random.Generator = None, length: int = None, min_len: int = 1, max_len: int = 1000,\n               min_start: int = 0, max_start: int = 1_000_000) -&gt; 'FeatureBatch':\n        \"\"\"Creates a batch of *n* random features for testing.\n\n        Args:\n            n: Number of features to generate.\n            rng: Random number generator (optional).\n            length: Fixed length for all features (optional).\n            min_len: Minimum feature length (default 1).\n            max_len: Maximum feature length (default 1000).\n            min_start: Minimum start coordinate (default 0).\n            max_start: Maximum start coordinate (default 1,000,000).\n\n        Returns:\n            A new ``FeatureBatch`` with random keys and intervals.\n\n        Examples:\n            &gt;&gt;&gt; batch = FeatureBatch.random(10, rng=np.random.default_rng(42))\n            &gt;&gt;&gt; len(batch)\n            10\n        \"\"\"\n        intervals = IntervalBatch.random(n, rng, length, min_len, max_len, min_start, max_start)\n        if rng is None: rng = np.random.default_rng()\n\n        # Select random keys from available FeatureKey values\n        valid_keys = np.array([k.value for k in FeatureKey], dtype=np.int16)\n        keys = rng.choice(valid_keys, size=n)\n\n        qualifiers = QualifierBatch.zeros(n)\n\n        return cls(intervals, keys, qualifiers)\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'FeatureBatch':\n        \"\"\"Creates a batch of *n* zero-length placeholder features.\n\n        Args:\n            n: Number of features to create.\n\n        Returns:\n            A ``FeatureBatch`` where every feature has interval ``(0, 0)``\n            and key ``0``.\n\n        Examples:\n            &gt;&gt;&gt; batch = FeatureBatch.zeros(3)\n            &gt;&gt;&gt; len(batch)\n            3\n        \"\"\"\n        return cls(\n            IntervalBatch.build([Interval(0, 0)] * n),\n            np.zeros(n, dtype=np.int16),\n            QualifierBatch.zeros(n)\n        )\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>Feature</code> class.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.intervals","title":"<code>intervals</code>  <code>property</code>","text":"<p>Returns the interval array for all features.</p> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An <code>IntervalBatch</code> of feature coordinates.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.keys","title":"<code>keys</code>  <code>property</code>","text":"<p>Returns the raw <code>int16</code> array of feature key values.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of <code>FeatureKey</code> integer values.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by intervals, keys, and qualifiers.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Reconstructs a Feature or slices a sub-batch.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <p>Integer index (returns <code>Feature</code>) or slice (returns <code>FeatureBatch</code>).</p> required <p>Returns:</p> Type Description <code>Union[Feature, FeatureBatch]</code> <p>A single <code>Feature</code> or a sliced <code>FeatureBatch</code>.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def __getitem__(self, item) -&gt; Union['Feature', 'FeatureBatch']:\n    \"\"\"Reconstructs a Feature or slices a sub-batch.\n\n    Args:\n        item: Integer index (returns ``Feature``) or slice (returns ``FeatureBatch``).\n\n    Returns:\n        A single ``Feature`` or a sliced ``FeatureBatch``.\n    \"\"\"\n    if isinstance(item, (int, np.integer)):\n        interval = Interval(\n            self._intervals.starts[item],\n            self._intervals.ends[item],\n            self._intervals.strands[item]\n        )\n        return Feature(interval, self.get_key(item), self._qualifiers[item])\n\n    if isinstance(item, slice):\n        # Slice components\n        new_intervals = self._intervals[item]\n        new_keys = self._keys[item]\n        new_quals = self._qualifiers[item]\n\n        obj = object.__new__(FeatureBatch)\n        obj._intervals = new_intervals\n        obj._keys = new_keys\n        obj._qualifiers = new_quals\n        return obj\n\n    raise TypeError(f\"Invalid index type: {type(item)}\")\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.build","title":"<code>build(features)</code>  <code>classmethod</code>","text":"<p>Constructs a FeatureBatch from a list of Feature objects.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>list[Feature]</code> <p>List of <code>Feature</code> objects to batch.</p> required <p>Returns:</p> Type Description <code>FeatureBatch</code> <p>A new <code>FeatureBatch</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = FeatureBatch.build([feat1, feat2])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>@classmethod\ndef build(cls, features: list[Feature]) -&gt; 'FeatureBatch':\n    \"\"\"Constructs a FeatureBatch from a list of Feature objects.\n\n    Args:\n        features: List of ``Feature`` objects to batch.\n\n    Returns:\n        A new ``FeatureBatch``.\n\n    Examples:\n        &gt;&gt;&gt; batch = FeatureBatch.build([feat1, feat2])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    n = len(features)\n\n    # Manual extraction to ensure sort=False (IntervalBatch.from_features sorts by default)\n    starts = np.empty(n, dtype=np.int32)\n    ends = np.empty(n, dtype=np.int32)\n    strands = np.empty(n, dtype=np.int32)\n    keys = np.empty(n, dtype=np.int16)\n\n    for i, f in enumerate(features):\n        iv = f.interval\n        starts[i] = iv.start\n        ends[i] = iv.end\n        strands[i] = iv.strand\n        keys[i] = f.key.value\n\n    intervals = IntervalBatch(starts, ends, strands, sort=False)\n\n    # Generator to avoid creating intermediate list of qualifier lists\n    qualifiers = QualifierBatch.build(f.qualifiers for f in features)\n    return cls(intervals, keys, qualifiers)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple FeatureBatch objects into one.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[FeatureBatch]</code> <p>An iterable of <code>FeatureBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>FeatureBatch</code> <p>A single concatenated <code>FeatureBatch</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combined = FeatureBatch.concat([batch_a, batch_b])\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['FeatureBatch']) -&gt; 'FeatureBatch':\n    \"\"\"Concatenates multiple FeatureBatch objects into one.\n\n    Args:\n        batches: An iterable of ``FeatureBatch`` objects.\n\n    Returns:\n        A single concatenated ``FeatureBatch``.\n\n    Examples:\n        &gt;&gt;&gt; combined = FeatureBatch.concat([batch_a, batch_b])\n    \"\"\"\n    batches = list(batches)\n    if not batches: return cls.empty()\n\n    intervals = IntervalBatch.concat([b.intervals for b in batches])\n    keys = np.concatenate([b.keys for b in batches])\n    qualifiers = QualifierBatch.concat([b._qualifiers for b in batches])\n\n    return cls(intervals, keys, qualifiers)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>FeatureBatch</code> <p>A new <code>FeatureBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def copy(self) -&gt; 'FeatureBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``FeatureBatch`` with copied arrays.\n    \"\"\"\n    return self.__class__(self._intervals.copy(), self._keys.copy(), self._qualifiers.copy())\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty FeatureBatch with zero features.</p> <p>Returns:</p> Type Description <code>FeatureBatch</code> <p>An empty <code>FeatureBatch</code>.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'FeatureBatch':\n    \"\"\"Creates an empty FeatureBatch with zero features.\n\n    Returns:\n        An empty ``FeatureBatch``.\n    \"\"\"\n    return cls(IntervalBatch.empty(), np.empty(0, dtype=np.int16), QualifierBatch.empty())\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.get_key","title":"<code>get_key(idx)</code>","text":"<p>Returns the FeatureKey for the feature at idx.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Feature index.</p> required <p>Returns:</p> Type Description <code>FeatureKey</code> <p>The <code>FeatureKey</code> enum member.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch.get_key(0)\n&lt;FeatureKey.CDS: 2&gt;\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>def get_key(self, idx: int) -&gt; FeatureKey:\n    \"\"\"Returns the FeatureKey for the feature at *idx*.\n\n    Args:\n        idx: Feature index.\n\n    Returns:\n        The ``FeatureKey`` enum member.\n\n    Examples:\n        &gt;&gt;&gt; batch.get_key(0)\n        &lt;FeatureKey.CDS: 2&gt;\n    \"\"\"\n    return FeatureKey(self._keys[idx])\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.get_qualifiers","title":"<code>get_qualifiers(idx)</code>","text":"<p>Returns the qualifier list for the feature at idx.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Feature index.</p> required <p>Returns:</p> Type Description <code>list[tuple]</code> <p>A list of <code>(key, value)</code> tuples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch.get_qualifiers(0)\n[(b'gene', b'dnaA'), (b'product', b'initiator')]\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>def get_qualifiers(self, idx: int) -&gt; list[tuple]:\n    \"\"\"Returns the qualifier list for the feature at *idx*.\n\n    Args:\n        idx: Feature index.\n\n    Returns:\n        A list of ``(key, value)`` tuples.\n\n    Examples:\n        &gt;&gt;&gt; batch.get_qualifiers(0)\n        [(b'gene', b'dnaA'), (b'product', b'initiator')]\n    \"\"\"\n    return self._qualifiers[idx]\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.random","title":"<code>random(n, rng=None, length=None, min_len=1, max_len=1000, min_start=0, max_start=1000000)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n random features for testing.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of features to generate.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator (optional).</p> <code>None</code> <code>length</code> <code>int</code> <p>Fixed length for all features (optional).</p> <code>None</code> <code>min_len</code> <code>int</code> <p>Minimum feature length (default 1).</p> <code>1</code> <code>max_len</code> <code>int</code> <p>Maximum feature length (default 1000).</p> <code>1000</code> <code>min_start</code> <code>int</code> <p>Minimum start coordinate (default 0).</p> <code>0</code> <code>max_start</code> <code>int</code> <p>Maximum start coordinate (default 1,000,000).</p> <code>1000000</code> <p>Returns:</p> Type Description <code>FeatureBatch</code> <p>A new <code>FeatureBatch</code> with random keys and intervals.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = FeatureBatch.random(10, rng=np.random.default_rng(42))\n&gt;&gt;&gt; len(batch)\n10\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>@classmethod\ndef random(cls, n: int, rng: np.random.Generator = None, length: int = None, min_len: int = 1, max_len: int = 1000,\n           min_start: int = 0, max_start: int = 1_000_000) -&gt; 'FeatureBatch':\n    \"\"\"Creates a batch of *n* random features for testing.\n\n    Args:\n        n: Number of features to generate.\n        rng: Random number generator (optional).\n        length: Fixed length for all features (optional).\n        min_len: Minimum feature length (default 1).\n        max_len: Maximum feature length (default 1000).\n        min_start: Minimum start coordinate (default 0).\n        max_start: Maximum start coordinate (default 1,000,000).\n\n    Returns:\n        A new ``FeatureBatch`` with random keys and intervals.\n\n    Examples:\n        &gt;&gt;&gt; batch = FeatureBatch.random(10, rng=np.random.default_rng(42))\n        &gt;&gt;&gt; len(batch)\n        10\n    \"\"\"\n    intervals = IntervalBatch.random(n, rng, length, min_len, max_len, min_start, max_start)\n    if rng is None: rng = np.random.default_rng()\n\n    # Select random keys from available FeatureKey values\n    valid_keys = np.array([k.value for k in FeatureKey], dtype=np.int16)\n    keys = rng.choice(valid_keys, size=n)\n\n    qualifiers = QualifierBatch.zeros(n)\n\n    return cls(intervals, keys, qualifiers)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n zero-length placeholder features.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of features to create.</p> required <p>Returns:</p> Type Description <code>FeatureBatch</code> <p>A <code>FeatureBatch</code> where every feature has interval <code>(0, 0)</code></p> <code>FeatureBatch</code> <p>and key <code>0</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = FeatureBatch.zeros(3)\n&gt;&gt;&gt; len(batch)\n3\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'FeatureBatch':\n    \"\"\"Creates a batch of *n* zero-length placeholder features.\n\n    Args:\n        n: Number of features to create.\n\n    Returns:\n        A ``FeatureBatch`` where every feature has interval ``(0, 0)``\n        and key ``0``.\n\n    Examples:\n        &gt;&gt;&gt; batch = FeatureBatch.zeros(3)\n        &gt;&gt;&gt; len(batch)\n        3\n    \"\"\"\n    return cls(\n        IntervalBatch.build([Interval(0, 0)] * n),\n        np.zeros(n, dtype=np.int16),\n        QualifierBatch.zeros(n)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureKey","title":"<code>FeatureKey</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Valid INSDC Feature Table Keys.</p> <p>Each member maps to a string representation via <code>__str__</code> and a bytes representation via the <code>bytes</code> property. Special characters (like <code>5'UTR</code> and <code>D-loop</code>) that are not valid Python identifiers are handled automatically.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; FeatureKey.CDS\n&lt;FeatureKey.CDS: 2&gt;\n&gt;&gt;&gt; str(FeatureKey.CDS)\n'CDS'\n&gt;&gt;&gt; FeatureKey.from_bytes(b'CDS')\n&lt;FeatureKey.CDS: 2&gt;\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>class FeatureKey(IntEnum):\n    \"\"\"\n    Valid INSDC Feature Table Keys.\n\n    Each member maps to a string representation via ``__str__`` and a bytes\n    representation via the ``bytes`` property. Special characters\n    (like ``5'UTR`` and ``D-loop``) that are not valid Python identifiers\n    are handled automatically.\n\n    Examples:\n        &gt;&gt;&gt; FeatureKey.CDS\n        &lt;FeatureKey.CDS: 2&gt;\n        &gt;&gt;&gt; str(FeatureKey.CDS)\n        'CDS'\n        &gt;&gt;&gt; FeatureKey.from_bytes(b'CDS')\n        &lt;FeatureKey.CDS: 2&gt;\n    \"\"\"\n    # --- Genes, Coding &amp; Transcriptional Products ---\n    GENE = auto()\n    CDS = auto()\n    MRNA = auto()\n    TRNA = auto()\n    RRNA = auto()\n    NCRNA = auto()\n    TMRNA = auto()\n    PRECURSOR_RNA = auto()\n    PRIM_TRANSCRIPT = auto()\n    EXON = auto()\n    INTRON = auto()\n    # Handled specifically in __str__\n    FIVE_PRIME_UTR = auto()\n    THREE_PRIME_UTR = auto()\n    # --- Protein Maturation &amp; Signaling ---\n    SIG_PEPTIDE = auto()\n    MAT_PEPTIDE = auto()\n    PROPEPTIDE = auto()\n    TRANSIT_PEPTIDE = auto()\n    # --- Immunoglobulin &amp; T-cell Receptor ---\n    V_REGION = auto()\n    D_SEGMENT = auto()\n    J_SEGMENT = auto()\n    C_REGION = auto()\n    N_REGION = auto()\n    S_REGION = auto()\n    V_SEGMENT = auto()\n    IDNA = auto()\n    # --- Genomic Structure &amp; Regulation ---\n    REGULATORY = auto()  # Replaces promoter, enhancer, terminator, etc.\n    OPERON = auto()\n    POLYA_SITE = auto()\n    REP_ORIGIN = auto()\n    ORIT = auto()\n    CENTROMERE = auto()\n    TELOMERE = auto()\n    MOBILE_ELEMENT = auto()\n    REPEAT_REGION = auto()\n    # --- Sequence, Variation &amp; Binding ---\n    GAP = auto()\n    ASSEMBLY_GAP = auto()\n    VARIATION = auto()\n    MISC_DIFFERENCE = auto()\n    MODIFIED_BASE = auto()\n    PROTEIN_BIND = auto()\n    PRIMER_BIND = auto()\n    MISC_BINDING = auto()\n    # --- Secondary Structure ---\n    STEM_LOOP = auto()\n    D_LOOP = auto()  # Handled specifically in __str__\n    MISC_STRUCTURE = auto()\n    # --- Miscellaneous / Other ---\n    SOURCE = auto()\n    MISC_FEATURE = auto()\n    MISC_RECOMB = auto()\n    MISC_RNA = auto()\n    OLD_SEQUENCE = auto()\n    MOTIF = auto()\n    STS = auto()\n    UNSURE = auto()\n\n    _STR_CACHE: ClassVar[dict]\n    _BYTES_CACHE: ClassVar[dict]\n    _FROM_BYTES_CACHE: ClassVar[dict]\n\n    def __str__(self): return self._STR_CACHE[self]\n\n    @property\n    def bytes(self) -&gt; bytes:\n        \"\"\"Returns the INSDC byte-string representation of this key.\n\n        Returns:\n            The key name as ASCII bytes (e.g. ``b'CDS'``, ``b\"5'UTR\"``).\n        \"\"\"\n        return self._BYTES_CACHE[self]\n\n    @classmethod\n    def from_bytes(cls, b: bytes) -&gt; 'FeatureKey':\n        \"\"\"Looks up a FeatureKey by its byte-string representation.\n\n        Falls back to ``MISC_FEATURE`` for unrecognised keys.\n\n        Args:\n            b: INSDC key as bytes (e.g. ``b'CDS'``).\n\n        Returns:\n            The matching FeatureKey member.\n\n        Examples:\n            &gt;&gt;&gt; FeatureKey.from_bytes(b'CDS')\n            &lt;FeatureKey.CDS: 2&gt;\n            &gt;&gt;&gt; FeatureKey.from_bytes(b'unknown')\n            &lt;FeatureKey.MISC_FEATURE: ...&gt;\n        \"\"\"\n        return cls._FROM_BYTES_CACHE.get(b, cls.MISC_FEATURE)\n\n    @classmethod\n    def _init_caches(cls):\n        cls._STR_CACHE = {}\n        cls._BYTES_CACHE = {}\n        cls._FROM_BYTES_CACHE = {}\n        special = {\n            cls.FIVE_PRIME_UTR: \"5'UTR\",\n            cls.THREE_PRIME_UTR: \"3'UTR\",\n            cls.D_LOOP: \"D-loop\",\n            cls.IDNA: \"iDNA\",\n            cls.ORIT: \"oriT\",\n            # Standard INSDC Keys with specific casing\n            cls.CDS: \"CDS\",\n            cls.MRNA: \"mRNA\",\n            cls.TRNA: \"tRNA\",\n            cls.RRNA: \"rRNA\",\n            cls.NCRNA: \"ncRNA\",\n            cls.TMRNA: \"tmRNA\",\n            cls.PRECURSOR_RNA: \"precursor_RNA\",\n            cls.MISC_RNA: \"misc_RNA\",\n            cls.STS: \"STS\",\n            cls.V_REGION: \"V_region\",\n            cls.D_SEGMENT: \"D_segment\",\n            cls.J_SEGMENT: \"J_segment\",\n            cls.C_REGION: \"C_region\",\n            cls.N_REGION: \"N_region\",\n            cls.S_REGION: \"S_region\",\n            cls.V_SEGMENT: \"V_segment\",\n            cls.POLYA_SITE: \"polyA_site\",\n            cls.SIG_PEPTIDE: \"sig_peptide\",\n            cls.MAT_PEPTIDE: \"mat_peptide\",\n            cls.TRANSIT_PEPTIDE: \"transit_peptide\",\n            cls.REP_ORIGIN: \"rep_origin\",\n            cls.MOTIF: \"motif\",\n        }\n        for k in cls:\n            s = special.get(k, k.name.lower())\n            cls._STR_CACHE[k] = s\n            b = s.encode('ascii')\n            cls._BYTES_CACHE[k] = b\n            cls._FROM_BYTES_CACHE[b] = k\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureKey.bytes","title":"<code>bytes</code>  <code>property</code>","text":"<p>Returns the INSDC byte-string representation of this key.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>The key name as ASCII bytes (e.g. <code>b'CDS'</code>, <code>b\"5'UTR\"</code>).</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureKey.from_bytes","title":"<code>from_bytes(b)</code>  <code>classmethod</code>","text":"<p>Looks up a FeatureKey by its byte-string representation.</p> <p>Falls back to <code>MISC_FEATURE</code> for unrecognised keys.</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>bytes</code> <p>INSDC key as bytes (e.g. <code>b'CDS'</code>).</p> required <p>Returns:</p> Type Description <code>FeatureKey</code> <p>The matching FeatureKey member.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; FeatureKey.from_bytes(b'CDS')\n&lt;FeatureKey.CDS: 2&gt;\n&gt;&gt;&gt; FeatureKey.from_bytes(b'unknown')\n&lt;FeatureKey.MISC_FEATURE: ...&gt;\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>@classmethod\ndef from_bytes(cls, b: bytes) -&gt; 'FeatureKey':\n    \"\"\"Looks up a FeatureKey by its byte-string representation.\n\n    Falls back to ``MISC_FEATURE`` for unrecognised keys.\n\n    Args:\n        b: INSDC key as bytes (e.g. ``b'CDS'``).\n\n    Returns:\n        The matching FeatureKey member.\n\n    Examples:\n        &gt;&gt;&gt; FeatureKey.from_bytes(b'CDS')\n        &lt;FeatureKey.CDS: 2&gt;\n        &gt;&gt;&gt; FeatureKey.from_bytes(b'unknown')\n        &lt;FeatureKey.MISC_FEATURE: ...&gt;\n    \"\"\"\n    return cls._FROM_BYTES_CACHE.get(b, cls.MISC_FEATURE)\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureList","title":"<code>FeatureList</code>","text":"<p>               Bases: <code>MutableSequence</code>, <code>HasIntervals</code></p> <p>A mutable list of genomic features with a lazily-built spatial index.</p> <p>The spatial index is automatically invalidated when features are added, removed, or reordered, and rebuilt on next access via the <code>intervals</code> property.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Iterable[Feature]</code> <p>Optional iterable of <code>Feature</code> objects.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fl = FeatureList([feat1, feat2])\n&gt;&gt;&gt; fl.append(feat3)\n&gt;&gt;&gt; overlapping = list(fl.get_overlapping(100, 500))\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>class FeatureList(MutableSequence, HasIntervals):\n    \"\"\"\n    A mutable list of genomic features with a lazily-built spatial index.\n\n    The spatial index is automatically invalidated when features are added,\n    removed, or reordered, and rebuilt on next access via the ``intervals``\n    property.\n\n    Args:\n        features: Optional iterable of ``Feature`` objects.\n\n    Examples:\n        &gt;&gt;&gt; fl = FeatureList([feat1, feat2])\n        &gt;&gt;&gt; fl.append(feat3)\n        &gt;&gt;&gt; overlapping = list(fl.get_overlapping(100, 500))\n    \"\"\"\n    __slots__ = ('_data', '_intervals')\n\n    def __init__(self, features: Iterable['Feature'] = None):\n        self._data: list[Feature] = list(features) if features else []\n        self._intervals = None\n\n    @property\n    def intervals(self) -&gt; 'IntervalBatch':\n        \"\"\"Returns the spatial index over all feature intervals.\n\n        Lazily built on first access and invalidated when the list is modified.\n\n        Returns:\n            An ``IntervalBatch`` covering all features.\n        \"\"\"\n        if self._intervals is None:\n            self._intervals = IntervalBatch.from_features(self._data)\n        return self._intervals\n\n    def __getitem__(self, index): return self._data[index]\n    def __repr__(self):\n        if len(self) &gt; 6:\n            return f\"[{', '.join(repr(x) for x in self[:3])}, ..., {', '.join(repr(x) for x in self[-3:])}]\"\n        return repr(self._data)\n    def __len__(self): return len(self._data)\n    def __iter__(self): return iter(self._data)\n    def _flag_dirty(self): self._intervals = None\n    def __setitem__(self, index, value):\n        self._data[index] = value\n        self._flag_dirty()\n\n    def __delitem__(self, index):\n        del self._data[index]\n        self._flag_dirty()\n\n    def insert(self, index: int, value: 'Feature'):\n        \"\"\"Inserts a feature at the given index.\n\n        Args:\n            index: Position to insert at.\n            value: The ``Feature`` to insert.\n        \"\"\"\n        self._data.insert(index, value)\n        self._flag_dirty()\n\n    def extend(self, values: Iterable['Feature']):\n        \"\"\"Appends multiple features to the list.\n\n        Args:\n            values: An iterable of ``Feature`` objects.\n        \"\"\"\n        self._data.extend(values)\n        self._flag_dirty()\n\n    def reverse(self):\n        \"\"\"Reverses the feature list in place.\"\"\"\n        self._data.reverse()\n        self._flag_dirty()\n\n    def sort(self, key=None, reverse: bool = False):\n        \"\"\"Sorts the feature list in place.\n\n        Args:\n            key: Optional sort key function.\n            reverse: If ``True``, sort in descending order.\n        \"\"\"\n        self._data.sort(key=key, reverse=reverse)\n        self._flag_dirty()\n\n    def get_overlapping(self, start: int, end: int) -&gt; Generator['Feature', None, None]:\n        \"\"\"Yields features overlapping the interval ``[start, end)``.\n\n        Uses the spatial index for fast querying.\n\n        Args:\n            start: Start coordinate (inclusive).\n            end: End coordinate (exclusive).\n\n        Yields:\n            ``Feature`` objects whose intervals overlap the query range.\n\n        Examples:\n            &gt;&gt;&gt; for feat in fl.get_overlapping(100, 500):\n            ...     print(feat.key)\n        \"\"\"\n        # self.intervals ensures the index is built and up-to-date\n        indices = self.intervals.query(start, end)\n        for i in indices: yield self._data[i]\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureList.intervals","title":"<code>intervals</code>  <code>property</code>","text":"<p>Returns the spatial index over all feature intervals.</p> <p>Lazily built on first access and invalidated when the list is modified.</p> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An <code>IntervalBatch</code> covering all features.</p>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureList.extend","title":"<code>extend(values)</code>","text":"<p>Appends multiple features to the list.</p> <p>Parameters:</p> Name Type Description Default <code>values</code> <code>Iterable[Feature]</code> <p>An iterable of <code>Feature</code> objects.</p> required Source code in <code>baclib/containers/feature.py</code> <pre><code>def extend(self, values: Iterable['Feature']):\n    \"\"\"Appends multiple features to the list.\n\n    Args:\n        values: An iterable of ``Feature`` objects.\n    \"\"\"\n    self._data.extend(values)\n    self._flag_dirty()\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureList.get_overlapping","title":"<code>get_overlapping(start, end)</code>","text":"<p>Yields features overlapping the interval <code>[start, end)</code>.</p> <p>Uses the spatial index for fast querying.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Start coordinate (inclusive).</p> required <code>end</code> <code>int</code> <p>End coordinate (exclusive).</p> required <p>Yields:</p> Type Description <code>Feature</code> <p><code>Feature</code> objects whose intervals overlap the query range.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; for feat in fl.get_overlapping(100, 500):\n...     print(feat.key)\n</code></pre> Source code in <code>baclib/containers/feature.py</code> <pre><code>def get_overlapping(self, start: int, end: int) -&gt; Generator['Feature', None, None]:\n    \"\"\"Yields features overlapping the interval ``[start, end)``.\n\n    Uses the spatial index for fast querying.\n\n    Args:\n        start: Start coordinate (inclusive).\n        end: End coordinate (exclusive).\n\n    Yields:\n        ``Feature`` objects whose intervals overlap the query range.\n\n    Examples:\n        &gt;&gt;&gt; for feat in fl.get_overlapping(100, 500):\n        ...     print(feat.key)\n    \"\"\"\n    # self.intervals ensures the index is built and up-to-date\n    indices = self.intervals.query(start, end)\n    for i in indices: yield self._data[i]\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureList.insert","title":"<code>insert(index, value)</code>","text":"<p>Inserts a feature at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int</code> <p>Position to insert at.</p> required <code>value</code> <code>Feature</code> <p>The <code>Feature</code> to insert.</p> required Source code in <code>baclib/containers/feature.py</code> <pre><code>def insert(self, index: int, value: 'Feature'):\n    \"\"\"Inserts a feature at the given index.\n\n    Args:\n        index: Position to insert at.\n        value: The ``Feature`` to insert.\n    \"\"\"\n    self._data.insert(index, value)\n    self._flag_dirty()\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureList.reverse","title":"<code>reverse()</code>","text":"<p>Reverses the feature list in place.</p> Source code in <code>baclib/containers/feature.py</code> <pre><code>def reverse(self):\n    \"\"\"Reverses the feature list in place.\"\"\"\n    self._data.reverse()\n    self._flag_dirty()\n</code></pre>"},{"location":"reference/baclib/containers/feature/#baclib.containers.feature.FeatureList.sort","title":"<code>sort(key=None, reverse=False)</code>","text":"<p>Sorts the feature list in place.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <p>Optional sort key function.</p> <code>None</code> <code>reverse</code> <code>bool</code> <p>If <code>True</code>, sort in descending order.</p> <code>False</code> Source code in <code>baclib/containers/feature.py</code> <pre><code>def sort(self, key=None, reverse: bool = False):\n    \"\"\"Sorts the feature list in place.\n\n    Args:\n        key: Optional sort key function.\n        reverse: If ``True``, sort in descending order.\n    \"\"\"\n    self._data.sort(key=key, reverse=reverse)\n    self._flag_dirty()\n</code></pre>"},{"location":"reference/baclib/containers/genome/","title":"genome","text":""},{"location":"reference/baclib/containers/genome/#baclib.containers.genome","title":"<code>baclib.containers.genome</code>","text":"<p>Container for representing bacterial genome assemblies composed of contigs and edges.</p>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly","title":"<code>GenomeAssembly</code>","text":"<p>An immutable representation of a bacterial genome assembly.</p> <p>Backed by a single <code>RecordBatch</code> for efficient, vectorized access to sequences and features. Contigs are keyed by ID for O(1) lookup.</p> <p>Parameters:</p> Name Type Description Default <code>contigs</code> <code>RecordBatch</code> <p>A <code>RecordBatch</code> containing the contig records.</p> required <code>edges</code> <code>EdgeBatch</code> <p>Optional <code>EdgeBatch</code> of connectivity edges (e.g. from GFA).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.fasta')\n&gt;&gt;&gt; assembly.n_contigs\n5\n&gt;&gt;&gt; assembly[b'contig_1']  # O(1) lookup by ID\nRecord(...)\n</code></pre> Source code in <code>baclib/containers/genome.py</code> <pre><code>class GenomeAssembly:\n    \"\"\"\n    An immutable representation of a bacterial genome assembly.\n\n    Backed by a single ``RecordBatch`` for efficient, vectorized access to sequences\n    and features. Contigs are keyed by ID for O(1) lookup.\n\n    Args:\n        contigs: A ``RecordBatch`` containing the contig records.\n        edges: Optional ``EdgeBatch`` of connectivity edges (e.g. from GFA).\n\n    Examples:\n        &gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.fasta')\n        &gt;&gt;&gt; assembly.n_contigs\n        5\n        &gt;&gt;&gt; assembly[b'contig_1']  # O(1) lookup by ID\n        Record(...)\n    \"\"\"\n    __slots__ = ('_contigs', '_edges', '_cached_graph', '_id_index')\n    _ALPHABET = Alphabet.DNA\n    _SEQ_FORMATS = {SeqFileFormat.FASTA, SeqFileFormat.GFA, SeqFileFormat.GENBANK}\n    _ANNOTATION_FORMATS = {SeqFileFormat.BED, SeqFileFormat.GFF}\n\n    def __init__(self, contigs: RecordBatch, edges: EdgeBatch = None):\n        self._contigs = contigs\n        self._edges: EdgeBatch = edges or EdgeBatch.empty()\n        self._cached_graph = None\n        self._id_index = {contigs.ids[i]: i for i in range(len(contigs))}\n\n    # --- Dunder methods ---\n    def __len__(self): return int(np.sum(self._contigs.seqs.lengths))\n    def __iter__(self): return iter(self._contigs)\n    def __getitem__(self, item: bytes) -&gt; Record: return self._contigs[self._id_index[item]]\n    def __contains__(self, item: bytes): return item in self._id_index\n    def __repr__(self): return f\"&lt;Genome: {len(self._contigs)} contigs, {len(self._edges)} edges&gt;\"\n\n    # --- Properties ---\n    @property\n    def contigs(self) -&gt; RecordBatch:\n        \"\"\"Returns the underlying ``RecordBatch`` of all contigs.\n\n        Returns:\n            The contig ``RecordBatch``.\n        \"\"\"\n        return self._contigs\n\n    @property\n    def n_contigs(self) -&gt; int:\n        \"\"\"Returns the number of contigs in the assembly.\n\n        Returns:\n            Contig count.\n        \"\"\"\n        return len(self._contigs)\n\n    @property\n    def ids(self) -&gt; np.ndarray:\n        \"\"\"Returns the contig IDs as a numpy object array.\n\n        Returns:\n            An object array of ``bytes`` contig IDs.\n        \"\"\"\n        return self._contigs.ids\n\n    @property\n    def edges(self) -&gt; EdgeBatch:\n        \"\"\"Returns the connectivity edges between contigs.\n\n        Returns:\n            An ``EdgeBatch`` of assembly edges.\n        \"\"\"\n        return self._edges\n\n    # --- Construction ---\n    @classmethod\n    def from_file(cls, file: Union[str, Path, BinaryIO, SeqFile],\n                  annotations: Union[str, Path, BinaryIO, SeqFile] = None,\n                  deduplicate: bool = True):\n        \"\"\"Loads a genome assembly from a sequence file with optional annotations.\n\n        Supports FASTA, GFA, and GenBank sequence formats, with optional\n        BED or GFF annotation overlays. Circular contigs are auto-detected\n        from ``topology=circular`` qualifiers and expressed as self-loop edges.\n\n        Args:\n            file: Path, file handle, or ``SeqFile`` for the sequence data.\n            annotations: Optional path or handle to a BED or GFF annotation file.\n            deduplicate: Whether to deduplicate identical contig sequences\n                (default ``True``).\n\n        Returns:\n            A new ``GenomeAssembly``.\n\n        Raises:\n            GenomeAssemblyError: If the file format is unsupported or contains\n                no sequence records.\n\n        Examples:\n            &gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.gfa')\n            &gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.fasta', annotations='features.gff')\n        \"\"\"\n        if not isinstance(file, SeqFile): file = SeqFile(file)\n        if file.format not in cls._SEQ_FORMATS:\n            raise GenomeAssemblyError(f'Genome assemblies must come from {cls._SEQ_FORMATS}, not {file.format}')\n\n        contigs, edges = [], []\n        for batch in file.batches():\n            if isinstance(batch, RecordBatch):\n                contigs.append(batch)\n            elif isinstance(batch, EdgeBatch):\n                edges.append(batch)\n\n        if not contigs:\n            raise GenomeAssemblyError('File contained no sequence records')\n\n        contigs = RecordBatch.concat(contigs, deduplicate=deduplicate)\n\n        # Auto-circularize contigs with topology=circular (FASTA/GenBank)\n        if file.format != SeqFileFormat.GFA:\n            for i in range(len(contigs)):\n                for k, v in contigs.get_qualifiers(i):\n                    if k == b'topology' and v == b'circular':\n                        rid = contigs.ids[i]\n                        edges.append(EdgeBatch.build(Edge(rid, rid, i) for i in (Strand.FORWARD, Strand.REVERSE)))\n                        break\n\n        # Load annotations\n        if annotations:\n            if not isinstance(annotations, SeqFile): annotations = SeqFile(annotations)\n            if annotations.format not in cls._ANNOTATION_FORMATS:\n                raise GenomeAssemblyError(\n                    f'Annotations must come from {cls._ANNOTATION_FORMATS}, not {annotations.format}')\n\n            feature_batches = [b for b in annotations.batches() if isinstance(b, FeatureBatch)]\n            if feature_batches:\n                all_features = FeatureBatch.concat(feature_batches) if len(feature_batches) &gt; 1 else feature_batches[0]\n                contigs = contigs.add_features(all_features, pivot_key=FeatureKey.SOURCE)\n\n        if edges:\n            edges = EdgeBatch.concat(edges) if len(edges) &gt; 1 else edges[0]\n        else:\n            edges = EdgeBatch.empty()\n\n        return cls(contigs, edges)\n\n    def n_features(self) -&gt; int:\n        \"\"\"Returns the total number of features across all contigs.\n\n        Returns:\n            Total feature count.\n        \"\"\"\n        return self._contigs.n_features\n\n    def as_graph(self, node_attributes: Iterable[bytes] = ()) -&gt; Graph:\n        \"\"\"Returns the assembly graph representation.\n\n        Nodes correspond to contigs, edges represent physical connectivity\n        (e.g. GFA links, circular topology). The graph is cached and rebuilt\n        only if the assembly changes.\n\n        Args:\n            node_attributes: Qualifier keys to include as node attributes\n                (e.g. ``[b'topology', b'length']``).\n\n        Returns:\n            A ``Graph`` object.\n\n        Examples:\n            &gt;&gt;&gt; g = assembly.as_graph(node_attributes=[b'topology'])\n            &gt;&gt;&gt; g.nodes\n            [b'contig_1', b'contig_2']\n        \"\"\"\n        if self._cached_graph is None:\n            g = Graph()\n            for i in range(len(self._contigs)):\n                rec_id = self._contigs.ids[i]\n                quals = self._contigs.get_qualifiers(i)\n                attrs = {k: v for k, v in quals if k in node_attributes}\n                g.add_node(rec_id, attrs)\n            if self._edges: g.add_edges(self._edges)\n            self._cached_graph = g\n        return self._cached_graph\n\n    def extract_features(self, key: FeatureKey = FeatureKey.CDS) -&gt; 'SeqBatch':\n        \"\"\"Extracts sequences for all features of a given type across the genome.\n\n        Uses the vectorized ``RecordBatch.extract_features`` kernel for\n        high-performance extraction.\n\n        Args:\n            key: The ``FeatureKey`` to filter by (default ``CDS``).\n\n        Returns:\n            A ``SeqBatch`` of the extracted feature sequences.\n\n        Examples:\n            &gt;&gt;&gt; cds_seqs = assembly.extract_features(FeatureKey.CDS)\n            &gt;&gt;&gt; trna_seqs = assembly.extract_features(FeatureKey.TRNA)\n        \"\"\"\n        return self._contigs.extract_features(key.bytes)\n</code></pre>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.contigs","title":"<code>contigs</code>  <code>property</code>","text":"<p>Returns the underlying <code>RecordBatch</code> of all contigs.</p> <p>Returns:</p> Type Description <code>RecordBatch</code> <p>The contig <code>RecordBatch</code>.</p>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Returns the connectivity edges between contigs.</p> <p>Returns:</p> Type Description <code>EdgeBatch</code> <p>An <code>EdgeBatch</code> of assembly edges.</p>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.ids","title":"<code>ids</code>  <code>property</code>","text":"<p>Returns the contig IDs as a numpy object array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An object array of <code>bytes</code> contig IDs.</p>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.n_contigs","title":"<code>n_contigs</code>  <code>property</code>","text":"<p>Returns the number of contigs in the assembly.</p> <p>Returns:</p> Type Description <code>int</code> <p>Contig count.</p>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.as_graph","title":"<code>as_graph(node_attributes=())</code>","text":"<p>Returns the assembly graph representation.</p> <p>Nodes correspond to contigs, edges represent physical connectivity (e.g. GFA links, circular topology). The graph is cached and rebuilt only if the assembly changes.</p> <p>Parameters:</p> Name Type Description Default <code>node_attributes</code> <code>Iterable[bytes]</code> <p>Qualifier keys to include as node attributes (e.g. <code>[b'topology', b'length']</code>).</p> <code>()</code> <p>Returns:</p> Type Description <code>Graph</code> <p>A <code>Graph</code> object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; g = assembly.as_graph(node_attributes=[b'topology'])\n&gt;&gt;&gt; g.nodes\n[b'contig_1', b'contig_2']\n</code></pre> Source code in <code>baclib/containers/genome.py</code> <pre><code>def as_graph(self, node_attributes: Iterable[bytes] = ()) -&gt; Graph:\n    \"\"\"Returns the assembly graph representation.\n\n    Nodes correspond to contigs, edges represent physical connectivity\n    (e.g. GFA links, circular topology). The graph is cached and rebuilt\n    only if the assembly changes.\n\n    Args:\n        node_attributes: Qualifier keys to include as node attributes\n            (e.g. ``[b'topology', b'length']``).\n\n    Returns:\n        A ``Graph`` object.\n\n    Examples:\n        &gt;&gt;&gt; g = assembly.as_graph(node_attributes=[b'topology'])\n        &gt;&gt;&gt; g.nodes\n        [b'contig_1', b'contig_2']\n    \"\"\"\n    if self._cached_graph is None:\n        g = Graph()\n        for i in range(len(self._contigs)):\n            rec_id = self._contigs.ids[i]\n            quals = self._contigs.get_qualifiers(i)\n            attrs = {k: v for k, v in quals if k in node_attributes}\n            g.add_node(rec_id, attrs)\n        if self._edges: g.add_edges(self._edges)\n        self._cached_graph = g\n    return self._cached_graph\n</code></pre>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.extract_features","title":"<code>extract_features(key=FeatureKey.CDS)</code>","text":"<p>Extracts sequences for all features of a given type across the genome.</p> <p>Uses the vectorized <code>RecordBatch.extract_features</code> kernel for high-performance extraction.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>FeatureKey</code> <p>The <code>FeatureKey</code> to filter by (default <code>CDS</code>).</p> <code>CDS</code> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A <code>SeqBatch</code> of the extracted feature sequences.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cds_seqs = assembly.extract_features(FeatureKey.CDS)\n&gt;&gt;&gt; trna_seqs = assembly.extract_features(FeatureKey.TRNA)\n</code></pre> Source code in <code>baclib/containers/genome.py</code> <pre><code>def extract_features(self, key: FeatureKey = FeatureKey.CDS) -&gt; 'SeqBatch':\n    \"\"\"Extracts sequences for all features of a given type across the genome.\n\n    Uses the vectorized ``RecordBatch.extract_features`` kernel for\n    high-performance extraction.\n\n    Args:\n        key: The ``FeatureKey`` to filter by (default ``CDS``).\n\n    Returns:\n        A ``SeqBatch`` of the extracted feature sequences.\n\n    Examples:\n        &gt;&gt;&gt; cds_seqs = assembly.extract_features(FeatureKey.CDS)\n        &gt;&gt;&gt; trna_seqs = assembly.extract_features(FeatureKey.TRNA)\n    \"\"\"\n    return self._contigs.extract_features(key.bytes)\n</code></pre>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.from_file","title":"<code>from_file(file, annotations=None, deduplicate=True)</code>  <code>classmethod</code>","text":"<p>Loads a genome assembly from a sequence file with optional annotations.</p> <p>Supports FASTA, GFA, and GenBank sequence formats, with optional BED or GFF annotation overlays. Circular contigs are auto-detected from <code>topology=circular</code> qualifiers and expressed as self-loop edges.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path, BinaryIO, SeqFile]</code> <p>Path, file handle, or <code>SeqFile</code> for the sequence data.</p> required <code>annotations</code> <code>Union[str, Path, BinaryIO, SeqFile]</code> <p>Optional path or handle to a BED or GFF annotation file.</p> <code>None</code> <code>deduplicate</code> <code>bool</code> <p>Whether to deduplicate identical contig sequences (default <code>True</code>).</p> <code>True</code> <p>Returns:</p> Type Description <p>A new <code>GenomeAssembly</code>.</p> <p>Raises:</p> Type Description <code>GenomeAssemblyError</code> <p>If the file format is unsupported or contains no sequence records.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.gfa')\n&gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.fasta', annotations='features.gff')\n</code></pre> Source code in <code>baclib/containers/genome.py</code> <pre><code>@classmethod\ndef from_file(cls, file: Union[str, Path, BinaryIO, SeqFile],\n              annotations: Union[str, Path, BinaryIO, SeqFile] = None,\n              deduplicate: bool = True):\n    \"\"\"Loads a genome assembly from a sequence file with optional annotations.\n\n    Supports FASTA, GFA, and GenBank sequence formats, with optional\n    BED or GFF annotation overlays. Circular contigs are auto-detected\n    from ``topology=circular`` qualifiers and expressed as self-loop edges.\n\n    Args:\n        file: Path, file handle, or ``SeqFile`` for the sequence data.\n        annotations: Optional path or handle to a BED or GFF annotation file.\n        deduplicate: Whether to deduplicate identical contig sequences\n            (default ``True``).\n\n    Returns:\n        A new ``GenomeAssembly``.\n\n    Raises:\n        GenomeAssemblyError: If the file format is unsupported or contains\n            no sequence records.\n\n    Examples:\n        &gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.gfa')\n        &gt;&gt;&gt; assembly = GenomeAssembly.from_file('genome.fasta', annotations='features.gff')\n    \"\"\"\n    if not isinstance(file, SeqFile): file = SeqFile(file)\n    if file.format not in cls._SEQ_FORMATS:\n        raise GenomeAssemblyError(f'Genome assemblies must come from {cls._SEQ_FORMATS}, not {file.format}')\n\n    contigs, edges = [], []\n    for batch in file.batches():\n        if isinstance(batch, RecordBatch):\n            contigs.append(batch)\n        elif isinstance(batch, EdgeBatch):\n            edges.append(batch)\n\n    if not contigs:\n        raise GenomeAssemblyError('File contained no sequence records')\n\n    contigs = RecordBatch.concat(contigs, deduplicate=deduplicate)\n\n    # Auto-circularize contigs with topology=circular (FASTA/GenBank)\n    if file.format != SeqFileFormat.GFA:\n        for i in range(len(contigs)):\n            for k, v in contigs.get_qualifiers(i):\n                if k == b'topology' and v == b'circular':\n                    rid = contigs.ids[i]\n                    edges.append(EdgeBatch.build(Edge(rid, rid, i) for i in (Strand.FORWARD, Strand.REVERSE)))\n                    break\n\n    # Load annotations\n    if annotations:\n        if not isinstance(annotations, SeqFile): annotations = SeqFile(annotations)\n        if annotations.format not in cls._ANNOTATION_FORMATS:\n            raise GenomeAssemblyError(\n                f'Annotations must come from {cls._ANNOTATION_FORMATS}, not {annotations.format}')\n\n        feature_batches = [b for b in annotations.batches() if isinstance(b, FeatureBatch)]\n        if feature_batches:\n            all_features = FeatureBatch.concat(feature_batches) if len(feature_batches) &gt; 1 else feature_batches[0]\n            contigs = contigs.add_features(all_features, pivot_key=FeatureKey.SOURCE)\n\n    if edges:\n        edges = EdgeBatch.concat(edges) if len(edges) &gt; 1 else edges[0]\n    else:\n        edges = EdgeBatch.empty()\n\n    return cls(contigs, edges)\n</code></pre>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssembly.n_features","title":"<code>n_features()</code>","text":"<p>Returns the total number of features across all contigs.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total feature count.</p> Source code in <code>baclib/containers/genome.py</code> <pre><code>def n_features(self) -&gt; int:\n    \"\"\"Returns the total number of features across all contigs.\n\n    Returns:\n        Total feature count.\n    \"\"\"\n    return self._contigs.n_features\n</code></pre>"},{"location":"reference/baclib/containers/genome/#baclib.containers.genome.GenomeAssemblyError","title":"<code>GenomeAssemblyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a genome assembly operation fails (e.g. missing contigs, format mismatch).</p> Source code in <code>baclib/containers/genome.py</code> <pre><code>class GenomeAssemblyError(Exception):\n    \"\"\"Raised when a genome assembly operation fails (e.g. missing contigs, format mismatch).\"\"\"\n</code></pre>"},{"location":"reference/baclib/containers/graph/","title":"graph","text":""},{"location":"reference/baclib/containers/graph/#baclib.containers.graph","title":"<code>baclib.containers.graph</code>","text":"<p>Graph data structures for representing directed and undirected networks of genomic segments.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Edge","title":"<code>Edge</code>","text":"<p>A directed edge between two nodes with strand orientation and attributes.</p> <p>Nodes can be specified as <code>bytes</code> IDs, strings, or objects with an <code>.id</code> attribute \u2014 they are coerced to <code>bytes</code> internally.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>Any</code> <p>Source node ID (or object with <code>.id</code>).</p> required <code>v</code> <code>Any</code> <p>Target node ID (or object with <code>.id</code>).</p> required <code>u_strand</code> <code>Union[Strand, int]</code> <p>Orientation of the source node (default <code>FORWARD</code>).</p> <code>FORWARD</code> <code>v_strand</code> <code>Union[Strand, int]</code> <p>Orientation of the target node (default <code>FORWARD</code>).</p> <code>FORWARD</code> <code>attributes</code> <code>dict[bytes, Any]</code> <p>Optional dictionary of edge attributes.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; e = Edge(b'contig_1', b'contig_2')\n&gt;&gt;&gt; e.reverse()\nEdge(contig_2(+)-&gt;contig_1(+))\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>class Edge:\n    \"\"\"\n    A directed edge between two nodes with strand orientation and attributes.\n\n    Nodes can be specified as ``bytes`` IDs, strings, or objects with an\n    ``.id`` attribute \u2014 they are coerced to ``bytes`` internally.\n\n    Args:\n        u: Source node ID (or object with ``.id``).\n        v: Target node ID (or object with ``.id``).\n        u_strand: Orientation of the source node (default ``FORWARD``).\n        v_strand: Orientation of the target node (default ``FORWARD``).\n        attributes: Optional dictionary of edge attributes.\n\n    Examples:\n        &gt;&gt;&gt; e = Edge(b'contig_1', b'contig_2')\n        &gt;&gt;&gt; e.reverse()\n        Edge(contig_2(+)-&gt;contig_1(+))\n    \"\"\"\n    __slots__ = ('_u', '_v', '_u_strand', '_v_strand', 'attributes')\n\n    def __init__(self, u: Any, v: Any, u_strand: Union[Strand, int] = Strand.FORWARD, \n                 v_strand: Union[Strand, int] = Strand.FORWARD, attributes: dict[bytes, Any] = None):\n        self._u: bytes = self._coerce_node(u)  # Extract the pointer\n        self._v: bytes = self._coerce_node(v)  # Extract the pointer\n        self._u_strand = Strand(u_strand)\n        self._v_strand = Strand(v_strand)\n        self.attributes = attributes or {}\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``EdgeBatch`` class.\n        \"\"\"\n        return EdgeBatch\n\n    @property\n    def u(self) -&gt; bytes:\n        \"\"\"Returns the source node ID.\n\n        Returns:\n            Source node as ``bytes``.\n        \"\"\"\n        return self._u\n\n    @property\n    def v(self) -&gt; bytes:\n        \"\"\"Returns the target node ID.\n\n        Returns:\n            Target node as ``bytes``.\n        \"\"\"\n        return self._v\n\n    @property\n    def u_strand(self) -&gt; Strand:\n        \"\"\"Returns the source node strand orientation.\n\n        Returns:\n            A ``Strand`` value.\n        \"\"\"\n        return self._u_strand\n\n    @property\n    def v_strand(self) -&gt; Strand:\n        \"\"\"Returns the target node strand orientation.\n\n        Returns:\n            A ``Strand`` value.\n        \"\"\"\n        return self._v_strand\n\n    @staticmethod\n    def _coerce_node(obj: Any) -&gt; bytes:\n        if isinstance(obj, bytes): return obj\n        if hasattr(obj, 'id'): return obj.id\n        if isinstance(obj, str): return obj.encode('ascii')\n        return str(obj).encode('ascii')\n\n    def reverse(self) -&gt; 'Edge':\n        \"\"\"Returns a new edge with swapped source and target.\n\n        Returns:\n            A reversed ``Edge`` with copied attributes.\n\n        Examples:\n            &gt;&gt;&gt; Edge(b'A', b'B').reverse()\n            Edge(B(+)-&gt;A(+))\n        \"\"\"\n        return Edge(self.v, self.u, self.v_strand, self.u_strand, self.attributes.copy())\n\n    def __eq__(self, other):\n        if not isinstance(other, Edge): return NotImplemented\n        return (self._u == other._u and self._v == other._v and \n                self._u_strand == other._u_strand and self._v_strand == other._v_strand and\n                self.attributes == other.attributes)\n\n    def __hash__(self):\n        return hash((self._u, self._v, self._u_strand, self._v_strand, tuple(sorted(self.attributes.items()))))\n\n    def __repr__(self): \n        u_s = b'+' if self._u_strand == 1 else b'-'\n        v_s = b'+' if self._v_strand == 1 else b'-'\n        return f\"Edge({self.u.decode()}({u_s.decode()})-&gt;{self.v.decode()}({v_s.decode()}))\"\n\n    def __getitem__(self, item): return self.attributes.get(item, None)\n    def __iter__(self): return iter((self.u, self.v, self.u_strand, self.v_strand, self.attributes))\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Edge.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>EdgeBatch</code> class.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Edge.u","title":"<code>u</code>  <code>property</code>","text":"<p>Returns the source node ID.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>Source node as <code>bytes</code>.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Edge.u_strand","title":"<code>u_strand</code>  <code>property</code>","text":"<p>Returns the source node strand orientation.</p> <p>Returns:</p> Type Description <code>Strand</code> <p>A <code>Strand</code> value.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Edge.v","title":"<code>v</code>  <code>property</code>","text":"<p>Returns the target node ID.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>Target node as <code>bytes</code>.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Edge.v_strand","title":"<code>v_strand</code>  <code>property</code>","text":"<p>Returns the target node strand orientation.</p> <p>Returns:</p> Type Description <code>Strand</code> <p>A <code>Strand</code> value.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Edge.reverse","title":"<code>reverse()</code>","text":"<p>Returns a new edge with swapped source and target.</p> <p>Returns:</p> Type Description <code>Edge</code> <p>A reversed <code>Edge</code> with copied attributes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Edge(b'A', b'B').reverse()\nEdge(B(+)-&gt;A(+))\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>def reverse(self) -&gt; 'Edge':\n    \"\"\"Returns a new edge with swapped source and target.\n\n    Returns:\n        A reversed ``Edge`` with copied attributes.\n\n    Examples:\n        &gt;&gt;&gt; Edge(b'A', b'B').reverse()\n        Edge(B(+)-&gt;A(+))\n    \"\"\"\n    return Edge(self.v, self.u, self.v_strand, self.u_strand, self.attributes.copy())\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch","title":"<code>EdgeBatch</code>","text":"<p>               Bases: <code>Batch</code></p> <p>Columnar container for edges, optimized for batch I/O and graph construction.</p> <p>Stores source/target node IDs, strand orientations, and attributes as parallel numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>u</code> <code>ndarray</code> <p>Object array of source node IDs.</p> required <code>v</code> <code>ndarray</code> <p>Object array of target node IDs.</p> required <code>u_strands</code> <code>ndarray</code> <p><code>int8</code> array of source strand orientations.</p> <code>None</code> <code>v_strands</code> <code>ndarray</code> <p><code>int8</code> array of target strand orientations.</p> <code>None</code> <code>attributes</code> <code>Dict[bytes, ndarray]</code> <p>Optional dict mapping attribute names to per-edge arrays.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = EdgeBatch.build([edge1, edge2])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>class EdgeBatch(Batch):\n    \"\"\"\n    Columnar container for edges, optimized for batch I/O and graph construction.\n\n    Stores source/target node IDs, strand orientations, and attributes as\n    parallel numpy arrays.\n\n    Args:\n        u: Object array of source node IDs.\n        v: Object array of target node IDs.\n        u_strands: ``int8`` array of source strand orientations.\n        v_strands: ``int8`` array of target strand orientations.\n        attributes: Optional dict mapping attribute names to per-edge arrays.\n\n    Examples:\n        &gt;&gt;&gt; batch = EdgeBatch.build([edge1, edge2])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    __slots__ = ('_u', '_v', '_u_strands', '_v_strands', '_attributes')\n\n    def __init__(self, u: np.ndarray, v: np.ndarray, \n                 u_strands: np.ndarray = None, v_strands: np.ndarray = None,\n                 attributes: Dict[bytes, np.ndarray] = None):\n        self._u = np.array(u, copy=False)\n        self._v = np.array(v, copy=False)\n        n = len(self._u)\n        self._u_strands = u_strands.astype(np.int8, copy=False) if u_strands is not None else np.full(n, 1, dtype=np.int8)\n        self._v_strands = v_strands.astype(np.int8, copy=False) if v_strands is not None else np.full(n, 1, dtype=np.int8)\n        self._attributes = attributes or {}\n\n        if len(self._u) != len(self._v):\n            raise ValueError(\"u and v arrays must have the same length\")\n\n    @classmethod\n    def build(cls, components: Iterable[Edge]) -&gt; 'EdgeBatch':\n        \"\"\"Constructs an EdgeBatch from an iterable of Edge objects.\n\n        Args:\n            components: An iterable of ``Edge`` objects.\n\n        Returns:\n            A new ``EdgeBatch``.\n\n        Examples:\n            &gt;&gt;&gt; batch = EdgeBatch.build([edge1, edge2])\n        \"\"\"\n        edges = list(components)\n        if not edges: return cls.empty()\n        u = np.array([e.u for e in edges])\n        v = np.array([e.v for e in edges])\n        us = np.array([e.u_strand for e in edges], dtype=np.int8)\n        vs = np.array([e.v_strand for e in edges], dtype=np.int8)\n        return cls(u, v, us, vs)\n\n    @classmethod\n    def concat(cls, batches: Iterable['EdgeBatch']) -&gt; 'EdgeBatch':\n        \"\"\"Concatenates multiple EdgeBatch objects into one.\n\n        Args:\n            batches: An iterable of ``EdgeBatch`` objects.\n\n        Returns:\n            A single concatenated ``EdgeBatch``.\n        \"\"\"\n        batches = list(batches)\n        if not batches: return cls.empty()\n        u = np.concatenate([b.u for b in batches])\n        v = np.concatenate([b.v for b in batches])\n        us = np.concatenate([b.u_strands for b in batches])\n        vs = np.concatenate([b.v_strands for b in batches])\n        return cls(u, v, us, vs)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by all internal arrays.\n        \"\"\"\n        return self._u.nbytes + self._v.nbytes + self._u_strands.nbytes + self._v_strands.nbytes\n\n    @classmethod\n    def empty(cls) -&gt; 'EdgeBatch':\n        \"\"\"Creates an empty EdgeBatch with zero edges.\n\n        Returns:\n            An empty ``EdgeBatch``.\n        \"\"\"\n        return cls.zeros(0)\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'EdgeBatch':\n        \"\"\"Creates an EdgeBatch with *n* placeholder edges.\n\n        Args:\n            n: Number of placeholder edge slots.\n\n        Returns:\n            An ``EdgeBatch`` with empty IDs and forward strands.\n        \"\"\"\n        return cls(\n            np.full(n, b'', dtype='S1'),\n            np.full(n, b'', dtype='S1'),\n            np.ones(n, dtype=np.int8),\n            np.ones(n, dtype=np.int8),\n            {}\n        )\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``Edge`` class.\n        \"\"\"\n        return Edge\n\n    def __repr__(self): return f\"&lt;EdgeBatch: {len(self)} edges&gt;\"\n\n    @property\n    def u(self) -&gt; np.ndarray:\n        \"\"\"Returns the source node IDs.\n\n        Returns:\n            An object array of ``bytes``.\n        \"\"\"\n        return self._u\n\n    @property\n    def v(self) -&gt; np.ndarray:\n        \"\"\"Returns the target node IDs.\n\n        Returns:\n            An object array of ``bytes``.\n        \"\"\"\n        return self._v\n\n    @property\n    def u_strands(self) -&gt; np.ndarray:\n        \"\"\"Returns the source strand orientations.\n\n        Returns:\n            An ``int8`` numpy array.\n        \"\"\"\n        return self._u_strands\n\n    @property\n    def v_strands(self) -&gt; np.ndarray:\n        \"\"\"Returns the target strand orientations.\n\n        Returns:\n            An ``int8`` numpy array.\n        \"\"\"\n        return self._v_strands\n\n    @property\n    def attributes(self) -&gt; Dict[bytes, np.ndarray]:\n        \"\"\"Returns the per-edge attribute arrays.\n\n        Returns:\n            A dict mapping attribute names to numpy arrays.\n        \"\"\"\n        return self._attributes\n\n    def copy(self) -&gt; 'EdgeBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``EdgeBatch`` with copied arrays.\n        \"\"\"\n        # Note: attributes dict copy is shallow for arrays inside\n        return self.__class__(self._u.copy(), self._v.copy(), self._u_strands.copy(), self._v_strands.copy(), self._attributes.copy())\n\n    def __len__(self): return len(self._u)\n    def __iter__(self):\n        for i in range(len(self)):\n            attrs = {k: v[i] for k, v in self._attributes.items()}\n            yield Edge(self._u[i], self._v[i], self._u_strands[i], self._v_strands[i], attrs)\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            attrs = {k: v[item] for k, v in self._attributes.items()}\n            return Edge(self._u[item], self._v[item], self._u_strands[item], self._v_strands[item], attrs)\n        elif isinstance(item, (slice, np.ndarray, list)):\n            new_attrs = {k: v[item] for k, v in self._attributes.items()}\n            return EdgeBatch(self._u[item], self._v[item], self._u_strands[item], self._v_strands[item], new_attrs)\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.attributes","title":"<code>attributes</code>  <code>property</code>","text":"<p>Returns the per-edge attribute arrays.</p> <p>Returns:</p> Type Description <code>Dict[bytes, ndarray]</code> <p>A dict mapping attribute names to numpy arrays.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>Edge</code> class.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by all internal arrays.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.u","title":"<code>u</code>  <code>property</code>","text":"<p>Returns the source node IDs.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An object array of <code>bytes</code>.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.u_strands","title":"<code>u_strands</code>  <code>property</code>","text":"<p>Returns the source strand orientations.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An <code>int8</code> numpy array.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.v","title":"<code>v</code>  <code>property</code>","text":"<p>Returns the target node IDs.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An object array of <code>bytes</code>.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.v_strands","title":"<code>v_strands</code>  <code>property</code>","text":"<p>Returns the target strand orientations.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An <code>int8</code> numpy array.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Constructs an EdgeBatch from an iterable of Edge objects.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>Iterable[Edge]</code> <p>An iterable of <code>Edge</code> objects.</p> required <p>Returns:</p> Type Description <code>EdgeBatch</code> <p>A new <code>EdgeBatch</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = EdgeBatch.build([edge1, edge2])\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable[Edge]) -&gt; 'EdgeBatch':\n    \"\"\"Constructs an EdgeBatch from an iterable of Edge objects.\n\n    Args:\n        components: An iterable of ``Edge`` objects.\n\n    Returns:\n        A new ``EdgeBatch``.\n\n    Examples:\n        &gt;&gt;&gt; batch = EdgeBatch.build([edge1, edge2])\n    \"\"\"\n    edges = list(components)\n    if not edges: return cls.empty()\n    u = np.array([e.u for e in edges])\n    v = np.array([e.v for e in edges])\n    us = np.array([e.u_strand for e in edges], dtype=np.int8)\n    vs = np.array([e.v_strand for e in edges], dtype=np.int8)\n    return cls(u, v, us, vs)\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple EdgeBatch objects into one.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[EdgeBatch]</code> <p>An iterable of <code>EdgeBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>EdgeBatch</code> <p>A single concatenated <code>EdgeBatch</code>.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['EdgeBatch']) -&gt; 'EdgeBatch':\n    \"\"\"Concatenates multiple EdgeBatch objects into one.\n\n    Args:\n        batches: An iterable of ``EdgeBatch`` objects.\n\n    Returns:\n        A single concatenated ``EdgeBatch``.\n    \"\"\"\n    batches = list(batches)\n    if not batches: return cls.empty()\n    u = np.concatenate([b.u for b in batches])\n    v = np.concatenate([b.v for b in batches])\n    us = np.concatenate([b.u_strands for b in batches])\n    vs = np.concatenate([b.v_strands for b in batches])\n    return cls(u, v, us, vs)\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>EdgeBatch</code> <p>A new <code>EdgeBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>def copy(self) -&gt; 'EdgeBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``EdgeBatch`` with copied arrays.\n    \"\"\"\n    # Note: attributes dict copy is shallow for arrays inside\n    return self.__class__(self._u.copy(), self._v.copy(), self._u_strands.copy(), self._v_strands.copy(), self._attributes.copy())\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty EdgeBatch with zero edges.</p> <p>Returns:</p> Type Description <code>EdgeBatch</code> <p>An empty <code>EdgeBatch</code>.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'EdgeBatch':\n    \"\"\"Creates an empty EdgeBatch with zero edges.\n\n    Returns:\n        An empty ``EdgeBatch``.\n    \"\"\"\n    return cls.zeros(0)\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.EdgeBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates an EdgeBatch with n placeholder edges.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of placeholder edge slots.</p> required <p>Returns:</p> Type Description <code>EdgeBatch</code> <p>An <code>EdgeBatch</code> with empty IDs and forward strands.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'EdgeBatch':\n    \"\"\"Creates an EdgeBatch with *n* placeholder edges.\n\n    Args:\n        n: Number of placeholder edge slots.\n\n    Returns:\n        An ``EdgeBatch`` with empty IDs and forward strands.\n    \"\"\"\n    return cls(\n        np.full(n, b'', dtype='S1'),\n        np.full(n, b'', dtype='S1'),\n        np.ones(n, dtype=np.int8),\n        np.ones(n, dtype=np.int8),\n        {}\n    )\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph","title":"<code>Graph</code>","text":"<p>A general-purpose graph supporting directed/undirected and simple/multi-edge topologies.</p> <p>Nodes are identified by <code>bytes</code> IDs and may carry arbitrary attribute dicts. Edges are stored as a set of <code>Edge</code> objects. An internal topology cache (COO arrays) is maintained lazily for efficient matrix construction.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>Iterable[Union[Edge, tuple]]</code> <p>Optional initial edges to add (<code>Edge</code> objects or tuples).</p> <code>None</code> <code>directed</code> <code>bool</code> <p>Whether the graph is directed (default <code>True</code>).</p> <code>True</code> <code>multi</code> <code>bool</code> <p>Whether to allow parallel edges (default <code>True</code>).</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; g = Graph(directed=True)\n&gt;&gt;&gt; g.add_node(b'A')\n&gt;&gt;&gt; g.add_edges([Edge(b'A', b'B')])\n&gt;&gt;&gt; len(g)\n1\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>class Graph:\n    \"\"\"A general-purpose graph supporting directed/undirected and simple/multi-edge topologies.\n\n    Nodes are identified by ``bytes`` IDs and may carry arbitrary attribute dicts.\n    Edges are stored as a set of ``Edge`` objects. An internal topology cache\n    (COO arrays) is maintained lazily for efficient matrix construction.\n\n    Args:\n        edges: Optional initial edges to add (``Edge`` objects or tuples).\n        directed: Whether the graph is directed (default ``True``).\n        multi: Whether to allow parallel edges (default ``True``).\n\n    Examples:\n        &gt;&gt;&gt; g = Graph(directed=True)\n        &gt;&gt;&gt; g.add_node(b'A')\n        &gt;&gt;&gt; g.add_edges([Edge(b'A', b'B')])\n        &gt;&gt;&gt; len(g)\n        1\n    \"\"\"\n    __slots__ = ('_directed', '_multi', '_edges', '_nodes', '_node_to_idx', '_node_attributes',\n                 '_topology_cache', '_topo_lists', '_matrix_cache')\n\n    def __init__(self, edges: Iterable[Union[Edge, tuple]] = None, directed: bool = True, multi: bool = True):\n        self._directed = directed\n        self._multi = multi\n        self._edges: Set[Edge] = set()\n        self._nodes: List[bytes] = []\n        self._node_to_idx: Dict[bytes, int] = {}\n        self._node_attributes: Dict[bytes, dict] = {}\n        self._topology_cache = None\n        self._topo_lists = ([], [], [])\n        # Cache: (attr, source, aggregator, invert, default) -&gt; CSR Matrix\n        self._matrix_cache: Dict['MatrixPolicy', 'csr_matrix'] = {}\n        if edges: self.add_edges(edges)\n\n    @property\n    def directed(self) -&gt; bool:\n        \"\"\"Returns ``True`` if the graph is directed.\n\n        Returns:\n            Direction flag.\n        \"\"\"\n        return self._directed\n\n    @property\n    def multi(self) -&gt; bool:\n        \"\"\"Returns ``True`` if the graph allows parallel edges.\n\n        Returns:\n            Multi-edge flag.\n        \"\"\"\n        return self._multi\n\n    @property\n    def nodes(self) -&gt; List[bytes]:\n        \"\"\"Returns the ordered list of node IDs.\n\n        Returns:\n            A list of ``bytes`` node IDs.\n        \"\"\"\n        return self._nodes\n\n    @property\n    def edges(self) -&gt; Set[Edge]:\n        \"\"\"Returns the set of all edges in the graph.\n\n        Returns:\n            A set of ``Edge`` objects.\n        \"\"\"\n        return self._edges\n\n    @property\n    def node_to_idx(self) -&gt; Dict[bytes, int]:\n        \"\"\"Returns the node-to-index mapping.\n\n        Returns:\n            A dict mapping ``bytes`` node IDs to integer indices.\n        \"\"\"\n        return self._node_to_idx\n\n    @property\n    def node_attributes(self) -&gt; Dict[bytes, dict]:\n        \"\"\"Returns the per-node attribute dictionaries.\n\n        Returns:\n            A dict mapping node IDs to attribute dicts.\n        \"\"\"\n        return self._node_attributes\n\n    @property\n    def topology_cache(self):\n        \"\"\"Returns the COO topology cache, or ``None`` if not yet built.\n\n        Returns:\n            A tuple of ``(u_indices, v_indices, edge_list)`` or ``None``.\n        \"\"\"\n        return self._topology_cache\n\n    @property\n    def matrix_cache(self) -&gt; Dict:\n        \"\"\"Returns the sparse matrix cache.\n\n        Returns:\n            A dict mapping ``MatrixPolicy`` keys to CSR matrices.\n        \"\"\"\n        return self._matrix_cache\n\n    def __repr__(self):\n        type_str = \"Directed\" if self._directed else \"Undirected\"\n        kind_str = \"Multigraph\" if self._multi else \"Graph\"\n        return (f\"{type_str} {kind_str} with \"\n                f\"{len(self._nodes)} nodes and {len(self._edges)} defined edges\")\n\n    def __iter__(self): return iter(self._edges)\n    def __len__(self): return len(self._edges)\n    def __getitem__(self, item): return self._nodes[item]\n\n    def add_node(self, node: Any, attributes: dict = None):\n        \"\"\"Adds a node to the graph, optionally with attributes.\n\n        If the node already exists, its attributes are merged (updated).\n\n        Args:\n            node: The node ID (coerced to ``bytes``).\n            attributes: Optional dictionary of node attributes.\n\n        Examples:\n            &gt;&gt;&gt; g.add_node(b'contig_1', attributes={'length': 5000})\n        \"\"\"\n        node = Edge._coerce_node(node)\n        if node not in self._node_to_idx:\n            self._node_to_idx[node] = len(self._nodes)\n            self._nodes.append(node)\n            if attributes: self._node_attributes[node] = attributes\n            self._topology_cache = None\n            self._matrix_cache.clear()\n        elif attributes:\n            if node not in self._node_attributes: self._node_attributes[node] = {}\n            self._node_attributes[node].update(attributes)\n            self._matrix_cache.clear()\n\n    def add_edges(self, edges: Iterable[Union[Edge, tuple]]):\n        \"\"\"Adds multiple edges to the graph, auto-creating nodes as needed.\n\n        Accepts ``Edge`` objects, ``EdgeBatch`` instances, or tuples that\n        can be unpacked into an ``Edge`` constructor. Invalidates the\n        topology and matrix caches.\n\n        Args:\n            edges: An iterable of ``Edge`` objects, tuples, or an ``EdgeBatch``.\n\n        Examples:\n            &gt;&gt;&gt; g.add_edges([Edge(b'A', b'B'), Edge(b'B', b'C')])\n        \"\"\"\n        # Optimization: Collect unique nodes first to reduce dict lookups from O(E) to O(V)\n        if isinstance(edges, EdgeBatch):\n            # Fast path for batch: access arrays directly\n            potential_nodes = np.unique(np.concatenate((edges.u, edges.v)))\n            edge_objs = edges\n        else:\n            potential_nodes = set()\n            edge_objs = []\n            for e in edges:\n                if not isinstance(e, Edge): e = Edge(*e)\n                edge_objs.append(e)\n                potential_nodes.add(e.u)\n                potential_nodes.add(e.v)\n\n        # Batch add nodes\n        for n in potential_nodes:\n            if n not in self._node_to_idx: # Double check\n                self._node_to_idx[n] = len(self._nodes)\n                self._nodes.append(n)\n\n        # Batch add edges\n        u_list, v_list, e_list = self._topo_lists\n        node_map = self._node_to_idx\n        added = False\n        for e in edge_objs:\n            if e not in self._edges:\n                self._edges.add(e)\n                u_list.append(node_map[e.u])\n                v_list.append(node_map[e.v])\n                e_list.append(e)\n                added = True\n\n        if added: self._topology_cache = None\n        if added: self._matrix_cache.clear()\n\n    def ensure_topology(self):\n        \"\"\"Builds or refreshes the COO topology cache if stale.\n\n        After calling this, ``self.topology_cache`` is guaranteed to be\n        a valid ``(u_indices, v_indices, edge_list)`` tuple.\n        \"\"\"\n        if self._topology_cache is not None: return\n\n        u_list, v_list, e_list = self._topo_lists\n        self._topology_cache = (\n            np.array(u_list, dtype=np.int32),\n            np.array(v_list, dtype=np.int32),\n            e_list\n        )\n\n    def subgraph(self, nodes: Iterable[Any]) -&gt; 'Graph':\n        \"\"\"Returns an induced subgraph containing only the specified nodes.\n\n        Edges are kept only if both endpoints are in the node set.\n\n        Args:\n            nodes: An iterable of node IDs to keep.\n\n        Returns:\n            A new ``Graph`` with the selected nodes and their inter-edges.\n\n        Examples:\n            &gt;&gt;&gt; sub = g.subgraph([b'A', b'B'])\n        \"\"\"\n        keep_set = {Edge._coerce_node(n) for n in nodes}\n        # Filter to only existing nodes\n        keep_set.intersection_update(self._node_to_idx)\n\n        if not keep_set:\n            return Graph(directed=self._directed, multi=self._multi)\n\n        valid_edges = []\n\n        # Use topology cache for vectorized filtering if available\n        if self._topology_cache is not None:\n            u_idx, v_idx, e_list = self._topology_cache\n\n            # Build boolean mask of nodes to keep\n            n_nodes = len(self._nodes)\n            mask = np.zeros(n_nodes, dtype=bool)\n\n            indices = [self._node_to_idx[n] for n in keep_set]\n            mask[indices] = True\n\n            # Find edges where both u and v are in mask\n            edge_mask = mask[u_idx] &amp; mask[v_idx]\n\n            valid_indices = np.flatnonzero(edge_mask)\n            for i in valid_indices:\n                valid_edges.append(e_list[i])\n        else:\n            for e in self._edges:\n                if e.u in keep_set and e.v in keep_set:\n                    valid_edges.append(e)\n\n        sub = Graph(valid_edges, directed=self._directed, multi=self._multi)\n        for n in keep_set:\n            attrs = self._node_attributes.get(n)\n            sub.add_node(n, attributes=attrs.copy() if attrs else None)\n\n        return sub\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.directed","title":"<code>directed</code>  <code>property</code>","text":"<p>Returns <code>True</code> if the graph is directed.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Direction flag.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.edges","title":"<code>edges</code>  <code>property</code>","text":"<p>Returns the set of all edges in the graph.</p> <p>Returns:</p> Type Description <code>Set[Edge]</code> <p>A set of <code>Edge</code> objects.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.matrix_cache","title":"<code>matrix_cache</code>  <code>property</code>","text":"<p>Returns the sparse matrix cache.</p> <p>Returns:</p> Type Description <code>Dict</code> <p>A dict mapping <code>MatrixPolicy</code> keys to CSR matrices.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.multi","title":"<code>multi</code>  <code>property</code>","text":"<p>Returns <code>True</code> if the graph allows parallel edges.</p> <p>Returns:</p> Type Description <code>bool</code> <p>Multi-edge flag.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.node_attributes","title":"<code>node_attributes</code>  <code>property</code>","text":"<p>Returns the per-node attribute dictionaries.</p> <p>Returns:</p> Type Description <code>Dict[bytes, dict]</code> <p>A dict mapping node IDs to attribute dicts.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.node_to_idx","title":"<code>node_to_idx</code>  <code>property</code>","text":"<p>Returns the node-to-index mapping.</p> <p>Returns:</p> Type Description <code>Dict[bytes, int]</code> <p>A dict mapping <code>bytes</code> node IDs to integer indices.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.nodes","title":"<code>nodes</code>  <code>property</code>","text":"<p>Returns the ordered list of node IDs.</p> <p>Returns:</p> Type Description <code>List[bytes]</code> <p>A list of <code>bytes</code> node IDs.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.topology_cache","title":"<code>topology_cache</code>  <code>property</code>","text":"<p>Returns the COO topology cache, or <code>None</code> if not yet built.</p> <p>Returns:</p> Type Description <p>A tuple of <code>(u_indices, v_indices, edge_list)</code> or <code>None</code>.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.add_edges","title":"<code>add_edges(edges)</code>","text":"<p>Adds multiple edges to the graph, auto-creating nodes as needed.</p> <p>Accepts <code>Edge</code> objects, <code>EdgeBatch</code> instances, or tuples that can be unpacked into an <code>Edge</code> constructor. Invalidates the topology and matrix caches.</p> <p>Parameters:</p> Name Type Description Default <code>edges</code> <code>Iterable[Union[Edge, tuple]]</code> <p>An iterable of <code>Edge</code> objects, tuples, or an <code>EdgeBatch</code>.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; g.add_edges([Edge(b'A', b'B'), Edge(b'B', b'C')])\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>def add_edges(self, edges: Iterable[Union[Edge, tuple]]):\n    \"\"\"Adds multiple edges to the graph, auto-creating nodes as needed.\n\n    Accepts ``Edge`` objects, ``EdgeBatch`` instances, or tuples that\n    can be unpacked into an ``Edge`` constructor. Invalidates the\n    topology and matrix caches.\n\n    Args:\n        edges: An iterable of ``Edge`` objects, tuples, or an ``EdgeBatch``.\n\n    Examples:\n        &gt;&gt;&gt; g.add_edges([Edge(b'A', b'B'), Edge(b'B', b'C')])\n    \"\"\"\n    # Optimization: Collect unique nodes first to reduce dict lookups from O(E) to O(V)\n    if isinstance(edges, EdgeBatch):\n        # Fast path for batch: access arrays directly\n        potential_nodes = np.unique(np.concatenate((edges.u, edges.v)))\n        edge_objs = edges\n    else:\n        potential_nodes = set()\n        edge_objs = []\n        for e in edges:\n            if not isinstance(e, Edge): e = Edge(*e)\n            edge_objs.append(e)\n            potential_nodes.add(e.u)\n            potential_nodes.add(e.v)\n\n    # Batch add nodes\n    for n in potential_nodes:\n        if n not in self._node_to_idx: # Double check\n            self._node_to_idx[n] = len(self._nodes)\n            self._nodes.append(n)\n\n    # Batch add edges\n    u_list, v_list, e_list = self._topo_lists\n    node_map = self._node_to_idx\n    added = False\n    for e in edge_objs:\n        if e not in self._edges:\n            self._edges.add(e)\n            u_list.append(node_map[e.u])\n            v_list.append(node_map[e.v])\n            e_list.append(e)\n            added = True\n\n    if added: self._topology_cache = None\n    if added: self._matrix_cache.clear()\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.add_node","title":"<code>add_node(node, attributes=None)</code>","text":"<p>Adds a node to the graph, optionally with attributes.</p> <p>If the node already exists, its attributes are merged (updated).</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Any</code> <p>The node ID (coerced to <code>bytes</code>).</p> required <code>attributes</code> <code>dict</code> <p>Optional dictionary of node attributes.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; g.add_node(b'contig_1', attributes={'length': 5000})\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>def add_node(self, node: Any, attributes: dict = None):\n    \"\"\"Adds a node to the graph, optionally with attributes.\n\n    If the node already exists, its attributes are merged (updated).\n\n    Args:\n        node: The node ID (coerced to ``bytes``).\n        attributes: Optional dictionary of node attributes.\n\n    Examples:\n        &gt;&gt;&gt; g.add_node(b'contig_1', attributes={'length': 5000})\n    \"\"\"\n    node = Edge._coerce_node(node)\n    if node not in self._node_to_idx:\n        self._node_to_idx[node] = len(self._nodes)\n        self._nodes.append(node)\n        if attributes: self._node_attributes[node] = attributes\n        self._topology_cache = None\n        self._matrix_cache.clear()\n    elif attributes:\n        if node not in self._node_attributes: self._node_attributes[node] = {}\n        self._node_attributes[node].update(attributes)\n        self._matrix_cache.clear()\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.ensure_topology","title":"<code>ensure_topology()</code>","text":"<p>Builds or refreshes the COO topology cache if stale.</p> <p>After calling this, <code>self.topology_cache</code> is guaranteed to be a valid <code>(u_indices, v_indices, edge_list)</code> tuple.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>def ensure_topology(self):\n    \"\"\"Builds or refreshes the COO topology cache if stale.\n\n    After calling this, ``self.topology_cache`` is guaranteed to be\n    a valid ``(u_indices, v_indices, edge_list)`` tuple.\n    \"\"\"\n    if self._topology_cache is not None: return\n\n    u_list, v_list, e_list = self._topo_lists\n    self._topology_cache = (\n        np.array(u_list, dtype=np.int32),\n        np.array(v_list, dtype=np.int32),\n        e_list\n    )\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Graph.subgraph","title":"<code>subgraph(nodes)</code>","text":"<p>Returns an induced subgraph containing only the specified nodes.</p> <p>Edges are kept only if both endpoints are in the node set.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>Iterable[Any]</code> <p>An iterable of node IDs to keep.</p> required <p>Returns:</p> Type Description <code>Graph</code> <p>A new <code>Graph</code> with the selected nodes and their inter-edges.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sub = g.subgraph([b'A', b'B'])\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>def subgraph(self, nodes: Iterable[Any]) -&gt; 'Graph':\n    \"\"\"Returns an induced subgraph containing only the specified nodes.\n\n    Edges are kept only if both endpoints are in the node set.\n\n    Args:\n        nodes: An iterable of node IDs to keep.\n\n    Returns:\n        A new ``Graph`` with the selected nodes and their inter-edges.\n\n    Examples:\n        &gt;&gt;&gt; sub = g.subgraph([b'A', b'B'])\n    \"\"\"\n    keep_set = {Edge._coerce_node(n) for n in nodes}\n    # Filter to only existing nodes\n    keep_set.intersection_update(self._node_to_idx)\n\n    if not keep_set:\n        return Graph(directed=self._directed, multi=self._multi)\n\n    valid_edges = []\n\n    # Use topology cache for vectorized filtering if available\n    if self._topology_cache is not None:\n        u_idx, v_idx, e_list = self._topology_cache\n\n        # Build boolean mask of nodes to keep\n        n_nodes = len(self._nodes)\n        mask = np.zeros(n_nodes, dtype=bool)\n\n        indices = [self._node_to_idx[n] for n in keep_set]\n        mask[indices] = True\n\n        # Find edges where both u and v are in mask\n        edge_mask = mask[u_idx] &amp; mask[v_idx]\n\n        valid_indices = np.flatnonzero(edge_mask)\n        for i in valid_indices:\n            valid_edges.append(e_list[i])\n    else:\n        for e in self._edges:\n            if e.u in keep_set and e.v in keep_set:\n                valid_edges.append(e)\n\n    sub = Graph(valid_edges, directed=self._directed, multi=self._multi)\n    for n in keep_set:\n        attrs = self._node_attributes.get(n)\n        sub.add_node(n, attributes=attrs.copy() if attrs else None)\n\n    return sub\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Path","title":"<code>Path</code>","text":"<p>An ordered sequence of node IDs representing a walk through a graph.</p> <p>Parameters:</p> Name Type Description Default <code>nodes</code> <code>list[bytes]</code> <p>List of <code>bytes</code> node IDs in traversal order.</p> required <code>cost</code> <code>float</code> <p>Cumulative edge weight along the path (default <code>0.0</code>).</p> <code>0.0</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; p = Path([b'A', b'B', b'C'], cost=2.5)\n&gt;&gt;&gt; len(p)\n3\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>class Path:\n    \"\"\"An ordered sequence of node IDs representing a walk through a graph.\n\n    Args:\n        nodes: List of ``bytes`` node IDs in traversal order.\n        cost: Cumulative edge weight along the path (default ``0.0``).\n\n    Examples:\n        &gt;&gt;&gt; p = Path([b'A', b'B', b'C'], cost=2.5)\n        &gt;&gt;&gt; len(p)\n        3\n    \"\"\"\n    __slots__ = ('nodes', 'total_cost')\n\n    def __init__(self, nodes: list[bytes], cost: float = 0.0):\n        self.nodes = nodes\n        self.total_cost = cost\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``PathBatch`` class.\n        \"\"\"\n        return PathBatch\n\n    def __len__(self): return len(self.nodes)\n    def __iter__(self): return iter(self.nodes)\n    def __getitem__(self, item): return self.nodes[item]\n    def __add__(self, other: 'Path') -&gt; 'Path':\n        if not isinstance(other, Path): return NotImplemented\n        if not self.nodes or not other.nodes: return self if self.nodes else other\n        if self.nodes[-1] != other.nodes[0]:\n            raise ValueError(f\"Paths are not continuous: {self.nodes[-1]} != {other.nodes[0]}\")\n        return Path(self.nodes + other.nodes[1:], self.total_cost + other.total_cost)\n\n    def __repr__(self): return f\"Path(steps={len(self.nodes)}, cost={self.total_cost:.4f})\"\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.Path.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>PathBatch</code> class.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch","title":"<code>PathBatch</code>","text":"<p>               Bases: <code>RaggedBatch</code></p> <p>Columnar batch of paths using ragged array storage.</p> <p>Stores all node IDs in a flat array with per-path offsets and costs.</p> <p>Parameters:</p> Name Type Description Default <code>flat_nodes</code> <code>ndarray</code> <p>Flat object array of all node IDs concatenated.</p> required <code>offsets</code> <code>ndarray</code> <p><code>int32</code> offset array (length = <code>n_paths + 1</code>).</p> required <code>costs</code> <code>ndarray</code> <p>Optional <code>float32</code> array of per-path costs.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = PathBatch.build([path1, path2])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>class PathBatch(RaggedBatch):\n    \"\"\"\n    Columnar batch of paths using ragged array storage.\n\n    Stores all node IDs in a flat array with per-path offsets and costs.\n\n    Args:\n        flat_nodes: Flat object array of all node IDs concatenated.\n        offsets: ``int32`` offset array (length = ``n_paths + 1``).\n        costs: Optional ``float32`` array of per-path costs.\n\n    Examples:\n        &gt;&gt;&gt; batch = PathBatch.build([path1, path2])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    __slots__ = ('_flat_nodes', '_costs')\n\n    def __init__(self, flat_nodes: np.ndarray, offsets: np.ndarray, costs: np.ndarray = None):\n        super().__init__(offsets)\n        self._flat_nodes = flat_nodes\n        n = len(offsets) - 1\n        self._costs = costs if costs is not None else np.zeros(n, dtype=np.float32)\n\n    @classmethod\n    def empty(cls) -&gt; 'PathBatch':\n        \"\"\"Creates an empty PathBatch with zero paths.\n\n        Returns:\n            An empty ``PathBatch``.\n        \"\"\"\n        return cls.zeros(0)\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'PathBatch':\n        \"\"\"Creates a PathBatch with *n* empty placeholder paths.\n\n        Args:\n            n: Number of placeholder path slots.\n\n        Returns:\n            A ``PathBatch`` with zero-cost, zero-length paths.\n        \"\"\"\n        return cls(\n            np.empty(0, dtype='S1'),\n            np.zeros(n + 1, dtype=np.int32),\n            np.zeros(n, dtype=np.float32)\n        )\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``Path`` class.\n        \"\"\"\n        return Path\n\n    @classmethod\n    def concat(cls, batches: Iterable['PathBatch']) -&gt; 'PathBatch':\n        \"\"\"Concatenates multiple PathBatch objects into one.\n\n        Args:\n            batches: An iterable of ``PathBatch`` objects.\n\n        Returns:\n            A single concatenated ``PathBatch``.\n        \"\"\"\n        batches = list(batches)\n        if not batches: return cls.empty()\n        flat_nodes = np.concatenate([b._flat_nodes for b in batches])\n        costs = np.concatenate([b._costs for b in batches])\n        offsets = cls._stack_offsets(batches)\n        return cls(flat_nodes, offsets, costs)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by all internal arrays.\n        \"\"\"\n        return super().nbytes + self._flat_nodes.nbytes + self._costs.nbytes\n\n    def copy(self) -&gt; 'PathBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``PathBatch`` with copied arrays.\n        \"\"\"\n        return self.__class__(self._flat_nodes.copy(), self._offsets.copy(), self._costs.copy())\n\n    def __repr__(self): return f\"&lt;PathBatch: {len(self)} paths&gt;\"\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            start = self._offsets[item]\n            end = self._offsets[item+1]\n            nodes = self._flat_nodes[start:end]\n            # Convert numpy array of bytes to list of bytes for Path compatibility\n            return Path(list(nodes), cost=self._costs[item])\n\n        if isinstance(item, slice):\n            new_offsets, val_start, val_end = self._get_slice_info(item)\n            return PathBatch(\n                self._flat_nodes[val_start:val_end],\n                new_offsets,\n                self._costs[item]\n            )\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    @classmethod\n    def build(cls, components: Iterable[Path]) -&gt; 'PathBatch':\n        \"\"\"Constructs a PathBatch from an iterable of Path objects.\n\n        Args:\n            components: An iterable of ``Path`` objects.\n\n        Returns:\n            A new ``PathBatch``.\n\n        Examples:\n            &gt;&gt;&gt; batch = PathBatch.build([path1, path2])\n        \"\"\"\n        paths = list(components)\n        if not paths:\n            return cls(np.empty(0, dtype='S1'), np.array([0], dtype=np.int32))\n\n        flat_nodes = []\n        offsets = [0]\n        curr = 0\n        for p in paths:\n            flat_nodes.extend(p.nodes)\n            curr += len(p.nodes)\n            offsets.append(curr)\n\n        return cls(\n            np.array(flat_nodes),\n            np.array(offsets, dtype=np.int32),\n            np.array([p.total_cost for p in paths], dtype=np.float32)\n        )\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>Path</code> class.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by all internal arrays.</p>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Constructs a PathBatch from an iterable of Path objects.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>Iterable[Path]</code> <p>An iterable of <code>Path</code> objects.</p> required <p>Returns:</p> Type Description <code>PathBatch</code> <p>A new <code>PathBatch</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = PathBatch.build([path1, path2])\n</code></pre> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable[Path]) -&gt; 'PathBatch':\n    \"\"\"Constructs a PathBatch from an iterable of Path objects.\n\n    Args:\n        components: An iterable of ``Path`` objects.\n\n    Returns:\n        A new ``PathBatch``.\n\n    Examples:\n        &gt;&gt;&gt; batch = PathBatch.build([path1, path2])\n    \"\"\"\n    paths = list(components)\n    if not paths:\n        return cls(np.empty(0, dtype='S1'), np.array([0], dtype=np.int32))\n\n    flat_nodes = []\n    offsets = [0]\n    curr = 0\n    for p in paths:\n        flat_nodes.extend(p.nodes)\n        curr += len(p.nodes)\n        offsets.append(curr)\n\n    return cls(\n        np.array(flat_nodes),\n        np.array(offsets, dtype=np.int32),\n        np.array([p.total_cost for p in paths], dtype=np.float32)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple PathBatch objects into one.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[PathBatch]</code> <p>An iterable of <code>PathBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>PathBatch</code> <p>A single concatenated <code>PathBatch</code>.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['PathBatch']) -&gt; 'PathBatch':\n    \"\"\"Concatenates multiple PathBatch objects into one.\n\n    Args:\n        batches: An iterable of ``PathBatch`` objects.\n\n    Returns:\n        A single concatenated ``PathBatch``.\n    \"\"\"\n    batches = list(batches)\n    if not batches: return cls.empty()\n    flat_nodes = np.concatenate([b._flat_nodes for b in batches])\n    costs = np.concatenate([b._costs for b in batches])\n    offsets = cls._stack_offsets(batches)\n    return cls(flat_nodes, offsets, costs)\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>PathBatch</code> <p>A new <code>PathBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>def copy(self) -&gt; 'PathBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``PathBatch`` with copied arrays.\n    \"\"\"\n    return self.__class__(self._flat_nodes.copy(), self._offsets.copy(), self._costs.copy())\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty PathBatch with zero paths.</p> <p>Returns:</p> Type Description <code>PathBatch</code> <p>An empty <code>PathBatch</code>.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'PathBatch':\n    \"\"\"Creates an empty PathBatch with zero paths.\n\n    Returns:\n        An empty ``PathBatch``.\n    \"\"\"\n    return cls.zeros(0)\n</code></pre>"},{"location":"reference/baclib/containers/graph/#baclib.containers.graph.PathBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a PathBatch with n empty placeholder paths.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of placeholder path slots.</p> required <p>Returns:</p> Type Description <code>PathBatch</code> <p>A <code>PathBatch</code> with zero-cost, zero-length paths.</p> Source code in <code>baclib/containers/graph.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'PathBatch':\n    \"\"\"Creates a PathBatch with *n* empty placeholder paths.\n\n    Args:\n        n: Number of placeholder path slots.\n\n    Returns:\n        A ``PathBatch`` with zero-cost, zero-length paths.\n    \"\"\"\n    return cls(\n        np.empty(0, dtype='S1'),\n        np.zeros(n + 1, dtype=np.int32),\n        np.zeros(n, dtype=np.float32)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/motif/","title":"motif","text":""},{"location":"reference/baclib/containers/motif/#baclib.containers.motif","title":"<code>baclib.containers.motif</code>","text":"<p>Containers for sequence motifs (PSSMs), background models, and motif scan hits.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Background","title":"<code>Background</code>","text":"<p>               Bases: <code>HasAlphabet</code></p> <p>Represents the background nucleotide frequencies.</p> <p>Used to calculate log-odds scores for motifs. Can be uniform or estimated from sequences or counts.</p> <p>Attributes:</p> Name Type Description <code>alphabet</code> <code>Alphabet</code> <p>The alphabet used.</p> <code>data</code> <code>ndarray</code> <p>Array of probabilities summing to 1.0.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; bg = Background.uniform(Alphabet.DNA)\n&gt;&gt;&gt; bg.data\narray([0.25, 0.25, 0.25, 0.25], dtype=float32)\n</code></pre> Source code in <code>baclib/containers/motif.py</code> <pre><code>class Background(HasAlphabet):\n    \"\"\"\n    Represents the background nucleotide frequencies.\n\n    Used to calculate log-odds scores for motifs. Can be uniform or estimated\n    from sequences or counts.\n\n    Attributes:\n        alphabet (Alphabet): The alphabet used.\n        data (np.ndarray): Array of probabilities summing to 1.0.\n\n    Examples:\n        &gt;&gt;&gt; bg = Background.uniform(Alphabet.DNA)\n        &gt;&gt;&gt; bg.data\n        array([0.25, 0.25, 0.25, 0.25], dtype=float32)\n    \"\"\"\n    __slots__ = ('_alphabet', '_data')\n    _DEFAULT_ALPHABET = Alphabet.DNA\n    _DTYPE = np.float32\n\n    def __init__(self):\n        self._data = None\n        self._alphabet = None\n\n    @property\n    def alphabet(self) -&gt; Alphabet:\n        \"\"\"Returns the alphabet.\n\n        Returns:\n            The ``Alphabet`` object.\n        \"\"\"\n        return self._alphabet\n\n    @property\n    def data(self) -&gt; np.ndarray:\n        \"\"\"Returns the background probabilities.\n\n        Returns:\n            A read-only numpy array of floats.\n        \"\"\"\n        return self._data\n\n    @classmethod\n    def from_counts(cls, counts: np.ndarray, alphabet: Alphabet = None) -&gt; 'Background':\n        \"\"\"Creates a Background model from raw counts.\n\n        Args:\n            counts: Array of counts corresponding to alphabet symbols.\n            alphabet: The ``Alphabet`` (defaults to DNA).\n\n        Returns:\n            A new ``Background`` instance.\n        \"\"\"\n        alphabet = alphabet or cls._DEFAULT_ALPHABET\n        assert len(counts) == len(alphabet)\n        new = cls()\n        new._alphabet = alphabet\n        new._data = counts / np.sum(counts, dtype=cls._DTYPE)\n        new._data.flags.writeable = False\n        return new\n\n    @classmethod\n    def from_seq(cls, seq: Union[Seq, SeqBatch]) -&gt; 'Background':\n        \"\"\"Estimates background frequencies from a sequence or batch.\n\n        Args:\n            seq: A ``Seq`` or ``SeqBatch``.\n\n        Returns:\n            A new ``Background`` instance.\n        \"\"\"\n        return cls.from_counts(np.bincount(seq.encoded), seq.alphabet)\n\n    @classmethod\n    def uniform(cls, alphabet: Alphabet = None) -&gt; 'Background':\n        \"\"\"Creates a uniform Background model (equal probability for all symbols).\n\n        Args:\n            alphabet: The ``Alphabet`` (defaults to DNA).\n\n        Returns:\n            A new ``Background`` instance.\n        \"\"\"\n        alphabet = alphabet or cls._DEFAULT_ALPHABET\n        new = cls()\n        new._alphabet = alphabet\n        n = len(alphabet)\n        new._data = np.ones(n, dtype=cls._DTYPE) / n\n        new._data.flags.writeable = False\n        return new\n\n    def __array__(self, dtype=None) -&gt; np.ndarray:\n        return self._data if dtype is None else self._data.astype(dtype)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Background.alphabet","title":"<code>alphabet</code>  <code>property</code>","text":"<p>Returns the alphabet.</p> <p>Returns:</p> Type Description <code>Alphabet</code> <p>The <code>Alphabet</code> object.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Background.data","title":"<code>data</code>  <code>property</code>","text":"<p>Returns the background probabilities.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A read-only numpy array of floats.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Background.from_counts","title":"<code>from_counts(counts, alphabet=None)</code>  <code>classmethod</code>","text":"<p>Creates a Background model from raw counts.</p> <p>Parameters:</p> Name Type Description Default <code>counts</code> <code>ndarray</code> <p>Array of counts corresponding to alphabet symbols.</p> required <code>alphabet</code> <code>Alphabet</code> <p>The <code>Alphabet</code> (defaults to DNA).</p> <code>None</code> <p>Returns:</p> Type Description <code>Background</code> <p>A new <code>Background</code> instance.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef from_counts(cls, counts: np.ndarray, alphabet: Alphabet = None) -&gt; 'Background':\n    \"\"\"Creates a Background model from raw counts.\n\n    Args:\n        counts: Array of counts corresponding to alphabet symbols.\n        alphabet: The ``Alphabet`` (defaults to DNA).\n\n    Returns:\n        A new ``Background`` instance.\n    \"\"\"\n    alphabet = alphabet or cls._DEFAULT_ALPHABET\n    assert len(counts) == len(alphabet)\n    new = cls()\n    new._alphabet = alphabet\n    new._data = counts / np.sum(counts, dtype=cls._DTYPE)\n    new._data.flags.writeable = False\n    return new\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Background.from_seq","title":"<code>from_seq(seq)</code>  <code>classmethod</code>","text":"<p>Estimates background frequencies from a sequence or batch.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Union[Seq, SeqBatch]</code> <p>A <code>Seq</code> or <code>SeqBatch</code>.</p> required <p>Returns:</p> Type Description <code>Background</code> <p>A new <code>Background</code> instance.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef from_seq(cls, seq: Union[Seq, SeqBatch]) -&gt; 'Background':\n    \"\"\"Estimates background frequencies from a sequence or batch.\n\n    Args:\n        seq: A ``Seq`` or ``SeqBatch``.\n\n    Returns:\n        A new ``Background`` instance.\n    \"\"\"\n    return cls.from_counts(np.bincount(seq.encoded), seq.alphabet)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Background.uniform","title":"<code>uniform(alphabet=None)</code>  <code>classmethod</code>","text":"<p>Creates a uniform Background model (equal probability for all symbols).</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>Alphabet</code> <p>The <code>Alphabet</code> (defaults to DNA).</p> <code>None</code> <p>Returns:</p> Type Description <code>Background</code> <p>A new <code>Background</code> instance.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef uniform(cls, alphabet: Alphabet = None) -&gt; 'Background':\n    \"\"\"Creates a uniform Background model (equal probability for all symbols).\n\n    Args:\n        alphabet: The ``Alphabet`` (defaults to DNA).\n\n    Returns:\n        A new ``Background`` instance.\n    \"\"\"\n    alphabet = alphabet or cls._DEFAULT_ALPHABET\n    new = cls()\n    new._alphabet = alphabet\n    n = len(alphabet)\n    new._data = np.ones(n, dtype=cls._DTYPE) / n\n    new._data.flags.writeable = False\n    return new\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif","title":"<code>Motif</code>","text":"<p>Position Specific Scoring Matrix (PSSM) for sequence motifs.</p> <p>Represents a motif model with log-odds scores relative to a background. Supports P-value calculation using discretized dynamic programming.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>bytes</code> <p>Motif name.</p> <code>background</code> <code>Background</code> <p>Background model used for scoring.</p> <code>pssm</code> <code>ndarray</code> <p>The log-odds scoring matrix (Bases x Positions).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = Motif.sigma70_35()\n&gt;&gt;&gt; len(m)\n6\n</code></pre> Source code in <code>baclib/containers/motif.py</code> <pre><code>class Motif:\n    \"\"\"\n    Position Specific Scoring Matrix (PSSM) for sequence motifs.\n\n    Represents a motif model with log-odds scores relative to a background.\n    Supports P-value calculation using discretized dynamic programming.\n\n    Attributes:\n        name (bytes): Motif name.\n        background (Background): Background model used for scoring.\n        pssm (np.ndarray): The log-odds scoring matrix (Bases x Positions).\n\n    Examples:\n        &gt;&gt;&gt; m = Motif.sigma70_35()\n        &gt;&gt;&gt; len(m)\n        6\n    \"\"\"\n    _DTYPE = np.float32\n    _DEFAULT_GRANULARITY = 1000\n    _DEFAULT_PSEUDOCOUNT = 0.1\n    __slots__ = ('_name', '_background', '_count', '_discrete', '_frequency', '_scoring', '_weight',\n                 '_granularity', '_pseudocount', '_min_score', '_max_score', '_pdf', '_max_suffix', '_max_suffix_rc')\n\n    def __init__(self, name: bytes, scores: np.ndarray, background: Background, granularity: int = _DEFAULT_GRANULARITY,\n                 pseudocount: float = _DEFAULT_PSEUDOCOUNT, counts: np.ndarray = None, frequencies: np.ndarray = None,\n                 weights: np.ndarray = None):\n        self._name: bytes = name\n        self._background: Background = background\n        self._granularity: int = granularity\n        self._pseudocount: float = pseudocount\n        self._count: Optional[np.ndarray] = counts  # A matrix storing symbol occurrences at each position.\n        self._frequency: Optional[np.ndarray] = frequencies  # A matrix storing symbol frequencies at each position.\n        self._weight: Optional[\n            np.ndarray] = weights  # A matrix storing odds ratio of symbol occurrences at each position.\n        self._scoring: np.ndarray = scores  # A matrix storing log-odds ratio of symbol occurrences at each position.\n        self._discrete = np.round(self._scoring * self._granularity).astype(\n            np.int32)  # A position-specific scoring matrix discretized over u8::MIN..u8::MAX.\n        self._min_score = self._discrete.min(axis=0).sum()\n        self._max_score = self._discrete.max(axis=0).sum()\n        self._pdf = None\n        self._max_suffix = _calc_max_suffix(self._scoring)\n        self._max_suffix_rc = _calc_max_suffix(self.pssm_rc) if self.pssm_rc is not None else None\n        # Enforce immutability for thread safety\n        for array in [self._count, self._frequency, self._weight, self._scoring, self._discrete]:\n            if array is not None: array.flags.writeable = False\n\n    def __repr__(self):\n        return f\"&lt;Motif: {self.name.decode(errors='ignore')}, len={len(self)}&gt;\"\n\n    def __len__(self) -&gt; int:\n        return len(self._scoring.shape[1])\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``MotifBatch`` class.\n        \"\"\"\n        return MotifBatch\n\n    @property\n    def name(self) -&gt; bytes:\n        \"\"\"Returns the motif name as bytes.\n\n        Returns:\n            Motif name.\n        \"\"\"\n        return self._name\n\n    @property\n    def background(self) -&gt; Background:\n        \"\"\"Returns the background model.\n\n        Returns:\n            The ``Background`` object.\n        \"\"\"\n        return self._background\n\n    @property\n    def pssm(self) -&gt; np.ndarray:\n        \"\"\"Returns the log-odds scoring matrix.\n\n        Returns:\n            A (Bases, Positions) numpy array.\n        \"\"\"\n        return self._scoring\n\n    @property\n    def pssm_rc(self) -&gt; Optional[np.ndarray]:\n        \"\"\"Returns the reverse complement PSSM.\n\n        Returns:\n            A (Bases, Positions) numpy array or ``None`` if alphabet has no complement.\n        \"\"\"\n        comp = self.background.alphabet.complement\n        if comp is None: return None\n        return np.ascontiguousarray(self._scoring[comp, ::-1])\n\n    @classmethod\n    def from_counts(cls, name: bytes, counts: np.ndarray, background: Background,\n                    granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT) -&gt; 'Motif':\n        \"\"\"Creates a Motif from a count matrix (Rows=Bases, Cols=Positions).\n\n        Args:\n            name: Motif name.\n            counts: Matrix of observation counts.\n            background: Background model.\n            granularity: Discretization factor for P-value calculation.\n            pseudocount: Pseudocount added to avoid zero probabilities.\n\n        Returns:\n            A new ``Motif``.\n        \"\"\"\n        adjusted = counts + pseudocount\n        frequencies = adjusted / adjusted.sum(axis=0)\n        return cls.from_frequencies(name, frequencies, background, granularity, pseudocount, counts)\n\n    @classmethod\n    def from_frequencies(cls, name: bytes, frequencies: np.ndarray, background: Background,\n                         granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT,\n                         counts: np.ndarray = None) -&gt; 'Motif':\n        \"\"\"Creates a Motif from a frequency matrix.\n\n        Args:\n            name: Motif name.\n            frequencies: Matrix of probabilities (columns sum to 1).\n            background: Background model.\n            granularity: Discretization factor for P-value calculation.\n            pseudocount: Pseudocount used in derivation (optional tracking).\n            counts: Original counts (optional tracking).\n\n        Returns:\n            A new ``Motif``.\n        \"\"\"\n        weights = frequencies / background.data[:, None]\n        return cls.from_weights(name, weights, background, granularity, pseudocount, counts, frequencies)\n\n    @classmethod\n    def from_weights(cls, name: bytes, weights: np.ndarray, background: Background,\n                     granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT,\n                     counts: np.ndarray = None,\n                     frequencies: np.ndarray = None) -&gt; 'Motif':\n        \"\"\"Creates a Motif from a weight matrix (odds ratios).\n\n        Args:\n            name: Motif name.\n            weights: Matrix of odds ratios.\n            background: Background model.\n\n        Returns:\n            A new ``Motif``.\n        \"\"\"\n        with np.errstate(divide='ignore', invalid='ignore'):\n            scores = np.log2(weights, dtype=cls._DTYPE)\n\n        # Clamp lower bound to -100.0 bits to prevent:\n        # 1. -inf causing NaN/Errors in integer conversion\n        # 2. Massive DP tables in p-value calculation\n        min_score = -100.0\n        scores[scores &lt; min_score] = min_score\n        scores[np.isnan(scores)] = min_score\n\n        return cls.from_scores(name, scores, background, granularity, pseudocount, counts, frequencies, weights)\n\n    @classmethod\n    def from_scores(cls, name: bytes, scores: np.ndarray, background: Background,\n                    granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT,\n                    counts: np.ndarray = None, frequencies: np.ndarray = None, weights: np.ndarray = None) -&gt; 'Motif':\n        \"\"\"Creates a Motif directly from a log-odds score matrix.\n\n        Args:\n            name: Motif name.\n            scores: Log-odds score matrix.\n            background: Background model.\n\n        Returns:\n            A new ``Motif``.\n        \"\"\"\n        return cls(name, scores, background, granularity, pseudocount, counts, frequencies, weights)\n\n    def pdf(self) -&gt; np.ndarray:\n        \"\"\"Computes the probability density function of scores.\n\n        Uses dynamic programming to compute the exact distribution of\n        discretized scores.\n\n        Returns:\n            An array where index corresponds to (score - min_score).\n        \"\"\"\n        if self._pdf is None:\n            bg = self._background.data.astype(np.float64)\n            self._pdf = _score_distribution_kernel(self._discrete, bg, self._min_score, self._max_score)\n            self._pdf.flags.writeable = False\n        return self._pdf\n\n    def get_score(self, p_value: float) -&gt; float:\n        \"\"\"Calculates the score threshold for a given P-value.\n\n        Args:\n            p_value: Target P-value (e.g. 1e-4).\n\n        Returns:\n            The score threshold.\n        \"\"\"\n        # Calculate CCDF (Survival Function) to find P-value\n        # We walk backwards from the max score until sum(probs) &gt;= p_value\n        cumulative = 0.0\n        threshold_int = self._min_score\n        pdf = self.pdf()\n        for s in range(len(pdf) - 1, -1, -1):\n            cumulative += pdf[s]\n            if cumulative &gt; p_value:\n                threshold_int = s + 1 + self._min_score\n                break\n        if threshold_int &gt; self._max_score: threshold_int = self._max_score\n        return threshold_int / self._granularity\n\n    def get_pvalue(self, score: float) -&gt; float:\n        \"\"\"Calculates the P-value for a given score.\n\n        Args:\n            score: The raw score.\n\n        Returns:\n            The probability of observing a score &gt;= *score* by chance.\n        \"\"\"\n        target_int = int(round(score * self._granularity))\n        idx = target_int - self._min_score\n        if idx &lt; 0: return 1.0\n        pdf = self.pdf()\n        if idx &gt;= len(pdf): return 0.0\n        return pdf[idx:].sum()\n\n    def scan(self, seqs: Union[Seq, SeqBatch], pvalue_threshold: float = 1e-4) -&gt; 'MotifHitBatch':\n        \"\"\"Scans sequences for this motif. Use ``MotifScanner`` for batch scanning.\n\n        Raises:\n            TypeError: Direct scanning not supported here.\n        \"\"\"\n        raise TypeError(\"Use baclib.engines.motif.MotifScanner(motif).scan(seqs) instead.\")\n\n    @classmethod\n    def sigma70_35(cls, bg: Background = None) -&gt; 'Motif':\n        \"\"\"Returns a built-in Sigma70 -35 box motif (E. coli).\"\"\"\n        # -35 Motif Consensus: T T G A C A\n        # Strong conservation at pos 0, 1, 2. Weaker at 3, 4, 5.\n        counts = np.array([\n            [820, 840, 90, 240, 180, 180],  # T\n            [50, 50, 60, 100, 540, 140],  # C\n            [69, 50, 60, 560, 140, 540],  # A\n            [61, 60, 790, 100, 140, 140],  # G\n        ], dtype=cls._DTYPE)\n        dna = Alphabet.DNA\n        if bg is not None:\n            if bg.alphabet != dna: raise ValueError('Background alphabet must be dna for this motif')\n        else:\n            bg = Background.uniform(dna)\n        return cls.from_counts(b\"Sigma70_-35\", counts, bg)\n\n    @classmethod\n    def sigma70_10(cls, bg: Background = None) -&gt; 'Motif':\n        \"\"\"Returns a built-in Sigma70 -10 box motif (E. coli).\"\"\"\n        # -10 Motif Consensus: T A T A A T\n        # Very strong conservation at pos 1 (A) and 5 (T).\n        # Pos 2 (T) is often variable (T or C).\n        counts = np.array([\n            [770, 30, 450, 140, 120, 960],  # T\n            [80, 10, 140, 140, 230, 20],  # C\n            [50, 950, 260, 590, 510, 10],  # A\n            [100, 10, 150, 130, 140, 10],  # G\n        ], dtype=cls._DTYPE)\n        dna = Alphabet.DNA\n        if bg is not None:\n            if bg.alphabet != dna: raise ValueError('Background alphabet must be dna for this motif')\n        else:\n            bg = Background.uniform(dna)\n        return cls.from_counts(b\"Sigma70_-10\", counts, bg)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.background","title":"<code>background</code>  <code>property</code>","text":"<p>Returns the background model.</p> <p>Returns:</p> Type Description <code>Background</code> <p>The <code>Background</code> object.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>MotifBatch</code> class.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns the motif name as bytes.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>Motif name.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.pssm","title":"<code>pssm</code>  <code>property</code>","text":"<p>Returns the log-odds scoring matrix.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A (Bases, Positions) numpy array.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.pssm_rc","title":"<code>pssm_rc</code>  <code>property</code>","text":"<p>Returns the reverse complement PSSM.</p> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>A (Bases, Positions) numpy array or <code>None</code> if alphabet has no complement.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.from_counts","title":"<code>from_counts(name, counts, background, granularity=_DEFAULT_GRANULARITY, pseudocount=_DEFAULT_PSEUDOCOUNT)</code>  <code>classmethod</code>","text":"<p>Creates a Motif from a count matrix (Rows=Bases, Cols=Positions).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>bytes</code> <p>Motif name.</p> required <code>counts</code> <code>ndarray</code> <p>Matrix of observation counts.</p> required <code>background</code> <code>Background</code> <p>Background model.</p> required <code>granularity</code> <code>int</code> <p>Discretization factor for P-value calculation.</p> <code>_DEFAULT_GRANULARITY</code> <code>pseudocount</code> <code>float</code> <p>Pseudocount added to avoid zero probabilities.</p> <code>_DEFAULT_PSEUDOCOUNT</code> <p>Returns:</p> Type Description <code>Motif</code> <p>A new <code>Motif</code>.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef from_counts(cls, name: bytes, counts: np.ndarray, background: Background,\n                granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT) -&gt; 'Motif':\n    \"\"\"Creates a Motif from a count matrix (Rows=Bases, Cols=Positions).\n\n    Args:\n        name: Motif name.\n        counts: Matrix of observation counts.\n        background: Background model.\n        granularity: Discretization factor for P-value calculation.\n        pseudocount: Pseudocount added to avoid zero probabilities.\n\n    Returns:\n        A new ``Motif``.\n    \"\"\"\n    adjusted = counts + pseudocount\n    frequencies = adjusted / adjusted.sum(axis=0)\n    return cls.from_frequencies(name, frequencies, background, granularity, pseudocount, counts)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.from_frequencies","title":"<code>from_frequencies(name, frequencies, background, granularity=_DEFAULT_GRANULARITY, pseudocount=_DEFAULT_PSEUDOCOUNT, counts=None)</code>  <code>classmethod</code>","text":"<p>Creates a Motif from a frequency matrix.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>bytes</code> <p>Motif name.</p> required <code>frequencies</code> <code>ndarray</code> <p>Matrix of probabilities (columns sum to 1).</p> required <code>background</code> <code>Background</code> <p>Background model.</p> required <code>granularity</code> <code>int</code> <p>Discretization factor for P-value calculation.</p> <code>_DEFAULT_GRANULARITY</code> <code>pseudocount</code> <code>float</code> <p>Pseudocount used in derivation (optional tracking).</p> <code>_DEFAULT_PSEUDOCOUNT</code> <code>counts</code> <code>ndarray</code> <p>Original counts (optional tracking).</p> <code>None</code> <p>Returns:</p> Type Description <code>Motif</code> <p>A new <code>Motif</code>.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef from_frequencies(cls, name: bytes, frequencies: np.ndarray, background: Background,\n                     granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT,\n                     counts: np.ndarray = None) -&gt; 'Motif':\n    \"\"\"Creates a Motif from a frequency matrix.\n\n    Args:\n        name: Motif name.\n        frequencies: Matrix of probabilities (columns sum to 1).\n        background: Background model.\n        granularity: Discretization factor for P-value calculation.\n        pseudocount: Pseudocount used in derivation (optional tracking).\n        counts: Original counts (optional tracking).\n\n    Returns:\n        A new ``Motif``.\n    \"\"\"\n    weights = frequencies / background.data[:, None]\n    return cls.from_weights(name, weights, background, granularity, pseudocount, counts, frequencies)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.from_scores","title":"<code>from_scores(name, scores, background, granularity=_DEFAULT_GRANULARITY, pseudocount=_DEFAULT_PSEUDOCOUNT, counts=None, frequencies=None, weights=None)</code>  <code>classmethod</code>","text":"<p>Creates a Motif directly from a log-odds score matrix.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>bytes</code> <p>Motif name.</p> required <code>scores</code> <code>ndarray</code> <p>Log-odds score matrix.</p> required <code>background</code> <code>Background</code> <p>Background model.</p> required <p>Returns:</p> Type Description <code>Motif</code> <p>A new <code>Motif</code>.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef from_scores(cls, name: bytes, scores: np.ndarray, background: Background,\n                granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT,\n                counts: np.ndarray = None, frequencies: np.ndarray = None, weights: np.ndarray = None) -&gt; 'Motif':\n    \"\"\"Creates a Motif directly from a log-odds score matrix.\n\n    Args:\n        name: Motif name.\n        scores: Log-odds score matrix.\n        background: Background model.\n\n    Returns:\n        A new ``Motif``.\n    \"\"\"\n    return cls(name, scores, background, granularity, pseudocount, counts, frequencies, weights)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.from_weights","title":"<code>from_weights(name, weights, background, granularity=_DEFAULT_GRANULARITY, pseudocount=_DEFAULT_PSEUDOCOUNT, counts=None, frequencies=None)</code>  <code>classmethod</code>","text":"<p>Creates a Motif from a weight matrix (odds ratios).</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>bytes</code> <p>Motif name.</p> required <code>weights</code> <code>ndarray</code> <p>Matrix of odds ratios.</p> required <code>background</code> <code>Background</code> <p>Background model.</p> required <p>Returns:</p> Type Description <code>Motif</code> <p>A new <code>Motif</code>.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef from_weights(cls, name: bytes, weights: np.ndarray, background: Background,\n                 granularity: int = _DEFAULT_GRANULARITY, pseudocount: float = _DEFAULT_PSEUDOCOUNT,\n                 counts: np.ndarray = None,\n                 frequencies: np.ndarray = None) -&gt; 'Motif':\n    \"\"\"Creates a Motif from a weight matrix (odds ratios).\n\n    Args:\n        name: Motif name.\n        weights: Matrix of odds ratios.\n        background: Background model.\n\n    Returns:\n        A new ``Motif``.\n    \"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        scores = np.log2(weights, dtype=cls._DTYPE)\n\n    # Clamp lower bound to -100.0 bits to prevent:\n    # 1. -inf causing NaN/Errors in integer conversion\n    # 2. Massive DP tables in p-value calculation\n    min_score = -100.0\n    scores[scores &lt; min_score] = min_score\n    scores[np.isnan(scores)] = min_score\n\n    return cls.from_scores(name, scores, background, granularity, pseudocount, counts, frequencies, weights)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.get_pvalue","title":"<code>get_pvalue(score)</code>","text":"<p>Calculates the P-value for a given score.</p> <p>Parameters:</p> Name Type Description Default <code>score</code> <code>float</code> <p>The raw score.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The probability of observing a score &gt;= score by chance.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def get_pvalue(self, score: float) -&gt; float:\n    \"\"\"Calculates the P-value for a given score.\n\n    Args:\n        score: The raw score.\n\n    Returns:\n        The probability of observing a score &gt;= *score* by chance.\n    \"\"\"\n    target_int = int(round(score * self._granularity))\n    idx = target_int - self._min_score\n    if idx &lt; 0: return 1.0\n    pdf = self.pdf()\n    if idx &gt;= len(pdf): return 0.0\n    return pdf[idx:].sum()\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.get_score","title":"<code>get_score(p_value)</code>","text":"<p>Calculates the score threshold for a given P-value.</p> <p>Parameters:</p> Name Type Description Default <code>p_value</code> <code>float</code> <p>Target P-value (e.g. 1e-4).</p> required <p>Returns:</p> Type Description <code>float</code> <p>The score threshold.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def get_score(self, p_value: float) -&gt; float:\n    \"\"\"Calculates the score threshold for a given P-value.\n\n    Args:\n        p_value: Target P-value (e.g. 1e-4).\n\n    Returns:\n        The score threshold.\n    \"\"\"\n    # Calculate CCDF (Survival Function) to find P-value\n    # We walk backwards from the max score until sum(probs) &gt;= p_value\n    cumulative = 0.0\n    threshold_int = self._min_score\n    pdf = self.pdf()\n    for s in range(len(pdf) - 1, -1, -1):\n        cumulative += pdf[s]\n        if cumulative &gt; p_value:\n            threshold_int = s + 1 + self._min_score\n            break\n    if threshold_int &gt; self._max_score: threshold_int = self._max_score\n    return threshold_int / self._granularity\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.pdf","title":"<code>pdf()</code>","text":"<p>Computes the probability density function of scores.</p> <p>Uses dynamic programming to compute the exact distribution of discretized scores.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An array where index corresponds to (score - min_score).</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def pdf(self) -&gt; np.ndarray:\n    \"\"\"Computes the probability density function of scores.\n\n    Uses dynamic programming to compute the exact distribution of\n    discretized scores.\n\n    Returns:\n        An array where index corresponds to (score - min_score).\n    \"\"\"\n    if self._pdf is None:\n        bg = self._background.data.astype(np.float64)\n        self._pdf = _score_distribution_kernel(self._discrete, bg, self._min_score, self._max_score)\n        self._pdf.flags.writeable = False\n    return self._pdf\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.scan","title":"<code>scan(seqs, pvalue_threshold=0.0001)</code>","text":"<p>Scans sequences for this motif. Use <code>MotifScanner</code> for batch scanning.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Direct scanning not supported here.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def scan(self, seqs: Union[Seq, SeqBatch], pvalue_threshold: float = 1e-4) -&gt; 'MotifHitBatch':\n    \"\"\"Scans sequences for this motif. Use ``MotifScanner`` for batch scanning.\n\n    Raises:\n        TypeError: Direct scanning not supported here.\n    \"\"\"\n    raise TypeError(\"Use baclib.engines.motif.MotifScanner(motif).scan(seqs) instead.\")\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.sigma70_10","title":"<code>sigma70_10(bg=None)</code>  <code>classmethod</code>","text":"<p>Returns a built-in Sigma70 -10 box motif (E. coli).</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef sigma70_10(cls, bg: Background = None) -&gt; 'Motif':\n    \"\"\"Returns a built-in Sigma70 -10 box motif (E. coli).\"\"\"\n    # -10 Motif Consensus: T A T A A T\n    # Very strong conservation at pos 1 (A) and 5 (T).\n    # Pos 2 (T) is often variable (T or C).\n    counts = np.array([\n        [770, 30, 450, 140, 120, 960],  # T\n        [80, 10, 140, 140, 230, 20],  # C\n        [50, 950, 260, 590, 510, 10],  # A\n        [100, 10, 150, 130, 140, 10],  # G\n    ], dtype=cls._DTYPE)\n    dna = Alphabet.DNA\n    if bg is not None:\n        if bg.alphabet != dna: raise ValueError('Background alphabet must be dna for this motif')\n    else:\n        bg = Background.uniform(dna)\n    return cls.from_counts(b\"Sigma70_-10\", counts, bg)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.Motif.sigma70_35","title":"<code>sigma70_35(bg=None)</code>  <code>classmethod</code>","text":"<p>Returns a built-in Sigma70 -35 box motif (E. coli).</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef sigma70_35(cls, bg: Background = None) -&gt; 'Motif':\n    \"\"\"Returns a built-in Sigma70 -35 box motif (E. coli).\"\"\"\n    # -35 Motif Consensus: T T G A C A\n    # Strong conservation at pos 0, 1, 2. Weaker at 3, 4, 5.\n    counts = np.array([\n        [820, 840, 90, 240, 180, 180],  # T\n        [50, 50, 60, 100, 540, 140],  # C\n        [69, 50, 60, 560, 140, 540],  # A\n        [61, 60, 790, 100, 140, 140],  # G\n    ], dtype=cls._DTYPE)\n    dna = Alphabet.DNA\n    if bg is not None:\n        if bg.alphabet != dna: raise ValueError('Background alphabet must be dna for this motif')\n    else:\n        bg = Background.uniform(dna)\n    return cls.from_counts(b\"Sigma70_-35\", counts, bg)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch","title":"<code>MotifBatch</code>","text":"<p>               Bases: <code>Batch</code></p> <p>Efficiently stores and scans multiple motifs.</p> <p>Concatenates PSSMs into a single large matrix for vectorized scanning.</p> <p>Parameters:</p> Name Type Description Default <code>motifs</code> <code>Iterable[Motif]</code> <p>Iterable of <code>Motif</code> objects (must share alphabet).</p> required Source code in <code>baclib/containers/motif.py</code> <pre><code>class MotifBatch(Batch):\n    \"\"\"\n    Efficiently stores and scans multiple motifs.\n\n    Concatenates PSSMs into a single large matrix for vectorized scanning.\n\n    Args:\n        motifs: Iterable of ``Motif`` objects (must share alphabet).\n    \"\"\"\n    __slots__ = ('_motifs', '_pssm_combined', '_pssm_rc_combined', '_offsets', '_max_suffixes', '_max_suffixes_rc')\n\n    @classmethod\n    def empty(cls) -&gt; 'MotifBatch':\n        \"\"\"Creates an empty MotifBatch.\"\"\"\n        return cls([])\n\n    def __init__(self, motifs: Iterable[Motif]):\n        self._motifs = list(motifs)\n        n = len(self._motifs)\n        if n == 0:\n            self._offsets = np.zeros(1, dtype=np.int32)\n            return\n\n        # If created via empty(), we might have no motifs but valid state\n        if n == 0 and not hasattr(self, '_offsets'):\n             self._offsets = np.zeros(1, dtype=np.int32)\n\n        # 1. Validate Alphabets\n        alpha = self._motifs[0].background.alphabet\n        for m in self._motifs:\n            if m.background.alphabet != alpha:\n                raise ValueError(\"All motifs in a batch must share the same alphabet.\")\n\n        # 2. Concatenate PSSMs (4, Total_Len)\n        pssm_list = [m.pssm for m in self._motifs]\n        self._pssm_combined = np.concatenate(pssm_list, axis=1)\n\n        # 3. Concatenate RC PSSMs\n        pssm_rc_list = [m.pssm_rc for m in self._motifs]\n        if any(p is None for p in pssm_rc_list):\n            self._pssm_rc_combined = None\n        else:\n            self._pssm_rc_combined = np.concatenate(pssm_rc_list, axis=1)\n\n        # 4. Offsets\n        lengths = [m.pssm.shape[1] for m in self._motifs]\n        self._offsets = np.zeros(n + 1, dtype=np.int32)\n        np.cumsum(lengths, out=self._offsets[1:])\n\n        # 5. Max Suffixes\n        suffixes = [_calc_max_suffix(p) for p in pssm_list]\n        self._max_suffixes = np.concatenate(suffixes)\n\n        if self._pssm_rc_combined is not None:\n            suffixes_rc = [_calc_max_suffix(p) for p in pssm_rc_list]\n            self._max_suffixes_rc = np.concatenate(suffixes_rc)\n        else:\n            self._max_suffixes_rc = None\n\n    @classmethod\n    def build(cls, components: Iterable[object]) -&gt; 'Batch':\n        \"\"\"Constructs a MotifBatch from an iterable of Motifs.\"\"\"\n        return cls(components)\n\n    @classmethod\n    def concat(cls, batches: Iterable['MotifBatch']) -&gt; 'MotifBatch':\n        \"\"\"Concatenates multiple MotifBatch objects.\"\"\"\n        # Simple list concatenation and rebuild\n        all_motifs = []\n        for b in batches:\n            all_motifs.extend(b._motifs)\n        return cls(all_motifs)\n\n    @property\n    def n_motifs(self) -&gt; int:\n        \"\"\"Returns the number of motifs in the batch.\"\"\"\n        return len(self._motifs)\n\n    def scan(self, seqs: SeqBatch, pvalue_threshold: float = 1e-4) -&gt; 'MotifHitBatch':\n        \"\"\"Scans sequences for these motifs. Use ``MotifScanner`` for batch scanning.\n\n        Raises:\n            TypeError: Direct scanning not supported.\n        \"\"\"\n        raise TypeError(\"Use baclib.engines.motif.MotifScanner(batch).scan(seqs) instead.\")\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\"\"\"\n        size = self._offsets.nbytes + self._max_suffixes.nbytes + self._pssm_combined.nbytes\n        if self._pssm_rc_combined is not None: size += self._pssm_rc_combined.nbytes\n        if self._max_suffixes_rc is not None: size += self._max_suffixes_rc.nbytes\n        return size\n\n    def copy(self) -&gt; 'MotifBatch':\n        \"\"\"Returns a copy of the batch.\"\"\"\n        # Motifs are effectively immutable, but we can return a new wrapper\n        return self.__class__(list(self._motifs))\n\n    @property\n    def component(self): return Motif\n\n    def __repr__(self):\n        return f\"&lt;MotifBatch: {len(self)} motifs&gt;\"\n\n    def __len__(self):\n        return len(self._motifs)\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            return self._motifs[item]\n\n        if isinstance(item, slice):\n            start, stop, step = item.indices(len(self))\n            if step != 1: raise NotImplementedError(\"Batch slicing with step != 1 not supported\")\n\n            # Zero-copy slicing of the batch\n            obj = object.__new__(MotifBatch)\n            obj._motifs = self._motifs[item]\n\n            val_start = self._offsets[start]\n            val_end = self._offsets[stop]\n            obj._offsets = self._offsets[start:stop + 1] - val_start\n\n            obj._pssm_combined = self._pssm_combined[:, val_start:val_end]\n            obj._pssm_rc_combined = self._pssm_rc_combined[\n                :, val_start:val_end] if self._pssm_rc_combined is not None else None\n\n            # Suffix offset = PSSM offset + Motif Index\n            s_start = val_start + start\n            s_end = val_end + stop\n            obj._max_suffixes = self._max_suffixes[s_start:s_end]\n            obj._max_suffixes_rc = self._max_suffixes_rc[s_start:s_end] if self._max_suffixes_rc is not None else None\n\n            return obj\n\n        if isinstance(item, (list, np.ndarray)):\n            return MotifBatch([self._motifs[i] for i in item])\n\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    def __iter__(self):\n        return iter(self._motifs)\n\n    @classmethod\n    def sigma70(cls, bg: Background = None) -&gt; 'MotifBatch':\n        \"\"\"Returns a batch containing Sigma70 -35 and -10 motifs.\"\"\"\n        dna = Alphabet.DNA\n        if bg is not None:\n            if bg.alphabet != dna: raise ValueError('Background alphabet must be dna for this motif')\n        else:\n            bg = Background.uniform(dna)\n        return MotifBatch([Motif.sigma70_35(bg), Motif.sigma70_10(bg)])\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.n_motifs","title":"<code>n_motifs</code>  <code>property</code>","text":"<p>Returns the number of motifs in the batch.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Constructs a MotifBatch from an iterable of Motifs.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable[object]) -&gt; 'Batch':\n    \"\"\"Constructs a MotifBatch from an iterable of Motifs.\"\"\"\n    return cls(components)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple MotifBatch objects.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['MotifBatch']) -&gt; 'MotifBatch':\n    \"\"\"Concatenates multiple MotifBatch objects.\"\"\"\n    # Simple list concatenation and rebuild\n    all_motifs = []\n    for b in batches:\n        all_motifs.extend(b._motifs)\n    return cls(all_motifs)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the batch.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def copy(self) -&gt; 'MotifBatch':\n    \"\"\"Returns a copy of the batch.\"\"\"\n    # Motifs are effectively immutable, but we can return a new wrapper\n    return self.__class__(list(self._motifs))\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty MotifBatch.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'MotifBatch':\n    \"\"\"Creates an empty MotifBatch.\"\"\"\n    return cls([])\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.scan","title":"<code>scan(seqs, pvalue_threshold=0.0001)</code>","text":"<p>Scans sequences for these motifs. Use <code>MotifScanner</code> for batch scanning.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Direct scanning not supported.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def scan(self, seqs: SeqBatch, pvalue_threshold: float = 1e-4) -&gt; 'MotifHitBatch':\n    \"\"\"Scans sequences for these motifs. Use ``MotifScanner`` for batch scanning.\n\n    Raises:\n        TypeError: Direct scanning not supported.\n    \"\"\"\n    raise TypeError(\"Use baclib.engines.motif.MotifScanner(batch).scan(seqs) instead.\")\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifBatch.sigma70","title":"<code>sigma70(bg=None)</code>  <code>classmethod</code>","text":"<p>Returns a batch containing Sigma70 -35 and -10 motifs.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef sigma70(cls, bg: Background = None) -&gt; 'MotifBatch':\n    \"\"\"Returns a batch containing Sigma70 -35 and -10 motifs.\"\"\"\n    dna = Alphabet.DNA\n    if bg is not None:\n        if bg.alphabet != dna: raise ValueError('Background alphabet must be dna for this motif')\n    else:\n        bg = Background.uniform(dna)\n    return MotifBatch([Motif.sigma70_35(bg), Motif.sigma70_10(bg)])\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHit","title":"<code>MotifHit</code>","text":"<p>               Bases: <code>Feature</code></p> <p>Represents a single occurrence of a Motif in a sequence.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Interval</code> <p>The location and strand of the hit.</p> required <code>score</code> <code>float</code> <p>The log-odds score.</p> required <code>motif</code> <code>Motif</code> <p>The matching <code>Motif</code> object.</p> required <code>pvalue</code> <code>float</code> <p>The P-value of the score (calculated lazily if <code>None</code>).</p> <code>None</code> <code>qualifiers</code> <code>Iterable[tuple[bytes, Any]]</code> <p>Optional qualifiers.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hit = MotifHit(Interval(10, 20), 12.5, motif)\n&gt;&gt;&gt; hit.pvalue\n1.2e-5\n</code></pre> Source code in <code>baclib/containers/motif.py</code> <pre><code>class MotifHit(Feature):\n    \"\"\"\n    Represents a single occurrence of a Motif in a sequence.\n\n    Args:\n        interval: The location and strand of the hit.\n        score: The log-odds score.\n        motif: The matching ``Motif`` object.\n        pvalue: The P-value of the score (calculated lazily if ``None``).\n        qualifiers: Optional qualifiers.\n\n    Examples:\n        &gt;&gt;&gt; hit = MotifHit(Interval(10, 20), 12.5, motif)\n        &gt;&gt;&gt; hit.pvalue\n        1.2e-5\n    \"\"\"\n    __slots__ = ('score', 'motif', '_pvalue')\n\n    def __init__(self, interval: Interval, score: float, motif: 'Motif',\n                 pvalue: float = None, qualifiers: Iterable[tuple[bytes, Any]] = None):\n        super().__init__(interval, key=FeatureKey.MOTIF, qualifiers=qualifiers)\n        self.score = score\n        self.motif = motif\n        self._pvalue = pvalue\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``MotifHitBatch`` class.\n        \"\"\"\n        return MotifHitBatch\n\n    @property\n    def name(self) -&gt; bytes:\n        \"\"\"Returns the motif name.\"\"\"\n        return self.motif.name\n\n    @property\n    def pvalue(self) -&gt; float:\n        \"\"\"Returns the P-value of the hit score.\"\"\"\n        if self._pvalue is None: self._pvalue = self.motif.get_pvalue(self.score)\n        return self._pvalue\n\n    def __getitem__(self, item):\n        if item == b'score': return self.score\n        if item == b'motif': return self.name\n        if item == b'pvalue': return self.pvalue\n        return super().__getitem__(item)\n\n    def __setitem__(self, key, value):\n        if key == b'score':\n            self.score = value\n        elif key == b'pvalue':\n            self._pvalue = value\n        elif key == b'motif':\n            raise AttributeError(\"Cannot set motif via dict access\")\n        else:\n            super().__setitem__(key, value)\n\n    def __repr__(self):\n        return f\"MotifHit({self.name.decode(Alphabet.ENCODING)}, score={self.score:.2f}, {self.interval})\"\n\n    def copy(self) -&gt; 'MotifHit':\n        \"\"\"Returns a deep copy of the hit.\"\"\"\n        return MotifHit(self.interval, self.score, self.motif, self._pvalue, list(self.qualifiers))\n\n    def shift(self, x: int, y: int = None) -&gt; 'MotifHit':\n        \"\"\"Shifts the hit coordinates.\"\"\"\n        return MotifHit(self.interval.shift(x, y), self.score, self.motif, self._pvalue, list(self.qualifiers))\n\n    def reverse_complement(self, parent_length: int) -&gt; 'MotifHit':\n        \"\"\"Reverse complements the hit.\"\"\"\n        return MotifHit(self.interval.reverse_complement(parent_length), self.score, self.motif, self._pvalue,\n                        list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHit.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>MotifHitBatch</code> class.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHit.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns the motif name.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHit.pvalue","title":"<code>pvalue</code>  <code>property</code>","text":"<p>Returns the P-value of the hit score.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHit.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of the hit.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def copy(self) -&gt; 'MotifHit':\n    \"\"\"Returns a deep copy of the hit.\"\"\"\n    return MotifHit(self.interval, self.score, self.motif, self._pvalue, list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHit.reverse_complement","title":"<code>reverse_complement(parent_length)</code>","text":"<p>Reverse complements the hit.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def reverse_complement(self, parent_length: int) -&gt; 'MotifHit':\n    \"\"\"Reverse complements the hit.\"\"\"\n    return MotifHit(self.interval.reverse_complement(parent_length), self.score, self.motif, self._pvalue,\n                    list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHit.shift","title":"<code>shift(x, y=None)</code>","text":"<p>Shifts the hit coordinates.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def shift(self, x: int, y: int = None) -&gt; 'MotifHit':\n    \"\"\"Shifts the hit coordinates.\"\"\"\n    return MotifHit(self.interval.shift(x, y), self.score, self.motif, self._pvalue, list(self.qualifiers))\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch","title":"<code>MotifHitBatch</code>","text":"<p>               Bases: <code>Batch</code></p> <p>Columnar container for motif scan results.</p> <p>Attributes:</p> Name Type Description <code>seq_indices</code> <code>ndarray</code> <p>Index of the sequence in the scanned batch.</p> <code>positions</code> <code>ndarray</code> <p>Start position of the hit.</p> <code>scores</code> <code>ndarray</code> <p>Log-odds score.</p> <code>strands</code> <code>ndarray</code> <p>Strand of the hit.</p> <code>motif_indices</code> <code>Optional[ndarray]</code> <p>Index of the motif in the scanner's batch (optional).</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>class MotifHitBatch(Batch):\n    \"\"\"\n    Columnar container for motif scan results.\n\n    Attributes:\n        seq_indices: Index of the sequence in the scanned batch.\n        positions: Start position of the hit.\n        scores: Log-odds score.\n        strands: Strand of the hit.\n        motif_indices: Index of the motif in the scanner's batch (optional).\n    \"\"\"\n    __slots__ = ('seq_indices', 'positions', 'scores', 'strands', 'motif_indices', '_source')\n\n    def __init__(self, seq_indices, positions, scores, strands, source, motif_indices=None):\n        self.seq_indices: np.ndarray = seq_indices\n        self.positions: np.ndarray = positions\n        self.scores: np.ndarray = scores\n        self.strands: np.ndarray = strands\n        self._source = source  # Motif or MotifBatch\n        self.motif_indices: Optional[np.ndarray] = motif_indices\n\n    @classmethod\n    def new_empty(cls, source):\n        \"\"\"Creates an empty batch for a given source.\"\"\"\n        return cls(np.array([], dtype=np.int32), np.array([], dtype=np.int32),\n                   np.array([], dtype=np.float32), np.array([], dtype=np.int8), source)\n\n    @classmethod\n    def build(cls, components: Iterable[object]) -&gt; 'Batch':\n        \"\"\"\n        Raises:\n            NotImplementedError: Hits should be created via scanning.\n        \"\"\"\n        raise NotImplementedError(\"MotifHitBatch should be created via Motif.scan()\")\n\n    @classmethod\n    def concat(cls, batches: Iterable['MotifHitBatch']) -&gt; 'MotifHitBatch':\n        \"\"\"Concatenates multiple hit batches.\"\"\"\n        batches = list(batches)\n        if not batches: raise ValueError(\"Cannot concat empty list\")\n        # Find first valid source\n        source = next((b._source for b in batches if b._source is not None), None)\n\n        seq_indices = np.concatenate([b.seq_indices for b in batches])\n        positions = np.concatenate([b.positions for b in batches])\n        scores = np.concatenate([b.scores for b in batches])\n        strands = np.concatenate([b.strands for b in batches])\n\n        # Handle motif indices if present\n        mi = None\n        # Check first non-empty batch for motif indices logic\n        first = batches[0]\n        if first.motif_indices is not None:\n             mi = np.concatenate([b.motif_indices for b in batches])\n\n        return cls(seq_indices, positions, scores, strands, source, mi)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns total memory usage in bytes.\"\"\"\n        base = self.seq_indices.nbytes + self.positions.nbytes + self.scores.nbytes + self.strands.nbytes\n        return base + (self.motif_indices.nbytes if self.motif_indices is not None else 0)\n\n    def copy(self) -&gt; 'MotifHitBatch':\n        \"\"\"Returns a copy of the batch.\"\"\"\n        return self.__class__(self.seq_indices.copy(), self.positions.copy(), self.scores.copy(), self.strands.copy(), self._source, self.motif_indices.copy() if self.motif_indices is not None else None)\n\n    @classmethod\n    def empty(cls) -&gt; 'MotifHitBatch':\n        \"\"\"Creates an empty hit batch.\"\"\"\n        return cls.new_empty(None)\n\n    @property\n    def component(self): return MotifHit\n\n    def __repr__(self):\n        return f\"&lt;MotifHitBatch: {len(self)} hits&gt;\"\n\n    def __len__(self):\n        return len(self.scores)\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            mi = self.motif_indices[item] if self.motif_indices is not None else None\n            return (self.seq_indices[item], self.positions[item], self.scores[item], self.strands[item], mi)\n\n        if isinstance(item, slice):\n            mi = self.motif_indices[item] if self.motif_indices is not None else None\n            return MotifHitBatch(\n                self.seq_indices[item], self.positions[item], self.scores[item], self.strands[item], self._source, mi\n            )\n\n        if isinstance(item, (np.ndarray, list)):\n            mi = self.motif_indices[item] if self.motif_indices is not None else None\n            return MotifHitBatch(\n                self.seq_indices[item], self.positions[item], self.scores[item], self.strands[item], self._source, mi\n            )\n\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    def to_features(self, seq_batch: SeqBatch) -&gt; list[list[MotifHit]]:\n        \"\"\"\n        Converts hits to Feature objects, grouped by sequence index.\n\n        Args:\n            seq_batch: The ``SeqBatch`` that was scanned (for context).\n\n        Returns:\n            A list of lists, where `list[i]` contains features for `seq_batch[i]`.\n        \"\"\"\n        n_seqs = len(seq_batch)\n        features_by_seq = [[] for _ in range(n_seqs)]\n\n        for i in range(len(self.scores)):\n            s_idx = self.seq_indices[i]\n            pos = self.positions[i]\n            score = self.scores[i]\n            strand = self.strands[i]\n\n            if self.motif_indices is not None:\n                motif = self._source[self.motif_indices[i]]\n            else:\n                motif = self._source\n\n            features_by_seq[s_idx].append(\n                MotifHit(Interval(pos, pos + len(motif), strand), score, motif)\n            )\n        return features_by_seq\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns total memory usage in bytes.</p>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Hits should be created via scanning.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable[object]) -&gt; 'Batch':\n    \"\"\"\n    Raises:\n        NotImplementedError: Hits should be created via scanning.\n    \"\"\"\n    raise NotImplementedError(\"MotifHitBatch should be created via Motif.scan()\")\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple hit batches.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['MotifHitBatch']) -&gt; 'MotifHitBatch':\n    \"\"\"Concatenates multiple hit batches.\"\"\"\n    batches = list(batches)\n    if not batches: raise ValueError(\"Cannot concat empty list\")\n    # Find first valid source\n    source = next((b._source for b in batches if b._source is not None), None)\n\n    seq_indices = np.concatenate([b.seq_indices for b in batches])\n    positions = np.concatenate([b.positions for b in batches])\n    scores = np.concatenate([b.scores for b in batches])\n    strands = np.concatenate([b.strands for b in batches])\n\n    # Handle motif indices if present\n    mi = None\n    # Check first non-empty batch for motif indices logic\n    first = batches[0]\n    if first.motif_indices is not None:\n         mi = np.concatenate([b.motif_indices for b in batches])\n\n    return cls(seq_indices, positions, scores, strands, source, mi)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a copy of the batch.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def copy(self) -&gt; 'MotifHitBatch':\n    \"\"\"Returns a copy of the batch.\"\"\"\n    return self.__class__(self.seq_indices.copy(), self.positions.copy(), self.scores.copy(), self.strands.copy(), self._source, self.motif_indices.copy() if self.motif_indices is not None else None)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty hit batch.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'MotifHitBatch':\n    \"\"\"Creates an empty hit batch.\"\"\"\n    return cls.new_empty(None)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch.new_empty","title":"<code>new_empty(source)</code>  <code>classmethod</code>","text":"<p>Creates an empty batch for a given source.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>@classmethod\ndef new_empty(cls, source):\n    \"\"\"Creates an empty batch for a given source.\"\"\"\n    return cls(np.array([], dtype=np.int32), np.array([], dtype=np.int32),\n               np.array([], dtype=np.float32), np.array([], dtype=np.int8), source)\n</code></pre>"},{"location":"reference/baclib/containers/motif/#baclib.containers.motif.MotifHitBatch.to_features","title":"<code>to_features(seq_batch)</code>","text":"<p>Converts hits to Feature objects, grouped by sequence index.</p> <p>Parameters:</p> Name Type Description Default <code>seq_batch</code> <code>SeqBatch</code> <p>The <code>SeqBatch</code> that was scanned (for context).</p> required <p>Returns:</p> Type Description <code>list[list[MotifHit]]</code> <p>A list of lists, where <code>list[i]</code> contains features for <code>seq_batch[i]</code>.</p> Source code in <code>baclib/containers/motif.py</code> <pre><code>def to_features(self, seq_batch: SeqBatch) -&gt; list[list[MotifHit]]:\n    \"\"\"\n    Converts hits to Feature objects, grouped by sequence index.\n\n    Args:\n        seq_batch: The ``SeqBatch`` that was scanned (for context).\n\n    Returns:\n        A list of lists, where `list[i]` contains features for `seq_batch[i]`.\n    \"\"\"\n    n_seqs = len(seq_batch)\n    features_by_seq = [[] for _ in range(n_seqs)]\n\n    for i in range(len(self.scores)):\n        s_idx = self.seq_indices[i]\n        pos = self.positions[i]\n        score = self.scores[i]\n        strand = self.strands[i]\n\n        if self.motif_indices is not None:\n            motif = self._source[self.motif_indices[i]]\n        else:\n            motif = self._source\n\n        features_by_seq[s_idx].append(\n            MotifHit(Interval(pos, pos + len(motif), strand), score, motif)\n        )\n    return features_by_seq\n</code></pre>"},{"location":"reference/baclib/containers/mutations/","title":"mutations","text":""},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations","title":"<code>baclib.containers.mutations</code>","text":"<p>Containers for representing and batch-processing genomic mutations (SNPs, indels).</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.Mutation","title":"<code>Mutation</code>","text":"<p>               Bases: <code>Feature</code></p> <p>A discrete sequence change: SNP, insertion, or deletion.</p> <p>Extends <code>Feature</code> with reference and alternative allele sequences, a predicted functional effect, and an optional amino acid change string.</p> <p>Parameters:</p> Name Type Description Default <code>interval</code> <code>Interval</code> <p>Location on the reference sequence.</p> required <code>ref_seq</code> <code>Seq</code> <p>The reference allele.</p> required <code>alt_seq</code> <code>Seq</code> <p>The alternative allele.</p> required <code>effect</code> <code>MutationEffect</code> <p>Predicted functional impact (default <code>UNKNOWN</code>).</p> <code>UNKNOWN</code> <code>aa_change</code> <code>bytes</code> <p>Optional amino acid change annotation (e.g. <code>b'S123A'</code>).</p> <code>None</code> <code>qualifiers</code> <code>Iterable[tuple[bytes, Any]]</code> <p>Optional <code>(key, value)</code> qualifier tuples.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mut = Mutation(Interval(100, 101, 1), ref, alt)\n&gt;&gt;&gt; mut.is_snp\nTrue\n</code></pre> Source code in <code>baclib/containers/mutations.py</code> <pre><code>class Mutation(Feature):\n    \"\"\"\n    A discrete sequence change: SNP, insertion, or deletion.\n\n    Extends ``Feature`` with reference and alternative allele sequences,\n    a predicted functional effect, and an optional amino acid change string.\n\n    Args:\n        interval: Location on the reference sequence.\n        ref_seq: The reference allele.\n        alt_seq: The alternative allele.\n        effect: Predicted functional impact (default ``UNKNOWN``).\n        aa_change: Optional amino acid change annotation (e.g. ``b'S123A'``).\n        qualifiers: Optional ``(key, value)`` qualifier tuples.\n\n    Examples:\n        &gt;&gt;&gt; mut = Mutation(Interval(100, 101, 1), ref, alt)\n        &gt;&gt;&gt; mut.is_snp\n        True\n    \"\"\"\n    __slots__ = ('ref_seq', 'alt_seq', 'effect', 'aa_change')\n\n    def __init__(self, interval: Interval, ref_seq: Seq, alt_seq: Seq,\n                 effect: MutationEffect = MutationEffect.UNKNOWN,\n                 aa_change: bytes = None,\n                 qualifiers: Iterable[tuple[bytes, Any]] = None):\n        super().__init__(interval, key=FeatureKey.MISC_DIFFERENCE, qualifiers=qualifiers)\n        self.ref_seq = ref_seq\n        self.alt_seq = alt_seq\n        self.effect = effect\n        self.aa_change = aa_change\n\n    @property\n    def is_snp(self) -&gt; bool:\n        \"\"\"Returns ``True`` if the mutation is a single-nucleotide polymorphism.\n\n        Returns:\n            ``True`` when both ref and alt are exactly 1 base long.\n        \"\"\"\n        return len(self.ref_seq) == 1 and len(self.alt_seq) == 1\n\n    @property\n    def is_indel(self) -&gt; bool:\n        \"\"\"Returns ``True`` if the mutation is an insertion or deletion.\n\n        Returns:\n            ``True`` when ref and alt differ in length.\n        \"\"\"\n        return len(self.ref_seq) != len(self.alt_seq)\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``MutationBatch`` class.\n        \"\"\"\n        return MutationBatch\n\n    @property\n    def diff(self) -&gt; int:\n        \"\"\"Returns the net change in sequence length (alt \u2212 ref).\n\n        Returns:\n            Positive for insertions, negative for deletions, zero for SNPs.\n        \"\"\"\n        return len(self.alt_seq) - len(self.ref_seq)\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.Mutation.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>MutationBatch</code> class.</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.Mutation.diff","title":"<code>diff</code>  <code>property</code>","text":"<p>Returns the net change in sequence length (alt \u2212 ref).</p> <p>Returns:</p> Type Description <code>int</code> <p>Positive for insertions, negative for deletions, zero for SNPs.</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.Mutation.is_indel","title":"<code>is_indel</code>  <code>property</code>","text":"<p>Returns <code>True</code> if the mutation is an insertion or deletion.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> when ref and alt differ in length.</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.Mutation.is_snp","title":"<code>is_snp</code>  <code>property</code>","text":"<p>Returns <code>True</code> if the mutation is a single-nucleotide polymorphism.</p> <p>Returns:</p> Type Description <code>bool</code> <p><code>True</code> when both ref and alt are exactly 1 base long.</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch","title":"<code>MutationBatch</code>","text":"<p>               Bases: <code>Batch</code>, <code>HasIntervals</code></p> <p>Columnar batch of mutations for efficient bulk operations.</p> <p>Stores intervals, ref/alt sequences, effects, and qualifiers in parallel numpy arrays for vectorized filtering and application.</p> <p>Parameters:</p> Name Type Description Default <code>intervals</code> <code>IntervalBatch</code> <p>An <code>IntervalBatch</code> of mutation positions.</p> required <code>ref_seqs</code> <code>SeqBatch</code> <p>A <code>SeqBatch</code> of reference alleles.</p> required <code>alt_seqs</code> <code>SeqBatch</code> <p>A <code>SeqBatch</code> of alternative alleles.</p> required <code>effects</code> <code>ndarray</code> <p>Optional <code>uint8</code> array of <code>MutationEffect</code> values.</p> <code>None</code> <code>aa_changes</code> <code>ndarray</code> <p>Optional object array of amino acid change annotations.</p> <code>None</code> <code>qualifiers</code> <code>QualifierBatch</code> <p>Optional <code>QualifierBatch</code> of per-mutation qualifiers.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = MutationBatch.build([mut1, mut2])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/mutations.py</code> <pre><code>class MutationBatch(Batch, HasIntervals):\n    \"\"\"\n    Columnar batch of mutations for efficient bulk operations.\n\n    Stores intervals, ref/alt sequences, effects, and qualifiers in\n    parallel numpy arrays for vectorized filtering and application.\n\n    Args:\n        intervals: An ``IntervalBatch`` of mutation positions.\n        ref_seqs: A ``SeqBatch`` of reference alleles.\n        alt_seqs: A ``SeqBatch`` of alternative alleles.\n        effects: Optional ``uint8`` array of ``MutationEffect`` values.\n        aa_changes: Optional object array of amino acid change annotations.\n        qualifiers: Optional ``QualifierBatch`` of per-mutation qualifiers.\n\n    Examples:\n        &gt;&gt;&gt; batch = MutationBatch.build([mut1, mut2])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    __slots__ = ('_intervals', '_ref_seqs', '_alt_seqs', '_effects', '_aa_changes', '_qualifiers')\n\n    def __init__(self, intervals: IntervalBatch, ref_seqs: SeqBatch, alt_seqs: SeqBatch,\n                 effects: np.ndarray = None, aa_changes: np.ndarray = None, qualifiers: QualifierBatch = None):\n        self._intervals = intervals\n        self._ref_seqs = ref_seqs\n        self._alt_seqs = alt_seqs\n        n = len(intervals)\n        self._effects = effects if effects is not None else np.zeros(n, dtype=np.uint8)\n        self._aa_changes = aa_changes if aa_changes is not None else np.full(n, None, dtype=object)\n        self._qualifiers = qualifiers if qualifiers is not None else QualifierBatch.zeros(n)\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``Mutation`` class.\n        \"\"\"\n        return Mutation\n\n    @classmethod\n    def empty(cls) -&gt; 'MutationBatch':\n        \"\"\"Creates an empty MutationBatch with zero mutations.\n\n        Returns:\n            An empty ``MutationBatch``.\n        \"\"\"\n        return cls(\n            IntervalBatch.empty(),\n            Alphabet.DNA.empty_batch(),\n            Alphabet.DNA.empty_batch()\n        )\n\n    @classmethod\n    def build(cls, components: Iterable[Mutation]) -&gt; 'MutationBatch':\n        \"\"\"Constructs a MutationBatch from an iterable of Mutation objects.\n\n        Args:\n            components: An iterable of ``Mutation`` objects.\n\n        Returns:\n            A new ``MutationBatch``.\n\n        Examples:\n            &gt;&gt;&gt; batch = MutationBatch.build([mut1, mut2])\n            &gt;&gt;&gt; len(batch)\n            2\n        \"\"\"\n        mutations = list(components)\n        if not mutations: return cls.empty()\n\n        # Manual extraction to ensure sort=False (preserve order)\n        n = len(mutations)\n        starts = np.empty(n, dtype=np.int32)\n        ends = np.empty(n, dtype=np.int32)\n        strands = np.empty(n, dtype=np.int32)\n\n        for i, m in enumerate(mutations):\n            iv = m.interval\n            starts[i] = iv.start\n            ends[i] = iv.end\n            strands[i] = iv.strand\n\n        intervals = IntervalBatch(starts, ends, strands, sort=False)\n\n        ref_seqs = SeqBatch.build([m.ref_seq for m in mutations])\n        alt_seqs = SeqBatch.build([m.alt_seq for m in mutations])\n\n        effects = np.array([m.effect for m in mutations], dtype=np.uint8)\n        aa_changes = np.array([m.aa_change for m in mutations], dtype=object)\n\n        qualifiers = QualifierBatch.build(m.qualifiers for m in mutations)\n\n        return cls(intervals, ref_seqs, alt_seqs, effects, aa_changes, qualifiers)\n\n    @classmethod\n    def concat(cls, batches: Iterable['MutationBatch']) -&gt; 'MutationBatch':\n        \"\"\"Concatenates multiple MutationBatch objects into one.\n\n        Args:\n            batches: An iterable of ``MutationBatch`` objects.\n\n        Returns:\n            A single concatenated ``MutationBatch``.\n\n        Examples:\n            &gt;&gt;&gt; combined = MutationBatch.concat([batch_a, batch_b])\n        \"\"\"\n        batches = list(batches)\n        if not batches: return cls.empty()\n\n        # Manual interval concat to maintain sync with other arrays (IntervalBatch.concat sorts)\n        i_batches = [b.intervals for b in batches]\n        starts = np.concatenate([b.starts for b in i_batches])\n        ends = np.concatenate([b.ends for b in i_batches])\n        strands = np.concatenate([b.strands for b in i_batches])\n        intervals = IntervalBatch(starts, ends, strands, sort=False)\n\n        ref_seqs = SeqBatch.concat([b._ref_seqs for b in batches])\n        alt_seqs = SeqBatch.concat([b._alt_seqs for b in batches])\n        effects = np.concatenate([b._effects for b in batches])\n        aa_changes = np.concatenate([b._aa_changes for b in batches])\n        qualifiers = QualifierBatch.concat([b._qualifiers for b in batches])\n\n        return cls(intervals, ref_seqs, alt_seqs, effects, aa_changes, qualifiers)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by all internal arrays.\n        \"\"\"\n        return (self._intervals.nbytes + self._ref_seqs.nbytes + \n                self._alt_seqs.nbytes + self._effects.nbytes + \n                self._aa_changes.nbytes + self._qualifiers.nbytes)\n\n    def copy(self) -&gt; 'MutationBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``MutationBatch`` with copied arrays.\n        \"\"\"\n        return MutationBatch(\n            self._intervals.copy(),\n            self._ref_seqs.copy(),\n            self._alt_seqs.copy(),\n            self._alt_seqs.copy(),\n            self._effects.copy(),\n            self._aa_changes.copy(),\n            self._qualifiers.copy()\n        )\n\n    def __repr__(self):\n        return f\"&lt;MutationBatch: {len(self)} mutations&gt;\"\n\n    def __len__(self):\n        return len(self._intervals)\n\n    @property\n    def intervals(self) -&gt; IntervalBatch:\n        \"\"\"Returns the interval array for all mutations.\n\n        Returns:\n            An ``IntervalBatch`` of mutation positions.\n        \"\"\"\n        return self._intervals\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield self[i]\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            interval = self._intervals[item]\n            ref = self._ref_seqs[item]\n            alt = self._alt_seqs[item]\n            eff = MutationEffect(self._effects[item])\n            aa = self._aa_changes[item]\n            quals = self._qualifiers[item]\n            return Mutation(interval, ref, alt, eff, aa, quals)\n\n        if isinstance(item, slice):\n            return MutationBatch(\n                self._intervals[item],\n                self._ref_seqs[item],\n                self._alt_seqs[item],\n                self._effects[item],\n                self._aa_changes[item],\n                self._qualifiers[item]\n            )\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'MutationBatch':\n        \"\"\"Creates a MutationBatch with *n* zero-length placeholder mutations.\n\n        Args:\n            n: Number of placeholder slots.\n\n        Returns:\n            A ``MutationBatch`` with empty sequences and default effects.\n\n        Examples:\n            &gt;&gt;&gt; batch = MutationBatch.zeros(5)\n            &gt;&gt;&gt; len(batch)\n            5\n        \"\"\"\n        return cls(\n            IntervalBatch.zeros(n),\n            Alphabet.DNA.zeros_batch(n),\n            Alphabet.DNA.zeros_batch(n),\n            effects=None,\n            aa_changes=None,\n            qualifiers=QualifierBatch.zeros(n)\n        )\n\n    def apply_to(self, reference: Seq) -&gt; Any:\n        \"\"\"Applies the mutations to a reference sequence.\n\n        Args:\n            reference: The reference ``Seq`` to mutate.\n\n        Returns:\n            A ``SparseSeq`` representing the mutated sequence.\n\n        Raises:\n            NotImplementedError: SparseSeq is not yet implemented.\n        \"\"\"\n        raise NotImplementedError(\"SparseSeq is not yet implemented.\")\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>Mutation</code> class.</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.intervals","title":"<code>intervals</code>  <code>property</code>","text":"<p>Returns the interval array for all mutations.</p> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An <code>IntervalBatch</code> of mutation positions.</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by all internal arrays.</p>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.apply_to","title":"<code>apply_to(reference)</code>","text":"<p>Applies the mutations to a reference sequence.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>Seq</code> <p>The reference <code>Seq</code> to mutate.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>A <code>SparseSeq</code> representing the mutated sequence.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>SparseSeq is not yet implemented.</p> Source code in <code>baclib/containers/mutations.py</code> <pre><code>def apply_to(self, reference: Seq) -&gt; Any:\n    \"\"\"Applies the mutations to a reference sequence.\n\n    Args:\n        reference: The reference ``Seq`` to mutate.\n\n    Returns:\n        A ``SparseSeq`` representing the mutated sequence.\n\n    Raises:\n        NotImplementedError: SparseSeq is not yet implemented.\n    \"\"\"\n    raise NotImplementedError(\"SparseSeq is not yet implemented.\")\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Constructs a MutationBatch from an iterable of Mutation objects.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>Iterable[Mutation]</code> <p>An iterable of <code>Mutation</code> objects.</p> required <p>Returns:</p> Type Description <code>MutationBatch</code> <p>A new <code>MutationBatch</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = MutationBatch.build([mut1, mut2])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/mutations.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable[Mutation]) -&gt; 'MutationBatch':\n    \"\"\"Constructs a MutationBatch from an iterable of Mutation objects.\n\n    Args:\n        components: An iterable of ``Mutation`` objects.\n\n    Returns:\n        A new ``MutationBatch``.\n\n    Examples:\n        &gt;&gt;&gt; batch = MutationBatch.build([mut1, mut2])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    mutations = list(components)\n    if not mutations: return cls.empty()\n\n    # Manual extraction to ensure sort=False (preserve order)\n    n = len(mutations)\n    starts = np.empty(n, dtype=np.int32)\n    ends = np.empty(n, dtype=np.int32)\n    strands = np.empty(n, dtype=np.int32)\n\n    for i, m in enumerate(mutations):\n        iv = m.interval\n        starts[i] = iv.start\n        ends[i] = iv.end\n        strands[i] = iv.strand\n\n    intervals = IntervalBatch(starts, ends, strands, sort=False)\n\n    ref_seqs = SeqBatch.build([m.ref_seq for m in mutations])\n    alt_seqs = SeqBatch.build([m.alt_seq for m in mutations])\n\n    effects = np.array([m.effect for m in mutations], dtype=np.uint8)\n    aa_changes = np.array([m.aa_change for m in mutations], dtype=object)\n\n    qualifiers = QualifierBatch.build(m.qualifiers for m in mutations)\n\n    return cls(intervals, ref_seqs, alt_seqs, effects, aa_changes, qualifiers)\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple MutationBatch objects into one.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[MutationBatch]</code> <p>An iterable of <code>MutationBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>MutationBatch</code> <p>A single concatenated <code>MutationBatch</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combined = MutationBatch.concat([batch_a, batch_b])\n</code></pre> Source code in <code>baclib/containers/mutations.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['MutationBatch']) -&gt; 'MutationBatch':\n    \"\"\"Concatenates multiple MutationBatch objects into one.\n\n    Args:\n        batches: An iterable of ``MutationBatch`` objects.\n\n    Returns:\n        A single concatenated ``MutationBatch``.\n\n    Examples:\n        &gt;&gt;&gt; combined = MutationBatch.concat([batch_a, batch_b])\n    \"\"\"\n    batches = list(batches)\n    if not batches: return cls.empty()\n\n    # Manual interval concat to maintain sync with other arrays (IntervalBatch.concat sorts)\n    i_batches = [b.intervals for b in batches]\n    starts = np.concatenate([b.starts for b in i_batches])\n    ends = np.concatenate([b.ends for b in i_batches])\n    strands = np.concatenate([b.strands for b in i_batches])\n    intervals = IntervalBatch(starts, ends, strands, sort=False)\n\n    ref_seqs = SeqBatch.concat([b._ref_seqs for b in batches])\n    alt_seqs = SeqBatch.concat([b._alt_seqs for b in batches])\n    effects = np.concatenate([b._effects for b in batches])\n    aa_changes = np.concatenate([b._aa_changes for b in batches])\n    qualifiers = QualifierBatch.concat([b._qualifiers for b in batches])\n\n    return cls(intervals, ref_seqs, alt_seqs, effects, aa_changes, qualifiers)\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>MutationBatch</code> <p>A new <code>MutationBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/mutations.py</code> <pre><code>def copy(self) -&gt; 'MutationBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``MutationBatch`` with copied arrays.\n    \"\"\"\n    return MutationBatch(\n        self._intervals.copy(),\n        self._ref_seqs.copy(),\n        self._alt_seqs.copy(),\n        self._alt_seqs.copy(),\n        self._effects.copy(),\n        self._aa_changes.copy(),\n        self._qualifiers.copy()\n    )\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty MutationBatch with zero mutations.</p> <p>Returns:</p> Type Description <code>MutationBatch</code> <p>An empty <code>MutationBatch</code>.</p> Source code in <code>baclib/containers/mutations.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'MutationBatch':\n    \"\"\"Creates an empty MutationBatch with zero mutations.\n\n    Returns:\n        An empty ``MutationBatch``.\n    \"\"\"\n    return cls(\n        IntervalBatch.empty(),\n        Alphabet.DNA.empty_batch(),\n        Alphabet.DNA.empty_batch()\n    )\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a MutationBatch with n zero-length placeholder mutations.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of placeholder slots.</p> required <p>Returns:</p> Type Description <code>MutationBatch</code> <p>A <code>MutationBatch</code> with empty sequences and default effects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = MutationBatch.zeros(5)\n&gt;&gt;&gt; len(batch)\n5\n</code></pre> Source code in <code>baclib/containers/mutations.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'MutationBatch':\n    \"\"\"Creates a MutationBatch with *n* zero-length placeholder mutations.\n\n    Args:\n        n: Number of placeholder slots.\n\n    Returns:\n        A ``MutationBatch`` with empty sequences and default effects.\n\n    Examples:\n        &gt;&gt;&gt; batch = MutationBatch.zeros(5)\n        &gt;&gt;&gt; len(batch)\n        5\n    \"\"\"\n    return cls(\n        IntervalBatch.zeros(n),\n        Alphabet.DNA.zeros_batch(n),\n        Alphabet.DNA.zeros_batch(n),\n        effects=None,\n        aa_changes=None,\n        qualifiers=QualifierBatch.zeros(n)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/mutations/#baclib.containers.mutations.MutationEffect","title":"<code>MutationEffect</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Predicted functional impact of a mutation on the gene product.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; MutationEffect.MISSENSE\n&lt;MutationEffect.MISSENSE: 3&gt;\n</code></pre> Source code in <code>baclib/containers/mutations.py</code> <pre><code>class MutationEffect(IntEnum):\n    \"\"\"\n    Predicted functional impact of a mutation on the gene product.\n\n    Examples:\n        &gt;&gt;&gt; MutationEffect.MISSENSE\n        &lt;MutationEffect.MISSENSE: 3&gt;\n    \"\"\"\n    UNKNOWN = auto()\n    SYNONYMOUS = auto()\n    MISSENSE = auto()\n    NONSENSE = auto()\n    FRAMESHIFT = auto()\n    INTERGENIC = auto()\n    NON_CODING = auto()\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/","title":"qualifier","text":""},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier","title":"<code>baclib.containers.qualifier</code>","text":"<p>List and batch containers for key-value qualifier annotations on genomic features.</p>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch","title":"<code>QualifierBatch</code>","text":"<p>               Bases: <code>RaggedBatch</code></p> <p>A batch of qualifier lists stored in columnar format (Structure-of-Arrays).</p> <p>Internally uses a shared vocabulary for keys (<code>_key_vocab</code>), integer key IDs (<code>_key_ids</code>), and an object array of values (<code>_values</code>), with ragged offsets to delineate each qualifier list.</p> <p>Parameters:</p> Name Type Description Default <code>key_vocab</code> <p>Array of unique qualifier key bytes.</p> required <code>key_ids</code> <p>Integer array mapping each qualifier entry to its vocab index.</p> required <code>values</code> <p>Object array of qualifier values.</p> required <code>offsets</code> <p>Integer array of cumulative offsets (length = n_items + 1).</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = QualifierBatch.build([\n...     [(b'gene', b'dnaA')],\n...     [(b'gene', b'dnaN'), (b'product', b'polymerase')],\n... ])\n&gt;&gt;&gt; len(batch)\n2\n&gt;&gt;&gt; batch[0]\n[(b'gene', b'dnaA')]\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>class QualifierBatch(RaggedBatch):\n    \"\"\"\n    A batch of qualifier lists stored in columnar format (Structure-of-Arrays).\n\n    Internally uses a shared vocabulary for keys (``_key_vocab``), integer key IDs\n    (``_key_ids``), and an object array of values (``_values``), with ragged offsets\n    to delineate each qualifier list.\n\n    Args:\n        key_vocab: Array of unique qualifier key bytes.\n        key_ids: Integer array mapping each qualifier entry to its vocab index.\n        values: Object array of qualifier values.\n        offsets: Integer array of cumulative offsets (length = n_items + 1).\n\n    Examples:\n        &gt;&gt;&gt; batch = QualifierBatch.build([\n        ...     [(b'gene', b'dnaA')],\n        ...     [(b'gene', b'dnaN'), (b'product', b'polymerase')],\n        ... ])\n        &gt;&gt;&gt; len(batch)\n        2\n        &gt;&gt;&gt; batch[0]\n        [(b'gene', b'dnaA')]\n    \"\"\"\n    __slots__ = ('_key_vocab', '_key_ids', '_values')\n\n    def __init__(self, key_vocab, key_ids, values, offsets):\n        super().__init__(offsets)\n        self._key_vocab = key_vocab\n        self._key_ids = key_ids\n        self._values = values\n\n    @classmethod\n    def build(cls, qualifiers_list: Iterable[Iterable[tuple[bytes, QualifierType]]]) -&gt; 'QualifierBatch':\n        \"\"\"Constructs a QualifierBatch from an iterable of qualifier iterables.\n\n        Args:\n            qualifiers_list: An iterable where each element is an iterable of\n                ``(key, value)`` tuples representing one qualifier list.\n\n        Returns:\n            A new QualifierBatch.\n\n        Examples:\n            &gt;&gt;&gt; batch = QualifierBatch.build([\n            ...     [(b'gene', b'dnaA')],\n            ...     [(b'gene', b'dnaN')],\n            ... ])\n            &gt;&gt;&gt; len(batch)\n            2\n        \"\"\"\n        key_to_id = {}\n        key_vocab = []\n        flat_key_ids = []\n        flat_values = []\n\n        offsets = [0]\n        curr_idx = 0\n\n        for quals in qualifiers_list:\n            for k, v in quals:\n                if k not in key_to_id:\n                    key_to_id[k] = len(key_vocab)\n                    key_vocab.append(k)\n                flat_key_ids.append(key_to_id[k])\n                flat_values.append(v)\n                curr_idx += 1\n            offsets.append(curr_idx)\n\n        return cls(\n            np.array(key_vocab),\n            np.array(flat_key_ids, dtype=np.int32),\n            np.array(flat_values, dtype=object),\n            np.array(offsets, dtype=np.int32)\n        )\n\n    @classmethod\n    def concat(cls, batches: Iterable['QualifierBatch']) -&gt; 'QualifierBatch':\n        \"\"\"Concatenates multiple QualifierBatch objects into one.\n\n        Rebuilds the shared vocabulary to handle merging across batches.\n\n        Args:\n            batches: An iterable of QualifierBatch objects.\n\n        Returns:\n            A single concatenated QualifierBatch.\n\n        Examples:\n            &gt;&gt;&gt; a = QualifierBatch.build([[(b'gene', b'dnaA')]])\n            &gt;&gt;&gt; b = QualifierBatch.build([[(b'gene', b'dnaN')]])\n            &gt;&gt;&gt; combined = QualifierBatch.concat([a, b])\n            &gt;&gt;&gt; len(combined)\n            2\n        \"\"\"\n        def iterator():\n            for b in batches:\n                for item in b:\n                    yield item\n        return cls.build(iterator())\n\n    @classmethod\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by vocabulary, key IDs, values, and offsets.\n        \"\"\"\n        return super().nbytes + self._key_vocab.nbytes + self._key_ids.nbytes + self._values.nbytes\n\n    def copy(self) -&gt; 'QualifierBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new QualifierBatch with copied arrays.\n        \"\"\"\n        return self.__class__(self._key_vocab.copy(), self._key_ids.copy(), self._values.copy(), self._offsets.copy())\n\n    @classmethod\n    def empty(cls) -&gt; 'QualifierBatch':\n        \"\"\"Creates an empty QualifierBatch with zero items.\n\n        Returns:\n            An empty QualifierBatch.\n\n        Examples:\n            &gt;&gt;&gt; batch = QualifierBatch.empty()\n            &gt;&gt;&gt; len(batch)\n            0\n        \"\"\"\n        return cls(\n            np.array([], dtype='S1'),\n            np.array([], dtype=np.int32),\n            np.array([], dtype=object),\n            np.zeros(1, dtype=np.int32)\n        )\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'QualifierBatch':\n        \"\"\"Creates a QualifierBatch with *n* empty qualifier lists.\n\n        Args:\n            n: Number of empty qualifier slots.\n\n        Returns:\n            A QualifierBatch where each of the *n* items has zero qualifiers.\n\n        Examples:\n            &gt;&gt;&gt; batch = QualifierBatch.zeros(3)\n            &gt;&gt;&gt; len(batch)\n            3\n            &gt;&gt;&gt; batch[0]\n            []\n        \"\"\"\n        return cls(\n            np.array([], dtype='S1'),\n            np.array([], dtype=np.int32),\n            np.array([], dtype=object),\n            np.zeros(n + 1, dtype=np.int32)\n        )\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``tuple`` type.\n        \"\"\"\n        return tuple\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            start = self._offsets[item]\n            end = self._offsets[item + 1]\n            if start == end: return []\n            keys = self._key_vocab[self._key_ids[start:end]]\n            values = self._values[start:end]\n            return list(zip(keys, values))\n\n        if isinstance(item, slice):\n            new_offsets, val_start, val_end = self._get_slice_info(item)\n            new_key_ids = self._key_ids[val_start:val_end]\n            new_values = self._values[val_start:val_end]\n\n            # Create new batch (Zero Copy views where possible)\n            obj = object.__new__(QualifierBatch)\n            obj._key_vocab = self._key_vocab\n            obj._key_ids = new_key_ids\n            obj._values = new_values\n            obj._offsets = new_offsets\n            return obj\n\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>tuple</code> type.</p>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch.build","title":"<code>build(qualifiers_list)</code>  <code>classmethod</code>","text":"<p>Constructs a QualifierBatch from an iterable of qualifier iterables.</p> <p>Parameters:</p> Name Type Description Default <code>qualifiers_list</code> <code>Iterable[Iterable[tuple[bytes, QualifierType]]]</code> <p>An iterable where each element is an iterable of <code>(key, value)</code> tuples representing one qualifier list.</p> required <p>Returns:</p> Type Description <code>QualifierBatch</code> <p>A new QualifierBatch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = QualifierBatch.build([\n...     [(b'gene', b'dnaA')],\n...     [(b'gene', b'dnaN')],\n... ])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>@classmethod\ndef build(cls, qualifiers_list: Iterable[Iterable[tuple[bytes, QualifierType]]]) -&gt; 'QualifierBatch':\n    \"\"\"Constructs a QualifierBatch from an iterable of qualifier iterables.\n\n    Args:\n        qualifiers_list: An iterable where each element is an iterable of\n            ``(key, value)`` tuples representing one qualifier list.\n\n    Returns:\n        A new QualifierBatch.\n\n    Examples:\n        &gt;&gt;&gt; batch = QualifierBatch.build([\n        ...     [(b'gene', b'dnaA')],\n        ...     [(b'gene', b'dnaN')],\n        ... ])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    key_to_id = {}\n    key_vocab = []\n    flat_key_ids = []\n    flat_values = []\n\n    offsets = [0]\n    curr_idx = 0\n\n    for quals in qualifiers_list:\n        for k, v in quals:\n            if k not in key_to_id:\n                key_to_id[k] = len(key_vocab)\n                key_vocab.append(k)\n            flat_key_ids.append(key_to_id[k])\n            flat_values.append(v)\n            curr_idx += 1\n        offsets.append(curr_idx)\n\n    return cls(\n        np.array(key_vocab),\n        np.array(flat_key_ids, dtype=np.int32),\n        np.array(flat_values, dtype=object),\n        np.array(offsets, dtype=np.int32)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple QualifierBatch objects into one.</p> <p>Rebuilds the shared vocabulary to handle merging across batches.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[QualifierBatch]</code> <p>An iterable of QualifierBatch objects.</p> required <p>Returns:</p> Type Description <code>QualifierBatch</code> <p>A single concatenated QualifierBatch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; a = QualifierBatch.build([[(b'gene', b'dnaA')]])\n&gt;&gt;&gt; b = QualifierBatch.build([[(b'gene', b'dnaN')]])\n&gt;&gt;&gt; combined = QualifierBatch.concat([a, b])\n&gt;&gt;&gt; len(combined)\n2\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['QualifierBatch']) -&gt; 'QualifierBatch':\n    \"\"\"Concatenates multiple QualifierBatch objects into one.\n\n    Rebuilds the shared vocabulary to handle merging across batches.\n\n    Args:\n        batches: An iterable of QualifierBatch objects.\n\n    Returns:\n        A single concatenated QualifierBatch.\n\n    Examples:\n        &gt;&gt;&gt; a = QualifierBatch.build([[(b'gene', b'dnaA')]])\n        &gt;&gt;&gt; b = QualifierBatch.build([[(b'gene', b'dnaN')]])\n        &gt;&gt;&gt; combined = QualifierBatch.concat([a, b])\n        &gt;&gt;&gt; len(combined)\n        2\n    \"\"\"\n    def iterator():\n        for b in batches:\n            for item in b:\n                yield item\n    return cls.build(iterator())\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>QualifierBatch</code> <p>A new QualifierBatch with copied arrays.</p> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>def copy(self) -&gt; 'QualifierBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new QualifierBatch with copied arrays.\n    \"\"\"\n    return self.__class__(self._key_vocab.copy(), self._key_ids.copy(), self._values.copy(), self._offsets.copy())\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty QualifierBatch with zero items.</p> <p>Returns:</p> Type Description <code>QualifierBatch</code> <p>An empty QualifierBatch.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = QualifierBatch.empty()\n&gt;&gt;&gt; len(batch)\n0\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'QualifierBatch':\n    \"\"\"Creates an empty QualifierBatch with zero items.\n\n    Returns:\n        An empty QualifierBatch.\n\n    Examples:\n        &gt;&gt;&gt; batch = QualifierBatch.empty()\n        &gt;&gt;&gt; len(batch)\n        0\n    \"\"\"\n    return cls(\n        np.array([], dtype='S1'),\n        np.array([], dtype=np.int32),\n        np.array([], dtype=object),\n        np.zeros(1, dtype=np.int32)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch.nbytes","title":"<code>nbytes()</code>  <code>classmethod</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by vocabulary, key IDs, values, and offsets.</p> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>@classmethod\ndef nbytes(self) -&gt; int:\n    \"\"\"Returns the total memory usage in bytes.\n\n    Returns:\n        Total bytes consumed by vocabulary, key IDs, values, and offsets.\n    \"\"\"\n    return super().nbytes + self._key_vocab.nbytes + self._key_ids.nbytes + self._values.nbytes\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a QualifierBatch with n empty qualifier lists.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of empty qualifier slots.</p> required <p>Returns:</p> Type Description <code>QualifierBatch</code> <p>A QualifierBatch where each of the n items has zero qualifiers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = QualifierBatch.zeros(3)\n&gt;&gt;&gt; len(batch)\n3\n&gt;&gt;&gt; batch[0]\n[]\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'QualifierBatch':\n    \"\"\"Creates a QualifierBatch with *n* empty qualifier lists.\n\n    Args:\n        n: Number of empty qualifier slots.\n\n    Returns:\n        A QualifierBatch where each of the *n* items has zero qualifiers.\n\n    Examples:\n        &gt;&gt;&gt; batch = QualifierBatch.zeros(3)\n        &gt;&gt;&gt; len(batch)\n        3\n        &gt;&gt;&gt; batch[0]\n        []\n    \"\"\"\n    return cls(\n        np.array([], dtype='S1'),\n        np.array([], dtype=np.int32),\n        np.array([], dtype=object),\n        np.zeros(n + 1, dtype=np.int32)\n    )\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierList","title":"<code>QualifierList</code>","text":"<p>               Bases: <code>MutableSequence</code></p> <p>A list-like container for (key, value) tuples that also supports dictionary-style access.</p> <p>Maintains insertion order and allows duplicate keys. Used to store feature and record qualifiers from formats like GenBank and GFF.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>Union[Iterable[tuple[bytes, QualifierType]], dict]</code> <p>Initial qualifier data as an iterable of <code>(key, value)</code> tuples or a dictionary.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA'), (b'product', b'replication initiator')])\n&gt;&gt;&gt; quals[b'gene']\nb'dnaA'\n&gt;&gt;&gt; quals.add(b'note', b'essential')\n&gt;&gt;&gt; len(quals)\n3\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>class QualifierList(MutableSequence):\n    \"\"\"\n    A list-like container for (key, value) tuples that also supports dictionary-style access.\n\n    Maintains insertion order and allows duplicate keys. Used to store\n    feature and record qualifiers from formats like GenBank and GFF.\n\n    Args:\n        items: Initial qualifier data as an iterable of ``(key, value)`` tuples\n            or a dictionary.\n\n    Examples:\n        &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA'), (b'product', b'replication initiator')])\n        &gt;&gt;&gt; quals[b'gene']\n        b'dnaA'\n        &gt;&gt;&gt; quals.add(b'note', b'essential')\n        &gt;&gt;&gt; len(quals)\n        3\n    \"\"\"\n    __slots__ = ('_data',)\n\n    def __init__(self, items: Union[Iterable[tuple[bytes, QualifierType]], dict] = None):\n        if isinstance(items, dict):\n            self._data = list(items.items())\n        else:\n            self._data = list(items) if items else []\n\n    def __getitem__(self, item):\n        # List-style access\n        if isinstance(item, (int, slice)): return self._data[item]\n        # Dict-style access (First match)\n        for k, v in self._data:\n            if k == item: return v\n        return None\n\n    def __setitem__(self, key, value):\n        if isinstance(key, (int, slice)):\n            self._data[key] = value\n            return\n\n        # Dict-style set: Replace first occurrence or append\n        for i, (k, v) in enumerate(self._data):\n            if k == key:\n                self._data[i] = (key, value)\n                return\n        self._data.append((key, value))\n\n    def __delitem__(self, index): del self._data[index]\n    def __len__(self): return len(self._data)\n    def __iter__(self): return iter(self._data)\n    def __repr__(self):\n        if len(self) &gt; 6:\n            return f\"[{', '.join(repr(x) for x in self[:3])}, ..., {', '.join(repr(x) for x in self[-3:])}]\"\n        return repr(self._data)\n\n    def insert(self, index, value):\n        \"\"\"Inserts a ``(key, value)`` tuple at the given index.\n\n        Args:\n            index: Position to insert at.\n            value: A ``(key, value)`` tuple to insert.\n\n        Examples:\n            &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n            &gt;&gt;&gt; quals.insert(0, (b'locus_tag', b'b0001'))\n            &gt;&gt;&gt; quals[0]\n            (b'locus_tag', b'b0001')\n        \"\"\"\n        self._data.insert(index, value)\n\n    def __eq__(self, other):\n        if isinstance(other, QualifierList): return self._data == other._data\n        if isinstance(other, list): return self._data == other\n        return False\n\n    def get(self, key: bytes, default=None) -&gt; QualifierType:\n        \"\"\"Returns the first value for a key, or *default* if not found.\n\n        Args:\n            key: The qualifier key to look up.\n            default: Value to return if key is absent.\n\n        Returns:\n            The first matching value, or *default*.\n\n        Examples:\n            &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n            &gt;&gt;&gt; quals.get(b'gene')\n            b'dnaA'\n            &gt;&gt;&gt; quals.get(b'missing', b'N/A')\n            b'N/A'\n        \"\"\"\n        for k, v in self._data:\n            if k == key: return v\n        return default\n\n    def get_all(self, key: bytes) -&gt; list[QualifierType]:\n        \"\"\"Returns all values for a key.\n\n        Args:\n            key: The qualifier key to look up.\n\n        Returns:\n            A list of all matching values (empty if key is absent).\n\n        Examples:\n            &gt;&gt;&gt; quals = QualifierList([(b'db_xref', b'GI:123'), (b'db_xref', b'UniProt:P0A')])\n            &gt;&gt;&gt; quals.get_all(b'db_xref')\n            [b'GI:123', b'UniProt:P0A']\n        \"\"\"\n        return [v for k, v in self._data if k == key]\n\n    def add(self, key: bytes, value: QualifierType):\n        \"\"\"Appends a new key-value pair.\n\n        Args:\n            key: The qualifier key.\n            value: The qualifier value.\n\n        Examples:\n            &gt;&gt;&gt; quals = QualifierList()\n            &gt;&gt;&gt; quals.add(b'gene', b'dnaA')\n            &gt;&gt;&gt; len(quals)\n            1\n        \"\"\"\n        self._data.append((key, value))\n\n    def to_dict(self) -&gt; dict[bytes, QualifierType]:\n        \"\"\"Converts to a standard dictionary (lossy for duplicate keys).\n\n        Returns:\n            A dict mapping each key to its *last* value.\n\n        Examples:\n            &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA'), (b'product', b'initiator')])\n            &gt;&gt;&gt; quals.to_dict()\n            {b'gene': b'dnaA', b'product': b'initiator'}\n        \"\"\"\n        return {k: v for k, v in self._data}\n\n    def items(self):\n        \"\"\"Returns an iterator over ``(key, value)`` tuples.\n\n        Returns:\n            An iterator of ``(key, value)`` tuples.\n\n        Examples:\n            &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n            &gt;&gt;&gt; list(quals.items())\n            [(b'gene', b'dnaA')]\n        \"\"\"\n        return iter(self._data)\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierList.add","title":"<code>add(key, value)</code>","text":"<p>Appends a new key-value pair.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>bytes</code> <p>The qualifier key.</p> required <code>value</code> <code>QualifierType</code> <p>The qualifier value.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; quals = QualifierList()\n&gt;&gt;&gt; quals.add(b'gene', b'dnaA')\n&gt;&gt;&gt; len(quals)\n1\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>def add(self, key: bytes, value: QualifierType):\n    \"\"\"Appends a new key-value pair.\n\n    Args:\n        key: The qualifier key.\n        value: The qualifier value.\n\n    Examples:\n        &gt;&gt;&gt; quals = QualifierList()\n        &gt;&gt;&gt; quals.add(b'gene', b'dnaA')\n        &gt;&gt;&gt; len(quals)\n        1\n    \"\"\"\n    self._data.append((key, value))\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierList.get","title":"<code>get(key, default=None)</code>","text":"<p>Returns the first value for a key, or default if not found.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>bytes</code> <p>The qualifier key to look up.</p> required <code>default</code> <p>Value to return if key is absent.</p> <code>None</code> <p>Returns:</p> Type Description <code>QualifierType</code> <p>The first matching value, or default.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n&gt;&gt;&gt; quals.get(b'gene')\nb'dnaA'\n&gt;&gt;&gt; quals.get(b'missing', b'N/A')\nb'N/A'\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>def get(self, key: bytes, default=None) -&gt; QualifierType:\n    \"\"\"Returns the first value for a key, or *default* if not found.\n\n    Args:\n        key: The qualifier key to look up.\n        default: Value to return if key is absent.\n\n    Returns:\n        The first matching value, or *default*.\n\n    Examples:\n        &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n        &gt;&gt;&gt; quals.get(b'gene')\n        b'dnaA'\n        &gt;&gt;&gt; quals.get(b'missing', b'N/A')\n        b'N/A'\n    \"\"\"\n    for k, v in self._data:\n        if k == key: return v\n    return default\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierList.get_all","title":"<code>get_all(key)</code>","text":"<p>Returns all values for a key.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>bytes</code> <p>The qualifier key to look up.</p> required <p>Returns:</p> Type Description <code>list[QualifierType]</code> <p>A list of all matching values (empty if key is absent).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quals = QualifierList([(b'db_xref', b'GI:123'), (b'db_xref', b'UniProt:P0A')])\n&gt;&gt;&gt; quals.get_all(b'db_xref')\n[b'GI:123', b'UniProt:P0A']\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>def get_all(self, key: bytes) -&gt; list[QualifierType]:\n    \"\"\"Returns all values for a key.\n\n    Args:\n        key: The qualifier key to look up.\n\n    Returns:\n        A list of all matching values (empty if key is absent).\n\n    Examples:\n        &gt;&gt;&gt; quals = QualifierList([(b'db_xref', b'GI:123'), (b'db_xref', b'UniProt:P0A')])\n        &gt;&gt;&gt; quals.get_all(b'db_xref')\n        [b'GI:123', b'UniProt:P0A']\n    \"\"\"\n    return [v for k, v in self._data if k == key]\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierList.insert","title":"<code>insert(index, value)</code>","text":"<p>Inserts a <code>(key, value)</code> tuple at the given index.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <p>Position to insert at.</p> required <code>value</code> <p>A <code>(key, value)</code> tuple to insert.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n&gt;&gt;&gt; quals.insert(0, (b'locus_tag', b'b0001'))\n&gt;&gt;&gt; quals[0]\n(b'locus_tag', b'b0001')\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>def insert(self, index, value):\n    \"\"\"Inserts a ``(key, value)`` tuple at the given index.\n\n    Args:\n        index: Position to insert at.\n        value: A ``(key, value)`` tuple to insert.\n\n    Examples:\n        &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n        &gt;&gt;&gt; quals.insert(0, (b'locus_tag', b'b0001'))\n        &gt;&gt;&gt; quals[0]\n        (b'locus_tag', b'b0001')\n    \"\"\"\n    self._data.insert(index, value)\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierList.items","title":"<code>items()</code>","text":"<p>Returns an iterator over <code>(key, value)</code> tuples.</p> <p>Returns:</p> Type Description <p>An iterator of <code>(key, value)</code> tuples.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n&gt;&gt;&gt; list(quals.items())\n[(b'gene', b'dnaA')]\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>def items(self):\n    \"\"\"Returns an iterator over ``(key, value)`` tuples.\n\n    Returns:\n        An iterator of ``(key, value)`` tuples.\n\n    Examples:\n        &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA')])\n        &gt;&gt;&gt; list(quals.items())\n        [(b'gene', b'dnaA')]\n    \"\"\"\n    return iter(self._data)\n</code></pre>"},{"location":"reference/baclib/containers/qualifier/#baclib.containers.qualifier.QualifierList.to_dict","title":"<code>to_dict()</code>","text":"<p>Converts to a standard dictionary (lossy for duplicate keys).</p> <p>Returns:</p> Type Description <code>dict[bytes, QualifierType]</code> <p>A dict mapping each key to its last value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA'), (b'product', b'initiator')])\n&gt;&gt;&gt; quals.to_dict()\n{b'gene': b'dnaA', b'product': b'initiator'}\n</code></pre> Source code in <code>baclib/containers/qualifier.py</code> <pre><code>def to_dict(self) -&gt; dict[bytes, QualifierType]:\n    \"\"\"Converts to a standard dictionary (lossy for duplicate keys).\n\n    Returns:\n        A dict mapping each key to its *last* value.\n\n    Examples:\n        &gt;&gt;&gt; quals = QualifierList([(b'gene', b'dnaA'), (b'product', b'initiator')])\n        &gt;&gt;&gt; quals.to_dict()\n        {b'gene': b'dnaA', b'product': b'initiator'}\n    \"\"\"\n    return {k: v for k, v in self._data}\n</code></pre>"},{"location":"reference/baclib/containers/record/","title":"record","text":""},{"location":"reference/baclib/containers/record/#baclib.containers.record","title":"<code>baclib.containers.record</code>","text":"<p>Container for biological sequence records with metadata, features, and batch support.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record","title":"<code>Record</code>","text":"<p>               Bases: <code>HasIntervals</code>, <code>Batchable</code></p> <p>A biological sequence record with ID, description, qualifiers, and features.</p> <p>Typically represents a contig, chromosome, or plasmid \u2014 a named sequence annotated with genomic features (CDS, tRNA, etc.) and key-value qualifiers.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Seq</code> <p>The underlying <code>Seq</code> object.</p> required <code>id_</code> <code>bytes</code> <p>Record identifier as bytes (auto-generated UUID if omitted).</p> <code>None</code> <code>desc</code> <code>bytes</code> <p>Optional description line.</p> <code>None</code> <code>qualifiers</code> <code>Iterable[tuple]</code> <p>Optional iterable of <code>(key, value)</code> qualifier tuples.</p> <code>None</code> <code>features</code> <code>list[Feature]</code> <p>Optional list of <code>Feature</code> objects.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rec = Record(Alphabet.DNA.seq(b'ATGCGA'), id_=b'contig_1')\n&gt;&gt;&gt; len(rec)\n6\n&gt;&gt;&gt; rec.id\nb'contig_1'\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>class Record(HasIntervals, Batchable):\n    \"\"\"\n    A biological sequence record with ID, description, qualifiers, and features.\n\n    Typically represents a contig, chromosome, or plasmid \u2014 a named sequence\n    annotated with genomic features (CDS, tRNA, etc.) and key-value qualifiers.\n\n    Args:\n        seq: The underlying ``Seq`` object.\n        id_: Record identifier as bytes (auto-generated UUID if omitted).\n        desc: Optional description line.\n        qualifiers: Optional iterable of ``(key, value)`` qualifier tuples.\n        features: Optional list of ``Feature`` objects.\n\n    Examples:\n        &gt;&gt;&gt; rec = Record(Alphabet.DNA.seq(b'ATGCGA'), id_=b'contig_1')\n        &gt;&gt;&gt; len(rec)\n        6\n        &gt;&gt;&gt; rec.id\n        b'contig_1'\n    \"\"\"\n    __slots__ = ('_seq', 'id', 'description', '_qualifiers', '_features')\n    def __init__(self, seq: Seq, id_: bytes = None, desc: bytes = None, qualifiers: Iterable[tuple] = None,\n                 features: list['Feature'] = None):\n        self._seq: Seq = seq\n        self.id: bytes = id_ or hexlify(uuid4().bytes)\n        self.description: bytes = desc or b''\n        self.qualifiers = qualifiers\n        # FeatureList is a list subclass that likely handles parent linkage\n        self._features = FeatureList(features)\n    def __str__(self): return self.id.decode(errors='ignore')\n    def __repr__(self) -&gt; str: return f'{self.id} {self._seq.__repr__()}'\n    def __len__(self) -&gt; int: return len(self._seq)\n    def __hash__(self) -&gt; int: return hash(self.id)\n    def __iter__(self) -&gt; Iterator['Feature']: return iter(self.features)\n    def __eq__(self, other) -&gt; bool: return self.id == other.id if isinstance(other, Record) else False\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``RecordBatch`` class.\n        \"\"\"\n        return RecordBatch\n\n    @property\n    def seq(self) -&gt; Seq:\n        \"\"\"Returns the underlying sequence.\n\n        Returns:\n            The ``Seq`` object.\n        \"\"\"\n        return self._seq\n\n    @property\n    def features(self) -&gt; 'FeatureList':\n        \"\"\"Returns the list of features annotated on this record.\n\n        Returns:\n            A ``FeatureList``.\n        \"\"\"\n        return self._features\n\n    @features.setter\n    def features(self, value): self._features = FeatureList(value)\n\n    @property\n    def qualifiers(self) -&gt; QualifierList:\n        \"\"\"Returns the record-level qualifiers.\n\n        Returns:\n            A ``QualifierList`` of ``(key, value)`` tuples.\n        \"\"\"\n        return self._qualifiers\n\n    @qualifiers.setter\n    def qualifiers(self, value): self._qualifiers = QualifierList(value)\n\n    @property\n    def intervals(self) -&gt; 'IntervalBatch':\n        \"\"\"Returns the intervals of all features on this record.\n\n        Returns:\n            An ``IntervalBatch``.\n        \"\"\"\n        return self.features.intervals\n\n    def add_features(self, *features: 'Feature'):\n        \"\"\"Adds features to this record, re-sorting by start position.\n\n        Args:\n            features: One or more ``Feature`` objects to add.\n        \"\"\"\n        self.features.extend(features)\n        self.features.sort(key=attrgetter('interval.start'))\n\n    def extract_feature(self, feature: Feature) -&gt; Seq:\n        \"\"\"Extracts the subsequence corresponding to a feature.\n\n        Uses the feature's interval (including strand) to slice the record's\n        sequence, applying reverse complement if on the minus strand.\n\n        Args:\n            feature: The ``Feature`` whose sequence to extract.\n\n        Returns:\n            A ``Seq`` of the feature's sequence.\n\n        Examples:\n            &gt;&gt;&gt; cds = rec.features[0]\n            &gt;&gt;&gt; rec.extract_feature(cds)\n            ATGCGA\n        \"\"\"\n        return feature.extract(self._seq)\n\n    def reverse_complement(self) -&gt; 'Record':\n        \"\"\"Returns the reverse complement of this record and all its features.\n\n        Feature coordinates and strands are flipped relative to the new\n        sequence orientation.\n\n        Returns:\n            A new ``Record`` with the reverse-complemented sequence and\n            transformed feature coordinates.\n        \"\"\"\n        new_seq = self._seq.alphabet.reverse_complement(self._seq)\n        parent_len = len(self)\n        # Transform features: coordinates flip, strand flips\n        new_feats = [f.reverse_complement(parent_len) for f in self.features]\n        new_feats.sort(key=attrgetter('interval.start'))\n        return Record(new_seq, id_=self.id, desc=self.description,\n                      qualifiers=list(self.qualifiers), features=new_feats)\n\n    # --- Slicing &amp; Access ---\n    def __getitem__(self, item) -&gt; 'Record':\n        \"\"\"Slices the record, truncating overlapping features to fit the new bounds.\n\n        Args:\n            item: A slice, integer, or ``Interval``.\n\n        Returns:\n            A new ``Record`` with the sliced sequence and adjusted features.\n        \"\"\"\n        item = Interval.from_item(item, length=len(self._seq))\n        new_record = Record(self._seq[item])\n\n        # Overlapping features are already sorted by start due to IntervalBatch properties\n        overlapping = self.features.get_overlapping(item.start, item.end)\n\n        new_features = []\n        offset = item.start\n        limit = len(new_record)\n\n        for feature in overlapping:\n            # Direct calculation of new coordinates\n            s = max(0, feature.interval.start - offset)\n            e = min(limit, feature.interval.end - offset)\n\n            if s &lt; e:\n                new_f = Feature(Interval(s, e, feature.interval.strand), feature.key, feature.qualifiers)\n                new_features.append(new_f)\n\n        new_record.features.extend(new_features)\n        return new_record\n\n    # --- Modification ---\n    def __add__(self, other: 'Record') -&gt; 'Record':\n        if not isinstance(other, Record): raise TypeError(other)\n        # 1. Merge Sequence\n        new_seq = self._seq + other.seq\n\n        # 2. Handle Features\n        feats_self = self.features\n        feats_other = other.features\n        features = [f.copy() for f in feats_self]\n        offset = len(self)\n\n        # Merge Boundary Features Logic\n        # (Only if adjacent and same kind)\n        if (feats_self and feats_other and\n                feats_self[-1].interval.end == offset and\n                feats_other[0].interval.start == 0 and\n                feats_self[-1].key == feats_other[0].key):\n\n            last = features.pop()  # Remove last from self\n            first = feats_other[0]  # Get first from other\n\n            # Extend the last feature\n            # FIX: Interval is immutable, must create new instance\n            new_end = last.interval.end + (first.interval.end - first.interval.start)\n            last.interval = Interval(last.interval.start, new_end, last.interval.strand)\n            # Qualifiers merge logic? (For now, kept self's qualifiers)\n            features.append(last)\n\n            # Add remaining other features\n            for f in feats_other[1:]:\n                features.append(f.shift(offset))\n        else:\n            # Standard append\n            for f in feats_other:\n                features.append(f.shift(offset))\n\n        return Record(new_seq, features=features)\n\n    def __delitem__(self, key: Union[slice, int]):\n        \"\"\"Deletes a slice from the record, truncating overlapping features.\n\n        Args:\n            key: A slice specifying the region to delete.\n\n        Raises:\n            TypeError: If *key* is not a slice.\n        \"\"\"\n        if not isinstance(key, slice):\n            raise TypeError(\"Deletion supported for slices only.\")\n\n        start, stop, step = key.indices(len(self))\n        if step != 1: raise ValueError(\"Deletion step not supported.\")\n\n        slice_len = stop - start\n        if slice_len &lt;= 0: return\n\n        new_features = []\n\n        for feature in self.features:\n            f_start, f_end = feature.interval.start, feature.interval.end\n\n            # Case 1: Feature is entirely BEFORE the cut (Keep)\n            if f_end &lt;= start:\n                new_features.append(feature)\n                continue\n\n            # Case 2: Feature is entirely AFTER the cut (Shift Left)\n            if f_start &gt;= stop:\n                new_features.append(feature.shift(-slice_len))\n                continue\n\n            # Case 3: Feature is INSIDE or OVERLAPPING the cut (Modify)\n            ov_start = max(start, f_start)\n            ov_end = min(stop, f_end)\n            overlap_len = max(0, ov_end - ov_start)\n\n            if overlap_len == len(feature):\n                # Entirely deleted\n                continue\n\n            new_f = feature.copy()\n\n            if f_start &lt; start:\n                new_f.interval = Interval(new_f.interval.start, new_f.interval.end - overlap_len, new_f.interval.strand)\n            else:\n                new_start = start\n                new_end = start + (len(feature) - overlap_len)\n                new_f.interval = Interval(new_start, new_end, new_f.interval.strand)\n\n            new_features.append(new_f)\n\n        self.features = new_features\n        self._seq = self._seq[:start] + self._seq[stop:]  # This delegates to Seq slicing\n\n    # --- Utilities ---\n    def insert(self, other: 'Record', at: int, replace: bool = False) -&gt; 'Record':\n        \"\"\"Inserts another record's content at a given position.\n\n        Args:\n            other: The ``Record`` to insert.\n            at: The insertion position (0-indexed).\n            replace: If ``True``, overwrite ``len(other)`` bases starting at *at*.\n\n        Returns:\n            A new ``Record`` with the insertion applied.\n\n        Raises:\n            IndexError: If *at* is out of range.\n\n        Examples:\n            &gt;&gt;&gt; result = rec.insert(insertion, at=100)\n        \"\"\"\n        if not 0 &lt;= at &lt;= len(self): raise IndexError(f\"Insert index {at} out of range.\")\n        suffix_start = at + (len(other) if replace else 0)\n        return self[:at] + other + self[suffix_start:]\n\n    def shred(self, rng: np.random.Generator = None, n_breaks: int = None, break_points: list[int] = None\n              ) -&gt; Generator['Record', None, None]:\n        \"\"\"Randomly shreds the record into fragments at break points.\n\n        Args:\n            rng: NumPy random generator (uses global default if omitted).\n            n_breaks: Number of random break points.\n            break_points: Explicit list of break positions (overrides *n_breaks*).\n\n        Yields:\n            ``Record`` fragments.\n\n        Examples:\n            &gt;&gt;&gt; fragments = list(rec.shred(n_breaks=3))\n            &gt;&gt;&gt; len(fragments)\n            4\n        \"\"\"\n        if rng is None: rng = RESOURCES.rng\n        if not n_breaks and not break_points:\n            n_breaks = rng.integers(1, max(2, len(self) // 1000))  # improved default\n\n        if break_points is None:\n            # Use np.sort on the array directly for efficiency\n            break_points = np.sort(rng.choice(len(self), size=n_breaks, replace=False))\n        else:\n            break_points = np.sort(break_points)\n\n        previous_end = 0\n        for break_point in break_points:\n            # Yield slice\n            yield self[previous_end:break_point]\n            previous_end = break_point\n        yield self[previous_end:]\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>RecordBatch</code> class.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.features","title":"<code>features</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the list of features annotated on this record.</p> <p>Returns:</p> Type Description <code>FeatureList</code> <p>A <code>FeatureList</code>.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.intervals","title":"<code>intervals</code>  <code>property</code>","text":"<p>Returns the intervals of all features on this record.</p> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An <code>IntervalBatch</code>.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.qualifiers","title":"<code>qualifiers</code>  <code>property</code> <code>writable</code>","text":"<p>Returns the record-level qualifiers.</p> <p>Returns:</p> Type Description <code>QualifierList</code> <p>A <code>QualifierList</code> of <code>(key, value)</code> tuples.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.seq","title":"<code>seq</code>  <code>property</code>","text":"<p>Returns the underlying sequence.</p> <p>Returns:</p> Type Description <code>Seq</code> <p>The <code>Seq</code> object.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.__delitem__","title":"<code>__delitem__(key)</code>","text":"<p>Deletes a slice from the record, truncating overlapping features.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>Union[slice, int]</code> <p>A slice specifying the region to delete.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If key is not a slice.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>def __delitem__(self, key: Union[slice, int]):\n    \"\"\"Deletes a slice from the record, truncating overlapping features.\n\n    Args:\n        key: A slice specifying the region to delete.\n\n    Raises:\n        TypeError: If *key* is not a slice.\n    \"\"\"\n    if not isinstance(key, slice):\n        raise TypeError(\"Deletion supported for slices only.\")\n\n    start, stop, step = key.indices(len(self))\n    if step != 1: raise ValueError(\"Deletion step not supported.\")\n\n    slice_len = stop - start\n    if slice_len &lt;= 0: return\n\n    new_features = []\n\n    for feature in self.features:\n        f_start, f_end = feature.interval.start, feature.interval.end\n\n        # Case 1: Feature is entirely BEFORE the cut (Keep)\n        if f_end &lt;= start:\n            new_features.append(feature)\n            continue\n\n        # Case 2: Feature is entirely AFTER the cut (Shift Left)\n        if f_start &gt;= stop:\n            new_features.append(feature.shift(-slice_len))\n            continue\n\n        # Case 3: Feature is INSIDE or OVERLAPPING the cut (Modify)\n        ov_start = max(start, f_start)\n        ov_end = min(stop, f_end)\n        overlap_len = max(0, ov_end - ov_start)\n\n        if overlap_len == len(feature):\n            # Entirely deleted\n            continue\n\n        new_f = feature.copy()\n\n        if f_start &lt; start:\n            new_f.interval = Interval(new_f.interval.start, new_f.interval.end - overlap_len, new_f.interval.strand)\n        else:\n            new_start = start\n            new_end = start + (len(feature) - overlap_len)\n            new_f.interval = Interval(new_start, new_end, new_f.interval.strand)\n\n        new_features.append(new_f)\n\n    self.features = new_features\n    self._seq = self._seq[:start] + self._seq[stop:]  # This delegates to Seq slicing\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Slices the record, truncating overlapping features to fit the new bounds.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <p>A slice, integer, or <code>Interval</code>.</p> required <p>Returns:</p> Type Description <code>Record</code> <p>A new <code>Record</code> with the sliced sequence and adjusted features.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>def __getitem__(self, item) -&gt; 'Record':\n    \"\"\"Slices the record, truncating overlapping features to fit the new bounds.\n\n    Args:\n        item: A slice, integer, or ``Interval``.\n\n    Returns:\n        A new ``Record`` with the sliced sequence and adjusted features.\n    \"\"\"\n    item = Interval.from_item(item, length=len(self._seq))\n    new_record = Record(self._seq[item])\n\n    # Overlapping features are already sorted by start due to IntervalBatch properties\n    overlapping = self.features.get_overlapping(item.start, item.end)\n\n    new_features = []\n    offset = item.start\n    limit = len(new_record)\n\n    for feature in overlapping:\n        # Direct calculation of new coordinates\n        s = max(0, feature.interval.start - offset)\n        e = min(limit, feature.interval.end - offset)\n\n        if s &lt; e:\n            new_f = Feature(Interval(s, e, feature.interval.strand), feature.key, feature.qualifiers)\n            new_features.append(new_f)\n\n    new_record.features.extend(new_features)\n    return new_record\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.add_features","title":"<code>add_features(*features)</code>","text":"<p>Adds features to this record, re-sorting by start position.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>Feature</code> <p>One or more <code>Feature</code> objects to add.</p> <code>()</code> Source code in <code>baclib/containers/record.py</code> <pre><code>def add_features(self, *features: 'Feature'):\n    \"\"\"Adds features to this record, re-sorting by start position.\n\n    Args:\n        features: One or more ``Feature`` objects to add.\n    \"\"\"\n    self.features.extend(features)\n    self.features.sort(key=attrgetter('interval.start'))\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.extract_feature","title":"<code>extract_feature(feature)</code>","text":"<p>Extracts the subsequence corresponding to a feature.</p> <p>Uses the feature's interval (including strand) to slice the record's sequence, applying reverse complement if on the minus strand.</p> <p>Parameters:</p> Name Type Description Default <code>feature</code> <code>Feature</code> <p>The <code>Feature</code> whose sequence to extract.</p> required <p>Returns:</p> Type Description <code>Seq</code> <p>A <code>Seq</code> of the feature's sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cds = rec.features[0]\n&gt;&gt;&gt; rec.extract_feature(cds)\nATGCGA\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>def extract_feature(self, feature: Feature) -&gt; Seq:\n    \"\"\"Extracts the subsequence corresponding to a feature.\n\n    Uses the feature's interval (including strand) to slice the record's\n    sequence, applying reverse complement if on the minus strand.\n\n    Args:\n        feature: The ``Feature`` whose sequence to extract.\n\n    Returns:\n        A ``Seq`` of the feature's sequence.\n\n    Examples:\n        &gt;&gt;&gt; cds = rec.features[0]\n        &gt;&gt;&gt; rec.extract_feature(cds)\n        ATGCGA\n    \"\"\"\n    return feature.extract(self._seq)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.insert","title":"<code>insert(other, at, replace=False)</code>","text":"<p>Inserts another record's content at a given position.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Record</code> <p>The <code>Record</code> to insert.</p> required <code>at</code> <code>int</code> <p>The insertion position (0-indexed).</p> required <code>replace</code> <code>bool</code> <p>If <code>True</code>, overwrite <code>len(other)</code> bases starting at at.</p> <code>False</code> <p>Returns:</p> Type Description <code>Record</code> <p>A new <code>Record</code> with the insertion applied.</p> <p>Raises:</p> Type Description <code>IndexError</code> <p>If at is out of range.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; result = rec.insert(insertion, at=100)\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>def insert(self, other: 'Record', at: int, replace: bool = False) -&gt; 'Record':\n    \"\"\"Inserts another record's content at a given position.\n\n    Args:\n        other: The ``Record`` to insert.\n        at: The insertion position (0-indexed).\n        replace: If ``True``, overwrite ``len(other)`` bases starting at *at*.\n\n    Returns:\n        A new ``Record`` with the insertion applied.\n\n    Raises:\n        IndexError: If *at* is out of range.\n\n    Examples:\n        &gt;&gt;&gt; result = rec.insert(insertion, at=100)\n    \"\"\"\n    if not 0 &lt;= at &lt;= len(self): raise IndexError(f\"Insert index {at} out of range.\")\n    suffix_start = at + (len(other) if replace else 0)\n    return self[:at] + other + self[suffix_start:]\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.reverse_complement","title":"<code>reverse_complement()</code>","text":"<p>Returns the reverse complement of this record and all its features.</p> <p>Feature coordinates and strands are flipped relative to the new sequence orientation.</p> <p>Returns:</p> Type Description <code>Record</code> <p>A new <code>Record</code> with the reverse-complemented sequence and</p> <code>Record</code> <p>transformed feature coordinates.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>def reverse_complement(self) -&gt; 'Record':\n    \"\"\"Returns the reverse complement of this record and all its features.\n\n    Feature coordinates and strands are flipped relative to the new\n    sequence orientation.\n\n    Returns:\n        A new ``Record`` with the reverse-complemented sequence and\n        transformed feature coordinates.\n    \"\"\"\n    new_seq = self._seq.alphabet.reverse_complement(self._seq)\n    parent_len = len(self)\n    # Transform features: coordinates flip, strand flips\n    new_feats = [f.reverse_complement(parent_len) for f in self.features]\n    new_feats.sort(key=attrgetter('interval.start'))\n    return Record(new_seq, id_=self.id, desc=self.description,\n                  qualifiers=list(self.qualifiers), features=new_feats)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.Record.shred","title":"<code>shred(rng=None, n_breaks=None, break_points=None)</code>","text":"<p>Randomly shreds the record into fragments at break points.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>NumPy random generator (uses global default if omitted).</p> <code>None</code> <code>n_breaks</code> <code>int</code> <p>Number of random break points.</p> <code>None</code> <code>break_points</code> <code>list[int]</code> <p>Explicit list of break positions (overrides n_breaks).</p> <code>None</code> <p>Yields:</p> Type Description <code>Record</code> <p><code>Record</code> fragments.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; fragments = list(rec.shred(n_breaks=3))\n&gt;&gt;&gt; len(fragments)\n4\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>def shred(self, rng: np.random.Generator = None, n_breaks: int = None, break_points: list[int] = None\n          ) -&gt; Generator['Record', None, None]:\n    \"\"\"Randomly shreds the record into fragments at break points.\n\n    Args:\n        rng: NumPy random generator (uses global default if omitted).\n        n_breaks: Number of random break points.\n        break_points: Explicit list of break positions (overrides *n_breaks*).\n\n    Yields:\n        ``Record`` fragments.\n\n    Examples:\n        &gt;&gt;&gt; fragments = list(rec.shred(n_breaks=3))\n        &gt;&gt;&gt; len(fragments)\n        4\n    \"\"\"\n    if rng is None: rng = RESOURCES.rng\n    if not n_breaks and not break_points:\n        n_breaks = rng.integers(1, max(2, len(self) // 1000))  # improved default\n\n    if break_points is None:\n        # Use np.sort on the array directly for efficiency\n        break_points = np.sort(rng.choice(len(self), size=n_breaks, replace=False))\n    else:\n        break_points = np.sort(break_points)\n\n    previous_end = 0\n    for break_point in break_points:\n        # Yield slice\n        yield self[previous_end:break_point]\n        previous_end = break_point\n    yield self[previous_end:]\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch","title":"<code>RecordBatch</code>","text":"<p>               Bases: <code>Batch</code>, <code>HasIntervals</code></p> <p>High-performance, read-only columnar container for a collection of Records.</p> <p>Uses Structure-of-Arrays (SoA) layout: sequences, IDs, qualifiers, and features are stored in contiguous arrays for vectorized operations and Numba compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>records</code> <code>Iterable[Record]</code> <p>An iterable of <code>Record</code> objects to batch.</p> required <code>deduplicate</code> <code>bool</code> <p>If <code>True</code>, deduplicates identical sequences.</p> <code>False</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = RecordBatch([rec1, rec2, rec3])\n&gt;&gt;&gt; len(batch)\n3\n&gt;&gt;&gt; batch[0]\nRecord(...)\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>class RecordBatch(Batch, HasIntervals):\n    \"\"\"\n    High-performance, read-only columnar container for a collection of Records.\n\n    Uses Structure-of-Arrays (SoA) layout: sequences, IDs, qualifiers, and features\n    are stored in contiguous arrays for vectorized operations and Numba compatibility.\n\n    Args:\n        records: An iterable of ``Record`` objects to batch.\n        deduplicate: If ``True``, deduplicates identical sequences.\n\n    Examples:\n        &gt;&gt;&gt; batch = RecordBatch([rec1, rec2, rec3])\n        &gt;&gt;&gt; len(batch)\n        3\n        &gt;&gt;&gt; batch[0]\n        Record(...)\n    \"\"\"\n    __slots__ = ('_seqs', '_ids', '_features', '_feature_rec_indices', '_feature_offsets', '_qualifiers')\n\n\n    def __init__(self, records: Iterable[Record], deduplicate: bool = False):\n        # 1. Materialize list to avoid multiple passes if iterator\n        recs = list(records)\n        # 2. Batch Sequences\n        self._seqs = recs[0].seq.alphabet.batch_from((r.seq for r in recs), deduplicate=deduplicate)\n        # 3. Batch IDs (Fixed length numpy array for speed, or object array)\n        # We use object array of bytes to handle variable length IDs safely\n        self._ids = np.array([r.id for r in recs])\n        # 4. Batch Qualifiers\n        self._qualifiers = QualifierBatch.build(r.qualifiers for r in recs)\n        self._build_features(recs)\n\n    @classmethod\n    def random(cls, alphabet: Alphabet, rng: np.random.Generator = None, n_seqs: int = None, min_n: int = 1,\n               max_n: int = 1000, length: int = None, min_length: int = 10, max_length: int = 5_000_000, weights=None):\n        \"\"\"Generates a random RecordBatch for testing purposes.\n\n        Args:\n            alphabet: The ``Alphabet`` to use (e.g. ``Alphabet.DNA``).\n            rng: NumPy random generator.\n            n_seqs: Exact number of records (overrides *min_n*/*max_n*).\n            min_n: Minimum number of random records.\n            max_n: Maximum number of random records.\n            length: Exact sequence length (overrides *min_length*/*max_length*).\n            min_length: Minimum random sequence length.\n            max_length: Maximum random sequence length.\n            weights: Symbol frequency weights for random generation.\n\n        Returns:\n            A new ``RecordBatch`` with random sequences and UUID IDs.\n        \"\"\"\n        if rng is None: rng = RESOURCES.rng\n        seqs = alphabet.random_batch(rng=rng, n_seqs=n_seqs, min_seqs=min_n, max_seqs=max_n, length=length, min_len=min_length,\n                                     max_len=max_length, weights=weights)\n\n        # Generate random UUIDs (Standard for Records)\n        ids = np.array([hexlify(uuid4().bytes) for _ in range(len(seqs))])\n\n        # Construct the batch manually for performance\n        obj = cls.__new__(cls)\n        obj._seqs = seqs\n        obj._ids = ids\n        obj._features = FeatureBatch.empty()\n        obj._feature_offsets = np.zeros(len(seqs) + 1, dtype=np.int32)\n        obj._feature_rec_indices = np.empty(0, dtype=np.int32)\n        obj._qualifiers = QualifierBatch.zeros(len(seqs))\n        return obj\n\n\n    @classmethod\n    def from_aligned_batch(cls, batch: SeqBatch, records: list[Record]):\n        \"\"\"Creates a RecordBatch from a pre-computed SeqBatch and matching Records.\n\n        Assumes ``records[i]`` corresponds to ``batch[i]``. Useful when sequences\n        have already been aligned or deduplicated externally.\n\n        Args:\n            batch: A pre-computed ``SeqBatch``.\n            records: A list of ``Record`` objects providing IDs, qualifiers,\n                and features.\n\n        Returns:\n            A new ``RecordBatch``.\n        \"\"\"\n        obj = cls.__new__(cls)\n        obj._seqs = batch\n        obj._ids = np.array([r.id for r in records])\n        obj._qualifiers = QualifierBatch.build(r.qualifiers for r in records)\n        obj._build_features(records)\n        return obj\n\n    @classmethod\n    def build(cls, components: Iterable[Record]) -&gt; 'RecordBatch':\n        \"\"\"Constructs a RecordBatch from an iterable of Record objects.\n\n        Args:\n            components: An iterable of ``Record`` objects.\n\n        Returns:\n            A new ``RecordBatch``.\n        \"\"\"\n        return cls(components)\n\n    def _build_features(self, recs):\n        # 1. Count total features\n        n_feats = sum(len(r.features) for r in recs)\n\n        # 2. Allocate Arrays\n        starts = np.empty(n_feats, dtype=np.int32)\n        ends = np.empty(n_feats, dtype=np.int32)\n        strands = np.empty(n_feats, dtype=np.int32)\n        keys = np.empty(n_feats, dtype=np.int16)\n\n        # 4. Record offsets &amp; Indices\n        self._feature_offsets = np.zeros(len(recs) + 1, dtype=np.int32)\n        self._feature_rec_indices = np.empty(n_feats, dtype=np.int32)\n\n        # 5. Fill Arrays\n        curr_idx = 0\n        for i, r in enumerate(recs):\n            self._feature_offsets[i+1] = self._feature_offsets[i] + len(r.features)\n            for f in r.features:\n                starts[curr_idx] = f.interval.start\n                ends[curr_idx] = f.interval.end\n                strands[curr_idx] = f.interval.strand\n\n                keys[curr_idx] = f.key.value\n\n                self._feature_rec_indices[curr_idx] = i\n                curr_idx += 1\n\n        # 6. Create Batches\n        # IMPORTANT: Do not sort! We must preserve the record-grouped order.\n        intervals = IntervalBatch(starts, ends, strands, sort=False)\n\n        # Generator to avoid creating intermediate list of qualifier lists\n        # We iterate records again, but this is cheap compared to list allocation overhead\n        def qual_gen():\n            for r in recs:\n                for f in r.features:\n                    yield f.qualifiers\n\n        qualifiers = QualifierBatch.build(qual_gen())\n\n        self._features = FeatureBatch(intervals, keys, qualifiers)\n\n    @classmethod\n    def empty(cls, alphabet: Alphabet) -&gt; 'RecordBatch':\n        \"\"\"Creates an empty RecordBatch with zero records.\n\n        Args:\n            alphabet: The ``Alphabet`` for the (empty) sequence batch.\n\n        Returns:\n            An empty ``RecordBatch``.\n        \"\"\"\n        return cls.zeros(0, alphabet)\n\n    @classmethod\n    def zeros(cls, n: int, alphabet: Alphabet) -&gt; 'RecordBatch':\n        \"\"\"Creates a batch of *n* placeholder records with empty sequences.\n\n        Args:\n            n: Number of placeholder records.\n            alphabet: The ``Alphabet`` for the sequences.\n\n        Returns:\n            A ``RecordBatch`` with empty sequences and no features.\n\n        Examples:\n            &gt;&gt;&gt; batch = RecordBatch.zeros(5, Alphabet.DNA)\n            &gt;&gt;&gt; len(batch)\n            5\n        \"\"\"\n        obj = cls.__new__(cls)\n        obj._seqs = alphabet.zeros_batch(n)\n        obj._ids = np.full(n, b'', dtype='S1') # Empty bytes, will grow if needed or stay empty\n        obj._qualifiers = QualifierBatch.zeros(n)\n        obj._features = FeatureBatch.empty()\n        obj._feature_offsets = np.zeros(n + 1, dtype=np.int32)\n        obj._feature_rec_indices = np.empty(0, dtype=np.int32)\n        return obj\n\n    @classmethod\n    def concat(cls, batches: Iterable['RecordBatch'], deduplicate: bool = False) -&gt; 'RecordBatch':\n        \"\"\"Concatenates multiple RecordBatch objects into one.\n\n        Correctly adjusts feature-to-record index mappings and offset arrays\n        across batch boundaries.\n\n        Args:\n            batches: An iterable of ``RecordBatch`` objects.\n            deduplicate: Whether to deduplicate identical records.\n\n        Returns:\n            A single concatenated ``RecordBatch``.\n\n        Raises:\n            ValueError: If the list is empty.\n\n        Examples:\n            &gt;&gt;&gt; combined = RecordBatch.concat([batch_a, batch_b])\n        \"\"\"\n        batches = list(batches)\n        if not batches: raise ValueError(\"Cannot concatenate empty list\")\n\n        # 1. Simple Concatenations\n        seqs = SeqBatch.concat([b.seqs for b in batches])\n        ids = np.concatenate([b.ids for b in batches])\n        qualifiers = QualifierBatch.concat([b._qualifiers for b in batches])\n\n        # 2. Feature Concatenation (Complex)\n        # We must adjust rec_indices and offsets\n\n        # Calculate shifts for record indices (how many records were in previous batches)\n        rec_counts = [len(b) for b in batches]\n        rec_shifts = np.cumsum([0] + rec_counts[:-1])\n\n        # Adjust indices\n        new_rec_indices = np.concatenate([\n            b._feature_rec_indices + shift \n            for b, shift in zip(batches, rec_shifts)\n        ])\n\n        # Stack offsets (using RaggedBatch logic manually here as this isn't a RaggedBatch subclass)\n        feat_counts = [b.n_features for b in batches]\n        feat_shifts = np.cumsum([0] + feat_counts[:-1])\n\n        offset_parts = [batches[0]._feature_offsets]\n        for i in range(1, len(batches)):\n            # Take offsets[1:] and add the cumulative feature count\n            offset_parts.append(batches[i]._feature_offsets[1:] + feat_shifts[i])\n\n        new_offsets = np.concatenate(offset_parts)\n\n        # Merge FeatureBatches\n        features = FeatureBatch.concat([b._features for b in batches])\n\n        return cls._reconstruct(seqs, ids, features, new_rec_indices, new_offsets, qualifiers)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by all internal arrays.\n        \"\"\"\n        return (self._seqs.nbytes + self._ids.nbytes + self._features.nbytes + \n                self._qualifiers.nbytes + self._feature_offsets.nbytes + self._feature_rec_indices.nbytes)\n\n    def copy(self) -&gt; 'RecordBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``RecordBatch`` with copied arrays.\n        \"\"\"\n        return self._reconstruct(\n            self._seqs.copy(), self._ids.copy(), self._features.copy(), \n            self._feature_rec_indices.copy(), self._feature_offsets.copy(), self._qualifiers.copy())\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``Record`` class.\n        \"\"\"\n        return Record\n\n    def __repr__(self): return f\"&lt;RecordBatch: {len(self)} records&gt;\"\n    def __len__(self): return len(self._seqs)\n\n    @property\n    def ids(self) -&gt; np.ndarray:\n        \"\"\"Returns the record IDs as a numpy object array.\n\n        Returns:\n            A numpy array of ``bytes`` IDs.\n        \"\"\"\n        return self._ids\n\n    @property\n    def seqs(self) -&gt; SeqBatch:\n        \"\"\"Returns the underlying sequence batch.\n\n        Returns:\n            A ``SeqBatch``.\n        \"\"\"\n        return self._seqs\n\n    @property\n    def intervals(self) -&gt; IntervalBatch:\n        \"\"\"Returns the intervals of all features across all records.\n\n        Returns:\n            An ``IntervalBatch``.\n        \"\"\"\n        return self._features.intervals\n\n    @property\n    def n_features(self) -&gt; int:\n        \"\"\"Returns the total number of features across all records.\n\n        Returns:\n            Feature count.\n        \"\"\"\n        return len(self._features)\n\n    def get_record(self, idx: int) -&gt; 'Record':\n        \"\"\"Reconstructs a full Record object from the batch at index *idx*.\n\n        Args:\n            idx: Zero-based record index.\n\n        Returns:\n            A ``Record`` with its sequence, features, and qualifiers.\n\n        Examples:\n            &gt;&gt;&gt; rec = batch.get_record(0)\n            &gt;&gt;&gt; rec.id\n            b'contig_1'\n        \"\"\"\n        seq = self.seqs[idx]\n        rec_id = self.ids[idx]\n\n        start = self._feature_offsets[idx]\n        end = self._feature_offsets[idx+1]\n        features = [self._features[i] for i in range(start, end)]\n        quals = self._qualifiers[idx]\n\n        return Record(seq, id_=rec_id, features=features, qualifiers=quals)\n\n    def get_qualifiers(self, idx: int) -&gt; list[tuple]:\n        \"\"\"Returns the qualifiers for the record at *idx*.\n\n        Args:\n            idx: Zero-based record index.\n\n        Returns:\n            A list of ``(key, value)`` tuples.\n        \"\"\"\n        return self._qualifiers[idx]\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)): return self.get_record(item)\n\n        if isinstance(item, slice):\n            start, stop, step = item.indices(len(self))\n            if step != 1: raise NotImplementedError(\"RecordBatch slicing with step != 1 not supported\")\n\n            # 1. Slice simple arrays\n            new_seqs = self._seqs[item]\n            new_ids = self._ids[item]\n            new_quals = self._qualifiers[item]\n\n            # 2. Slice Features\n            feat_start = self._feature_offsets[start]\n            feat_end = self._feature_offsets[stop]\n\n            new_features = self._features[feat_start:feat_end]\n            new_offsets = self._feature_offsets[start:stop+1] - feat_start\n            new_rec_indices = self._feature_rec_indices[feat_start:feat_end] - start\n\n            # 3. Construct\n            return self._reconstruct(new_seqs, new_ids, new_features, new_rec_indices, new_offsets, new_quals)\n\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    @classmethod\n    def _reconstruct(cls, seqs, ids, features, rec_indices, offsets, qualifiers):\n        obj = cls.__new__(cls)\n        obj._seqs = seqs\n        obj._ids = ids\n        obj._features = features\n        obj._feature_rec_indices = rec_indices\n        obj._feature_offsets = offsets\n        obj._qualifiers = qualifiers\n        return obj\n\n    def add_features(self, features: 'FeatureBatch', pivot_key: FeatureKey = FeatureKey.SOURCE) -&gt; 'RecordBatch':\n        \"\"\"Returns a new RecordBatch with additional features mapped via a pivot qualifier.\n\n        Each feature is assigned to a record by matching a qualifier value\n        (identified by *pivot_key*) against record IDs. Uses vectorized\n        qualifier lookup through ``QualifierBatch._key_ids``.\n\n        Args:\n            features: A ``FeatureBatch`` of features to add.\n            pivot_key: The ``FeatureKey`` whose qualifier value identifies\n                the target record (default ``SOURCE``).\n\n        Returns:\n            A new ``RecordBatch`` with the matched features merged in.\n\n        Raises:\n            ValueError: If *pivot_key* is not found in the feature qualifiers.\n\n        Examples:\n            &gt;&gt;&gt; annotated = batch.add_features(gff_features, pivot_key=FeatureKey.SOURCE)\n        \"\"\"\n        if len(features) == 0: return self.copy()\n\n        # 1. Build id \u2192 record index lookup\n        id_to_idx = {self._ids[i]: i for i in range(len(self))}\n\n        # 2. Find the pivot key in the qualifier vocabulary\n        quals = features._qualifiers\n        pivot_bytes = pivot_key.bytes\n        pivot_vocab_idx = -1\n        for i, k in enumerate(quals._key_vocab):\n            if k == pivot_bytes:\n                pivot_vocab_idx = i\n                break\n\n        if pivot_vocab_idx == -1:\n            raise ValueError(f\"Pivot key '{pivot_key}' not found in feature qualifiers\")\n\n        # 3. Vectorized: find all qualifier entries matching the pivot key\n        is_pivot = (quals._key_ids == pivot_vocab_idx)\n\n        # 4. Map each qualifier position to its feature index\n        n_new = len(features)\n        feat_of_qual = np.repeat(np.arange(n_new, dtype=np.int32), np.diff(quals._offsets))\n\n        # 5. Extract the first pivot value per feature\n        pivot_positions = np.where(is_pivot)[0]\n        pivot_feat_indices = feat_of_qual[pivot_positions]\n        pivot_values = quals._values[pivot_positions]\n\n        # Take only first match per feature\n        _, first_idx = np.unique(pivot_feat_indices, return_index=True)\n        matched_feat_indices = pivot_feat_indices[first_idx]\n        matched_values = pivot_values[first_idx]\n\n        # 6. Map pivot values to record indices\n        rec_indices = np.array([id_to_idx.get(v, -1) for v in matched_values], dtype=np.int32)\n        valid = rec_indices &gt;= 0\n        matched_feat_indices = matched_feat_indices[valid]\n        new_rec_indices = rec_indices[valid]\n\n        if len(matched_feat_indices) == 0: return self.copy()\n\n        # 7. Slice matched features from the input batch\n        new_features = FeatureBatch.concat([features[int(i):int(i)+1] for i in matched_feat_indices])\n\n        # 8. Merge with existing features, grouped by record\n        # Sort new features by record index for offset construction\n        sort_order = np.argsort(new_rec_indices)\n        new_rec_indices = new_rec_indices[sort_order]\n        sorted_feat_indices = np.arange(len(new_rec_indices))\n        # Rebuild new_features in sorted order\n        reordered = [matched_feat_indices[sort_order[j]] for j in range(len(sort_order))]\n        new_features = FeatureBatch.concat([features[int(i):int(i)+1] for i in reordered])\n\n        # 9. Combine: interleave existing and new features per record\n        n_recs = len(self)\n        combined_features = FeatureBatch.concat([self._features, new_features])\n        combined_rec_indices = np.concatenate([self._feature_rec_indices, new_rec_indices])\n\n        # Rebuild offsets from combined rec_indices\n        new_offsets = np.zeros(n_recs + 1, dtype=np.int32)\n        if len(combined_rec_indices) &gt; 0:\n            counts = np.bincount(combined_rec_indices, minlength=n_recs)\n            np.cumsum(counts, out=new_offsets[1:])\n\n        # Sort combined features by (rec_index, position) for proper grouping\n        order = np.argsort(combined_rec_indices, kind='stable')\n        combined_rec_indices = combined_rec_indices[order]\n\n        # Reorder the FeatureBatch by the sort order\n        reordered_intervals = IntervalBatch(\n            combined_features.intervals.starts[order],\n            combined_features.intervals.ends[order],\n            combined_features.intervals.strands[order],\n            sort=False\n        )\n        reordered_keys = combined_features.keys[order]\n        reordered_quals = QualifierBatch.build(combined_features._qualifiers[int(i)] for i in order)\n        combined_features = FeatureBatch(reordered_intervals, reordered_keys, reordered_quals)\n\n        return self._reconstruct(\n            self._seqs, self._ids, combined_features,\n            combined_rec_indices, new_offsets, self._qualifiers\n        )\n\n    def extract_features(self, kind: bytes = None) -&gt; 'SeqBatch':\n        \"\"\"Extracts sequences for all features, optionally filtered by type.\n\n        Uses a Numba-accelerated kernel for parallel extraction with\n        automatic reverse complement for minus-strand features.\n\n        Args:\n            kind: Optional ``FeatureKey`` bytes value to filter by\n                (e.g. ``FeatureKey.CDS.bytes``). Extracts all features\n                if ``None``.\n\n        Returns:\n            A ``SeqBatch`` of the extracted feature sequences.\n\n        Examples:\n            &gt;&gt;&gt; cds_seqs = batch.extract_features(FeatureKey.CDS.bytes)\n            &gt;&gt;&gt; len(cds_seqs)\n            42\n        \"\"\"\n        if kind is not None:\n            # Find ID for kind\n            key_obj = FeatureKey.from_bytes(kind)\n            target_val = key_obj.value\n            mask = self._features.keys == target_val\n            starts = self._features.intervals.starts[mask]\n            ends = self._features.intervals.ends[mask]\n            strands = self._features.intervals.strands[mask]\n            rec_idxs = self._feature_rec_indices[mask]\n        else:\n            starts, ends, strands = self._features.intervals.starts, self._features.intervals.ends, self._features.intervals.strands\n            rec_idxs = self._feature_rec_indices\n\n        # Optimized Extraction\n        n_feats = len(starts)\n        lengths = ends - starts\n        # Clip negative lengths (safety)\n        np.maximum(lengths, 0, out=lengths)\n\n        out_starts = np.zeros(n_feats, dtype=np.int32)\n        if n_feats &gt; 1:\n            np.cumsum(lengths[:-1], out=out_starts[1:])\n\n        total_len = out_starts[-1] + lengths[-1] if n_feats &gt; 0 else 0\n        out_data = np.empty(total_len, dtype=np.uint8)\n\n        rc_table = self.seqs.alphabet.complement\n        if rc_table is None:\n            rc_table = np.empty(0, dtype=np.uint8) # Dummy\n\n        _batch_extract_kernel(\n            self.seqs.encoded, self.seqs.starts,\n            starts, strands, rec_idxs,\n            out_data, out_starts, lengths,\n            rc_table\n        )\n\n        return self.seqs.alphabet.new_batch(out_data, out_starts, lengths)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>Record</code> class.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.ids","title":"<code>ids</code>  <code>property</code>","text":"<p>Returns the record IDs as a numpy object array.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of <code>bytes</code> IDs.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.intervals","title":"<code>intervals</code>  <code>property</code>","text":"<p>Returns the intervals of all features across all records.</p> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An <code>IntervalBatch</code>.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.n_features","title":"<code>n_features</code>  <code>property</code>","text":"<p>Returns the total number of features across all records.</p> <p>Returns:</p> Type Description <code>int</code> <p>Feature count.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by all internal arrays.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.seqs","title":"<code>seqs</code>  <code>property</code>","text":"<p>Returns the underlying sequence batch.</p> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A <code>SeqBatch</code>.</p>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.add_features","title":"<code>add_features(features, pivot_key=FeatureKey.SOURCE)</code>","text":"<p>Returns a new RecordBatch with additional features mapped via a pivot qualifier.</p> <p>Each feature is assigned to a record by matching a qualifier value (identified by pivot_key) against record IDs. Uses vectorized qualifier lookup through <code>QualifierBatch._key_ids</code>.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>FeatureBatch</code> <p>A <code>FeatureBatch</code> of features to add.</p> required <code>pivot_key</code> <code>FeatureKey</code> <p>The <code>FeatureKey</code> whose qualifier value identifies the target record (default <code>SOURCE</code>).</p> <code>SOURCE</code> <p>Returns:</p> Type Description <code>RecordBatch</code> <p>A new <code>RecordBatch</code> with the matched features merged in.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If pivot_key is not found in the feature qualifiers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; annotated = batch.add_features(gff_features, pivot_key=FeatureKey.SOURCE)\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>def add_features(self, features: 'FeatureBatch', pivot_key: FeatureKey = FeatureKey.SOURCE) -&gt; 'RecordBatch':\n    \"\"\"Returns a new RecordBatch with additional features mapped via a pivot qualifier.\n\n    Each feature is assigned to a record by matching a qualifier value\n    (identified by *pivot_key*) against record IDs. Uses vectorized\n    qualifier lookup through ``QualifierBatch._key_ids``.\n\n    Args:\n        features: A ``FeatureBatch`` of features to add.\n        pivot_key: The ``FeatureKey`` whose qualifier value identifies\n            the target record (default ``SOURCE``).\n\n    Returns:\n        A new ``RecordBatch`` with the matched features merged in.\n\n    Raises:\n        ValueError: If *pivot_key* is not found in the feature qualifiers.\n\n    Examples:\n        &gt;&gt;&gt; annotated = batch.add_features(gff_features, pivot_key=FeatureKey.SOURCE)\n    \"\"\"\n    if len(features) == 0: return self.copy()\n\n    # 1. Build id \u2192 record index lookup\n    id_to_idx = {self._ids[i]: i for i in range(len(self))}\n\n    # 2. Find the pivot key in the qualifier vocabulary\n    quals = features._qualifiers\n    pivot_bytes = pivot_key.bytes\n    pivot_vocab_idx = -1\n    for i, k in enumerate(quals._key_vocab):\n        if k == pivot_bytes:\n            pivot_vocab_idx = i\n            break\n\n    if pivot_vocab_idx == -1:\n        raise ValueError(f\"Pivot key '{pivot_key}' not found in feature qualifiers\")\n\n    # 3. Vectorized: find all qualifier entries matching the pivot key\n    is_pivot = (quals._key_ids == pivot_vocab_idx)\n\n    # 4. Map each qualifier position to its feature index\n    n_new = len(features)\n    feat_of_qual = np.repeat(np.arange(n_new, dtype=np.int32), np.diff(quals._offsets))\n\n    # 5. Extract the first pivot value per feature\n    pivot_positions = np.where(is_pivot)[0]\n    pivot_feat_indices = feat_of_qual[pivot_positions]\n    pivot_values = quals._values[pivot_positions]\n\n    # Take only first match per feature\n    _, first_idx = np.unique(pivot_feat_indices, return_index=True)\n    matched_feat_indices = pivot_feat_indices[first_idx]\n    matched_values = pivot_values[first_idx]\n\n    # 6. Map pivot values to record indices\n    rec_indices = np.array([id_to_idx.get(v, -1) for v in matched_values], dtype=np.int32)\n    valid = rec_indices &gt;= 0\n    matched_feat_indices = matched_feat_indices[valid]\n    new_rec_indices = rec_indices[valid]\n\n    if len(matched_feat_indices) == 0: return self.copy()\n\n    # 7. Slice matched features from the input batch\n    new_features = FeatureBatch.concat([features[int(i):int(i)+1] for i in matched_feat_indices])\n\n    # 8. Merge with existing features, grouped by record\n    # Sort new features by record index for offset construction\n    sort_order = np.argsort(new_rec_indices)\n    new_rec_indices = new_rec_indices[sort_order]\n    sorted_feat_indices = np.arange(len(new_rec_indices))\n    # Rebuild new_features in sorted order\n    reordered = [matched_feat_indices[sort_order[j]] for j in range(len(sort_order))]\n    new_features = FeatureBatch.concat([features[int(i):int(i)+1] for i in reordered])\n\n    # 9. Combine: interleave existing and new features per record\n    n_recs = len(self)\n    combined_features = FeatureBatch.concat([self._features, new_features])\n    combined_rec_indices = np.concatenate([self._feature_rec_indices, new_rec_indices])\n\n    # Rebuild offsets from combined rec_indices\n    new_offsets = np.zeros(n_recs + 1, dtype=np.int32)\n    if len(combined_rec_indices) &gt; 0:\n        counts = np.bincount(combined_rec_indices, minlength=n_recs)\n        np.cumsum(counts, out=new_offsets[1:])\n\n    # Sort combined features by (rec_index, position) for proper grouping\n    order = np.argsort(combined_rec_indices, kind='stable')\n    combined_rec_indices = combined_rec_indices[order]\n\n    # Reorder the FeatureBatch by the sort order\n    reordered_intervals = IntervalBatch(\n        combined_features.intervals.starts[order],\n        combined_features.intervals.ends[order],\n        combined_features.intervals.strands[order],\n        sort=False\n    )\n    reordered_keys = combined_features.keys[order]\n    reordered_quals = QualifierBatch.build(combined_features._qualifiers[int(i)] for i in order)\n    combined_features = FeatureBatch(reordered_intervals, reordered_keys, reordered_quals)\n\n    return self._reconstruct(\n        self._seqs, self._ids, combined_features,\n        combined_rec_indices, new_offsets, self._qualifiers\n    )\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Constructs a RecordBatch from an iterable of Record objects.</p> <p>Parameters:</p> Name Type Description Default <code>components</code> <code>Iterable[Record]</code> <p>An iterable of <code>Record</code> objects.</p> required <p>Returns:</p> Type Description <code>RecordBatch</code> <p>A new <code>RecordBatch</code>.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable[Record]) -&gt; 'RecordBatch':\n    \"\"\"Constructs a RecordBatch from an iterable of Record objects.\n\n    Args:\n        components: An iterable of ``Record`` objects.\n\n    Returns:\n        A new ``RecordBatch``.\n    \"\"\"\n    return cls(components)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.concat","title":"<code>concat(batches, deduplicate=False)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple RecordBatch objects into one.</p> <p>Correctly adjusts feature-to-record index mappings and offset arrays across batch boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[RecordBatch]</code> <p>An iterable of <code>RecordBatch</code> objects.</p> required <code>deduplicate</code> <code>bool</code> <p>Whether to deduplicate identical records.</p> <code>False</code> <p>Returns:</p> Type Description <code>RecordBatch</code> <p>A single concatenated <code>RecordBatch</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the list is empty.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combined = RecordBatch.concat([batch_a, batch_b])\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['RecordBatch'], deduplicate: bool = False) -&gt; 'RecordBatch':\n    \"\"\"Concatenates multiple RecordBatch objects into one.\n\n    Correctly adjusts feature-to-record index mappings and offset arrays\n    across batch boundaries.\n\n    Args:\n        batches: An iterable of ``RecordBatch`` objects.\n        deduplicate: Whether to deduplicate identical records.\n\n    Returns:\n        A single concatenated ``RecordBatch``.\n\n    Raises:\n        ValueError: If the list is empty.\n\n    Examples:\n        &gt;&gt;&gt; combined = RecordBatch.concat([batch_a, batch_b])\n    \"\"\"\n    batches = list(batches)\n    if not batches: raise ValueError(\"Cannot concatenate empty list\")\n\n    # 1. Simple Concatenations\n    seqs = SeqBatch.concat([b.seqs for b in batches])\n    ids = np.concatenate([b.ids for b in batches])\n    qualifiers = QualifierBatch.concat([b._qualifiers for b in batches])\n\n    # 2. Feature Concatenation (Complex)\n    # We must adjust rec_indices and offsets\n\n    # Calculate shifts for record indices (how many records were in previous batches)\n    rec_counts = [len(b) for b in batches]\n    rec_shifts = np.cumsum([0] + rec_counts[:-1])\n\n    # Adjust indices\n    new_rec_indices = np.concatenate([\n        b._feature_rec_indices + shift \n        for b, shift in zip(batches, rec_shifts)\n    ])\n\n    # Stack offsets (using RaggedBatch logic manually here as this isn't a RaggedBatch subclass)\n    feat_counts = [b.n_features for b in batches]\n    feat_shifts = np.cumsum([0] + feat_counts[:-1])\n\n    offset_parts = [batches[0]._feature_offsets]\n    for i in range(1, len(batches)):\n        # Take offsets[1:] and add the cumulative feature count\n        offset_parts.append(batches[i]._feature_offsets[1:] + feat_shifts[i])\n\n    new_offsets = np.concatenate(offset_parts)\n\n    # Merge FeatureBatches\n    features = FeatureBatch.concat([b._features for b in batches])\n\n    return cls._reconstruct(seqs, ids, features, new_rec_indices, new_offsets, qualifiers)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>RecordBatch</code> <p>A new <code>RecordBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>def copy(self) -&gt; 'RecordBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``RecordBatch`` with copied arrays.\n    \"\"\"\n    return self._reconstruct(\n        self._seqs.copy(), self._ids.copy(), self._features.copy(), \n        self._feature_rec_indices.copy(), self._feature_offsets.copy(), self._qualifiers.copy())\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.empty","title":"<code>empty(alphabet)</code>  <code>classmethod</code>","text":"<p>Creates an empty RecordBatch with zero records.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>Alphabet</code> <p>The <code>Alphabet</code> for the (empty) sequence batch.</p> required <p>Returns:</p> Type Description <code>RecordBatch</code> <p>An empty <code>RecordBatch</code>.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>@classmethod\ndef empty(cls, alphabet: Alphabet) -&gt; 'RecordBatch':\n    \"\"\"Creates an empty RecordBatch with zero records.\n\n    Args:\n        alphabet: The ``Alphabet`` for the (empty) sequence batch.\n\n    Returns:\n        An empty ``RecordBatch``.\n    \"\"\"\n    return cls.zeros(0, alphabet)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.extract_features","title":"<code>extract_features(kind=None)</code>","text":"<p>Extracts sequences for all features, optionally filtered by type.</p> <p>Uses a Numba-accelerated kernel for parallel extraction with automatic reverse complement for minus-strand features.</p> <p>Parameters:</p> Name Type Description Default <code>kind</code> <code>bytes</code> <p>Optional <code>FeatureKey</code> bytes value to filter by (e.g. <code>FeatureKey.CDS.bytes</code>). Extracts all features if <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A <code>SeqBatch</code> of the extracted feature sequences.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cds_seqs = batch.extract_features(FeatureKey.CDS.bytes)\n&gt;&gt;&gt; len(cds_seqs)\n42\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>def extract_features(self, kind: bytes = None) -&gt; 'SeqBatch':\n    \"\"\"Extracts sequences for all features, optionally filtered by type.\n\n    Uses a Numba-accelerated kernel for parallel extraction with\n    automatic reverse complement for minus-strand features.\n\n    Args:\n        kind: Optional ``FeatureKey`` bytes value to filter by\n            (e.g. ``FeatureKey.CDS.bytes``). Extracts all features\n            if ``None``.\n\n    Returns:\n        A ``SeqBatch`` of the extracted feature sequences.\n\n    Examples:\n        &gt;&gt;&gt; cds_seqs = batch.extract_features(FeatureKey.CDS.bytes)\n        &gt;&gt;&gt; len(cds_seqs)\n        42\n    \"\"\"\n    if kind is not None:\n        # Find ID for kind\n        key_obj = FeatureKey.from_bytes(kind)\n        target_val = key_obj.value\n        mask = self._features.keys == target_val\n        starts = self._features.intervals.starts[mask]\n        ends = self._features.intervals.ends[mask]\n        strands = self._features.intervals.strands[mask]\n        rec_idxs = self._feature_rec_indices[mask]\n    else:\n        starts, ends, strands = self._features.intervals.starts, self._features.intervals.ends, self._features.intervals.strands\n        rec_idxs = self._feature_rec_indices\n\n    # Optimized Extraction\n    n_feats = len(starts)\n    lengths = ends - starts\n    # Clip negative lengths (safety)\n    np.maximum(lengths, 0, out=lengths)\n\n    out_starts = np.zeros(n_feats, dtype=np.int32)\n    if n_feats &gt; 1:\n        np.cumsum(lengths[:-1], out=out_starts[1:])\n\n    total_len = out_starts[-1] + lengths[-1] if n_feats &gt; 0 else 0\n    out_data = np.empty(total_len, dtype=np.uint8)\n\n    rc_table = self.seqs.alphabet.complement\n    if rc_table is None:\n        rc_table = np.empty(0, dtype=np.uint8) # Dummy\n\n    _batch_extract_kernel(\n        self.seqs.encoded, self.seqs.starts,\n        starts, strands, rec_idxs,\n        out_data, out_starts, lengths,\n        rc_table\n    )\n\n    return self.seqs.alphabet.new_batch(out_data, out_starts, lengths)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.from_aligned_batch","title":"<code>from_aligned_batch(batch, records)</code>  <code>classmethod</code>","text":"<p>Creates a RecordBatch from a pre-computed SeqBatch and matching Records.</p> <p>Assumes <code>records[i]</code> corresponds to <code>batch[i]</code>. Useful when sequences have already been aligned or deduplicated externally.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>SeqBatch</code> <p>A pre-computed <code>SeqBatch</code>.</p> required <code>records</code> <code>list[Record]</code> <p>A list of <code>Record</code> objects providing IDs, qualifiers, and features.</p> required <p>Returns:</p> Type Description <p>A new <code>RecordBatch</code>.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>@classmethod\ndef from_aligned_batch(cls, batch: SeqBatch, records: list[Record]):\n    \"\"\"Creates a RecordBatch from a pre-computed SeqBatch and matching Records.\n\n    Assumes ``records[i]`` corresponds to ``batch[i]``. Useful when sequences\n    have already been aligned or deduplicated externally.\n\n    Args:\n        batch: A pre-computed ``SeqBatch``.\n        records: A list of ``Record`` objects providing IDs, qualifiers,\n            and features.\n\n    Returns:\n        A new ``RecordBatch``.\n    \"\"\"\n    obj = cls.__new__(cls)\n    obj._seqs = batch\n    obj._ids = np.array([r.id for r in records])\n    obj._qualifiers = QualifierBatch.build(r.qualifiers for r in records)\n    obj._build_features(records)\n    return obj\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.get_qualifiers","title":"<code>get_qualifiers(idx)</code>","text":"<p>Returns the qualifiers for the record at idx.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Zero-based record index.</p> required <p>Returns:</p> Type Description <code>list[tuple]</code> <p>A list of <code>(key, value)</code> tuples.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>def get_qualifiers(self, idx: int) -&gt; list[tuple]:\n    \"\"\"Returns the qualifiers for the record at *idx*.\n\n    Args:\n        idx: Zero-based record index.\n\n    Returns:\n        A list of ``(key, value)`` tuples.\n    \"\"\"\n    return self._qualifiers[idx]\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.get_record","title":"<code>get_record(idx)</code>","text":"<p>Reconstructs a full Record object from the batch at index idx.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Zero-based record index.</p> required <p>Returns:</p> Type Description <code>Record</code> <p>A <code>Record</code> with its sequence, features, and qualifiers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; rec = batch.get_record(0)\n&gt;&gt;&gt; rec.id\nb'contig_1'\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>def get_record(self, idx: int) -&gt; 'Record':\n    \"\"\"Reconstructs a full Record object from the batch at index *idx*.\n\n    Args:\n        idx: Zero-based record index.\n\n    Returns:\n        A ``Record`` with its sequence, features, and qualifiers.\n\n    Examples:\n        &gt;&gt;&gt; rec = batch.get_record(0)\n        &gt;&gt;&gt; rec.id\n        b'contig_1'\n    \"\"\"\n    seq = self.seqs[idx]\n    rec_id = self.ids[idx]\n\n    start = self._feature_offsets[idx]\n    end = self._feature_offsets[idx+1]\n    features = [self._features[i] for i in range(start, end)]\n    quals = self._qualifiers[idx]\n\n    return Record(seq, id_=rec_id, features=features, qualifiers=quals)\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.random","title":"<code>random(alphabet, rng=None, n_seqs=None, min_n=1, max_n=1000, length=None, min_length=10, max_length=5000000, weights=None)</code>  <code>classmethod</code>","text":"<p>Generates a random RecordBatch for testing purposes.</p> <p>Parameters:</p> Name Type Description Default <code>alphabet</code> <code>Alphabet</code> <p>The <code>Alphabet</code> to use (e.g. <code>Alphabet.DNA</code>).</p> required <code>rng</code> <code>Generator</code> <p>NumPy random generator.</p> <code>None</code> <code>n_seqs</code> <code>int</code> <p>Exact number of records (overrides min_n/max_n).</p> <code>None</code> <code>min_n</code> <code>int</code> <p>Minimum number of random records.</p> <code>1</code> <code>max_n</code> <code>int</code> <p>Maximum number of random records.</p> <code>1000</code> <code>length</code> <code>int</code> <p>Exact sequence length (overrides min_length/max_length).</p> <code>None</code> <code>min_length</code> <code>int</code> <p>Minimum random sequence length.</p> <code>10</code> <code>max_length</code> <code>int</code> <p>Maximum random sequence length.</p> <code>5000000</code> <code>weights</code> <p>Symbol frequency weights for random generation.</p> <code>None</code> <p>Returns:</p> Type Description <p>A new <code>RecordBatch</code> with random sequences and UUID IDs.</p> Source code in <code>baclib/containers/record.py</code> <pre><code>@classmethod\ndef random(cls, alphabet: Alphabet, rng: np.random.Generator = None, n_seqs: int = None, min_n: int = 1,\n           max_n: int = 1000, length: int = None, min_length: int = 10, max_length: int = 5_000_000, weights=None):\n    \"\"\"Generates a random RecordBatch for testing purposes.\n\n    Args:\n        alphabet: The ``Alphabet`` to use (e.g. ``Alphabet.DNA``).\n        rng: NumPy random generator.\n        n_seqs: Exact number of records (overrides *min_n*/*max_n*).\n        min_n: Minimum number of random records.\n        max_n: Maximum number of random records.\n        length: Exact sequence length (overrides *min_length*/*max_length*).\n        min_length: Minimum random sequence length.\n        max_length: Maximum random sequence length.\n        weights: Symbol frequency weights for random generation.\n\n    Returns:\n        A new ``RecordBatch`` with random sequences and UUID IDs.\n    \"\"\"\n    if rng is None: rng = RESOURCES.rng\n    seqs = alphabet.random_batch(rng=rng, n_seqs=n_seqs, min_seqs=min_n, max_seqs=max_n, length=length, min_len=min_length,\n                                 max_len=max_length, weights=weights)\n\n    # Generate random UUIDs (Standard for Records)\n    ids = np.array([hexlify(uuid4().bytes) for _ in range(len(seqs))])\n\n    # Construct the batch manually for performance\n    obj = cls.__new__(cls)\n    obj._seqs = seqs\n    obj._ids = ids\n    obj._features = FeatureBatch.empty()\n    obj._feature_offsets = np.zeros(len(seqs) + 1, dtype=np.int32)\n    obj._feature_rec_indices = np.empty(0, dtype=np.int32)\n    obj._qualifiers = QualifierBatch.zeros(len(seqs))\n    return obj\n</code></pre>"},{"location":"reference/baclib/containers/record/#baclib.containers.record.RecordBatch.zeros","title":"<code>zeros(n, alphabet)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n placeholder records with empty sequences.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of placeholder records.</p> required <code>alphabet</code> <code>Alphabet</code> <p>The <code>Alphabet</code> for the sequences.</p> required <p>Returns:</p> Type Description <code>RecordBatch</code> <p>A <code>RecordBatch</code> with empty sequences and no features.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = RecordBatch.zeros(5, Alphabet.DNA)\n&gt;&gt;&gt; len(batch)\n5\n</code></pre> Source code in <code>baclib/containers/record.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int, alphabet: Alphabet) -&gt; 'RecordBatch':\n    \"\"\"Creates a batch of *n* placeholder records with empty sequences.\n\n    Args:\n        n: Number of placeholder records.\n        alphabet: The ``Alphabet`` for the sequences.\n\n    Returns:\n        A ``RecordBatch`` with empty sequences and no features.\n\n    Examples:\n        &gt;&gt;&gt; batch = RecordBatch.zeros(5, Alphabet.DNA)\n        &gt;&gt;&gt; len(batch)\n        5\n    \"\"\"\n    obj = cls.__new__(cls)\n    obj._seqs = alphabet.zeros_batch(n)\n    obj._ids = np.full(n, b'', dtype='S1') # Empty bytes, will grow if needed or stay empty\n    obj._qualifiers = QualifierBatch.zeros(n)\n    obj._features = FeatureBatch.empty()\n    obj._feature_offsets = np.zeros(n + 1, dtype=np.int32)\n    obj._feature_rec_indices = np.empty(0, dtype=np.int32)\n    return obj\n</code></pre>"},{"location":"reference/baclib/containers/seq/","title":"seq","text":""},{"location":"reference/baclib/containers/seq/#baclib.containers.seq","title":"<code>baclib.containers.seq</code>","text":"<p>Core sequence containers with alphabet-aware encoding, hashing, and batch support.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeq","title":"<code>CompressedSeq</code>","text":"<p>               Bases: <code>HasAlphabet</code>, <code>Batchable</code></p> <p>A bit-packed sequence for reduced memory footprint.</p> <p>Stores symbols using fewer bits per base (e.g. 2 bits for DNA), achieving up to 4\u00d7 compression over <code>Seq</code>.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Packed <code>uint8</code> numpy array.</p> required <code>length</code> <code>int</code> <p>Number of symbols (may differ from <code>len(data)</code> \u00d7 packing ratio due to padding).</p> required <code>alphabet</code> <code>Alphabet</code> <p>The owning <code>Alphabet</code>.</p> required <code>bits</code> <code>int</code> <p>Bits per symbol (e.g. 2 for DNA).</p> required <code>_validation_token</code> <code>object</code> <p>Internal token (must be the alphabet).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cseq = Alphabet.DNA.compress(seq)\n&gt;&gt;&gt; cseq.decompress() == seq\nTrue\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>class CompressedSeq(HasAlphabet, Batchable):\n    \"\"\"\n    A bit-packed sequence for reduced memory footprint.\n\n    Stores symbols using fewer bits per base (e.g. 2 bits for DNA),\n    achieving up to 4\u00d7 compression over ``Seq``.\n\n    Args:\n        data: Packed ``uint8`` numpy array.\n        length: Number of symbols (may differ from ``len(data)`` \u00d7 packing ratio\n            due to padding).\n        alphabet: The owning ``Alphabet``.\n        bits: Bits per symbol (e.g. 2 for DNA).\n        _validation_token: Internal token (must be the alphabet).\n\n    Examples:\n        &gt;&gt;&gt; cseq = Alphabet.DNA.compress(seq)\n        &gt;&gt;&gt; cseq.decompress() == seq\n        True\n    \"\"\"\n    __slots__ = ('_data', '_length', '_alphabet', '_bits')\n    def __init__(self, data: np.ndarray, length: int, alphabet: 'Alphabet', bits: int, _validation_token: object = None):\n        if _validation_token is not alphabet:\n            raise PermissionError(\"CompressedSeq objects must be created via an Alphabet\")\n        self._data = data\n        self._length = length\n        self._alphabet = alphabet\n        self._bits = bits\n\n    def __len__(self): return self._length\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``CompressedSeqBatch`` class.\n        \"\"\"\n        return CompressedSeqBatch\n\n    @property\n    def alphabet(self) -&gt; 'Alphabet':\n        \"\"\"Returns the alphabet used for encoding.\n\n        Returns:\n            The owning ``Alphabet`` singleton.\n        \"\"\"\n        return self._alphabet\n\n    def decompress(self) -&gt; Seq:\n        \"\"\"Decompresses back to a standard ``Seq``.\n\n        Returns:\n            A full ``Seq`` object with the original encoded data.\n\n        Examples:\n            &gt;&gt;&gt; cseq.decompress() == original_seq\n            True\n        \"\"\"\n        return self._alphabet.decompress(self)\n\n    def __repr__(self):\n        return f\"&lt;CompressedSeq: {self._length} bp, {self._bits} bits/sym&gt;\"\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeq.alphabet","title":"<code>alphabet</code>  <code>property</code>","text":"<p>Returns the alphabet used for encoding.</p> <p>Returns:</p> Type Description <code>Alphabet</code> <p>The owning <code>Alphabet</code> singleton.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeq.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>CompressedSeqBatch</code> class.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeq.decompress","title":"<code>decompress()</code>","text":"<p>Decompresses back to a standard <code>Seq</code>.</p> <p>Returns:</p> Type Description <code>Seq</code> <p>A full <code>Seq</code> object with the original encoded data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cseq.decompress() == original_seq\nTrue\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>def decompress(self) -&gt; Seq:\n    \"\"\"Decompresses back to a standard ``Seq``.\n\n    Returns:\n        A full ``Seq`` object with the original encoded data.\n\n    Examples:\n        &gt;&gt;&gt; cseq.decompress() == original_seq\n        True\n    \"\"\"\n    return self._alphabet.decompress(self)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch","title":"<code>CompressedSeqBatch</code>","text":"<p>               Bases: <code>Batch</code>, <code>HasAlphabet</code></p> <p>A batch of bit-packed sequences, byte-aligned for efficient random access.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Packed <code>uint8</code> array of all compressed data.</p> required <code>starts</code> <code>ndarray</code> <p><code>int32</code> offsets into data for each sequence.</p> required <code>lengths</code> <code>ndarray</code> <p><code>int32</code> original (uncompressed) symbol counts.</p> required <code>alphabet</code> <code>Alphabet</code> <p>The shared <code>Alphabet</code>.</p> required <code>bits</code> <code>int</code> <p>Bits per symbol.</p> required <code>_validation_token</code> <code>object</code> <p>Internal token (must be the alphabet).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; cbatch = Alphabet.DNA.compress_batch(batch)\n&gt;&gt;&gt; cbatch.decompress() == batch\nTrue\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>class CompressedSeqBatch(Batch, HasAlphabet):\n    \"\"\"\n    A batch of bit-packed sequences, byte-aligned for efficient random access.\n\n    Args:\n        data: Packed ``uint8`` array of all compressed data.\n        starts: ``int32`` offsets into *data* for each sequence.\n        lengths: ``int32`` original (uncompressed) symbol counts.\n        alphabet: The shared ``Alphabet``.\n        bits: Bits per symbol.\n        _validation_token: Internal token (must be the alphabet).\n\n    Examples:\n        &gt;&gt;&gt; cbatch = Alphabet.DNA.compress_batch(batch)\n        &gt;&gt;&gt; cbatch.decompress() == batch\n        True\n    \"\"\"\n    __slots__ = ('_data', '_starts', '_lengths', '_alphabet', '_bits', '_count')\n    def __init__(self, data: np.ndarray, starts: np.ndarray, lengths: np.ndarray, alphabet: 'Alphabet', bits: int,\n                 _validation_token: object = None):\n        if _validation_token is not alphabet:\n            raise PermissionError(\"CompressedSeqBatch objects must be created via an Alphabet\")\n        self._data = data\n        self._starts = starts\n        self._lengths = lengths\n        self._alphabet = alphabet\n        self._bits = bits\n        self._count = len(lengths)\n\n    def __len__(self): return self._count\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``CompressedSeq`` class.\n        \"\"\"\n        return CompressedSeq\n\n    @property\n    def alphabet(self) -&gt; 'Alphabet':\n        \"\"\"Returns the shared alphabet.\n\n        Returns:\n            The ``Alphabet`` singleton.\n        \"\"\"\n        return self._alphabet\n\n    @classmethod\n    def build(cls, components: Iterable[object]) -&gt; 'Batch':\n        \"\"\"Not supported directly \u2014 use ``Alphabet.compress()``.\n\n        Raises:\n            NotImplementedError: Always.\n        \"\"\"\n        raise NotImplementedError(\"Direct build not supported. Use Alphabet.compress()\")\n\n    @classmethod\n    def concat(cls, batches: Iterable['CompressedSeqBatch']) -&gt; 'CompressedSeqBatch':\n        \"\"\"Concatenates multiple CompressedSeqBatch objects into one.\n\n        Args:\n            batches: An iterable of ``CompressedSeqBatch`` objects.\n\n        Returns:\n            A single concatenated ``CompressedSeqBatch``.\n\n        Raises:\n            ValueError: If the list is empty.\n        \"\"\"\n        batches = list(batches)\n        if not batches: raise ValueError(\"Cannot concatenate empty list\")\n        first = batches[0]\n\n        data = np.concatenate([b._data for b in batches])\n        lengths = np.concatenate([b._lengths for b in batches])\n        starts = np.zeros(len(lengths), dtype=np.int32) \n\n        if len(lengths) &gt; 1: \n            per_byte = 8 // first._bits\n            byte_lens = (lengths + per_byte - 1) // per_byte\n            np.cumsum(byte_lens[:-1], out=starts[1:])\n\n            np.cumsum(byte_lens[:-1], out=starts[1:])\n\n        return cls(data, starts, lengths, first._alphabet, first._bits, _validation_token=first._alphabet)\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by data, starts, and lengths arrays.\n        \"\"\"\n        return self._data.nbytes + self._starts.nbytes + self._lengths.nbytes\n\n    def copy(self) -&gt; 'CompressedSeqBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``CompressedSeqBatch`` with copied arrays.\n        \"\"\"\n        return self.__class__(self._data.copy(), self._starts.copy(), self._lengths.copy(), self._alphabet, self._bits, _validation_token=self._alphabet)\n\n    @classmethod\n    def empty(cls) -&gt; 'CompressedSeqBatch':\n        \"\"\"Not supported directly \u2014 use ``Alphabet.empty_compressed()`` instead.\n\n        Raises:\n            TypeError: Always.\n        \"\"\"\n        raise TypeError(\"CompressedSeqBatch.empty() requires an Alphabet. Use Alphabet.empty_compressed() instead.\")\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'CompressedSeqBatch':\n        \"\"\"Not supported directly \u2014 use ``Alphabet.zeros_compressed(n)`` instead.\n\n        Raises:\n            TypeError: Always.\n        \"\"\"\n        raise TypeError(\"CompressedSeqBatch.zeros() requires an Alphabet. Use Alphabet.zeros_compressed(n) instead.\")\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            if item &lt; 0: item += self._count\n            if not 0 &lt;= item &lt; self._count: raise IndexError(\"Index out of range\")\n            start = self._starts[item]\n            length = self._lengths[item]\n            per_byte = 8 // self._bits\n            byte_len = (length + per_byte - 1) // per_byte\n            data = self._data[start : start + byte_len]\n            return CompressedSeq(data, length, self._alphabet, self._bits, _validation_token=self._alphabet)\n\n        if isinstance(item, slice):\n            start, stop, step = item.indices(self._count)\n            if step != 1: raise NotImplementedError(\"Batch slicing with step != 1 not supported\")\n            new_lengths = self._lengths[start:stop]\n            p_start = self._starts[start]\n            p_end = self._starts[stop] if stop &lt; self._count else len(self._data)\n            return CompressedSeqBatch(self._data[p_start:p_end], self._starts[start:stop] - p_start, new_lengths, self._alphabet, self._bits, _validation_token=self._alphabet)\n\n        raise NotImplementedError(\"Slicing not implemented for CompressedSeqBatch\")\n\n    def decompress(self) -&gt; SeqBatch:\n        \"\"\"Decompresses the batch back to a standard ``SeqBatch``.\n\n        Returns:\n            A ``SeqBatch`` with the original uncompressed symbol data.\n        \"\"\"\n        return self._alphabet.decompress_batch(self)\n\n    def __repr__(self): return f\"&lt;CompressedSeqBatch: {len(self)} sequences&gt;\"\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.alphabet","title":"<code>alphabet</code>  <code>property</code>","text":"<p>Returns the shared alphabet.</p> <p>Returns:</p> Type Description <code>Alphabet</code> <p>The <code>Alphabet</code> singleton.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>CompressedSeq</code> class.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by data, starts, and lengths arrays.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.build","title":"<code>build(components)</code>  <code>classmethod</code>","text":"<p>Not supported directly \u2014 use <code>Alphabet.compress()</code>.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>Always.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>@classmethod\ndef build(cls, components: Iterable[object]) -&gt; 'Batch':\n    \"\"\"Not supported directly \u2014 use ``Alphabet.compress()``.\n\n    Raises:\n        NotImplementedError: Always.\n    \"\"\"\n    raise NotImplementedError(\"Direct build not supported. Use Alphabet.compress()\")\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple CompressedSeqBatch objects into one.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[CompressedSeqBatch]</code> <p>An iterable of <code>CompressedSeqBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>CompressedSeqBatch</code> <p>A single concatenated <code>CompressedSeqBatch</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the list is empty.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['CompressedSeqBatch']) -&gt; 'CompressedSeqBatch':\n    \"\"\"Concatenates multiple CompressedSeqBatch objects into one.\n\n    Args:\n        batches: An iterable of ``CompressedSeqBatch`` objects.\n\n    Returns:\n        A single concatenated ``CompressedSeqBatch``.\n\n    Raises:\n        ValueError: If the list is empty.\n    \"\"\"\n    batches = list(batches)\n    if not batches: raise ValueError(\"Cannot concatenate empty list\")\n    first = batches[0]\n\n    data = np.concatenate([b._data for b in batches])\n    lengths = np.concatenate([b._lengths for b in batches])\n    starts = np.zeros(len(lengths), dtype=np.int32) \n\n    if len(lengths) &gt; 1: \n        per_byte = 8 // first._bits\n        byte_lens = (lengths + per_byte - 1) // per_byte\n        np.cumsum(byte_lens[:-1], out=starts[1:])\n\n        np.cumsum(byte_lens[:-1], out=starts[1:])\n\n    return cls(data, starts, lengths, first._alphabet, first._bits, _validation_token=first._alphabet)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>CompressedSeqBatch</code> <p>A new <code>CompressedSeqBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>def copy(self) -&gt; 'CompressedSeqBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``CompressedSeqBatch`` with copied arrays.\n    \"\"\"\n    return self.__class__(self._data.copy(), self._starts.copy(), self._lengths.copy(), self._alphabet, self._bits, _validation_token=self._alphabet)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.decompress","title":"<code>decompress()</code>","text":"<p>Decompresses the batch back to a standard <code>SeqBatch</code>.</p> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A <code>SeqBatch</code> with the original uncompressed symbol data.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>def decompress(self) -&gt; SeqBatch:\n    \"\"\"Decompresses the batch back to a standard ``SeqBatch``.\n\n    Returns:\n        A ``SeqBatch`` with the original uncompressed symbol data.\n    \"\"\"\n    return self._alphabet.decompress_batch(self)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Not supported directly \u2014 use <code>Alphabet.empty_compressed()</code> instead.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Always.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'CompressedSeqBatch':\n    \"\"\"Not supported directly \u2014 use ``Alphabet.empty_compressed()`` instead.\n\n    Raises:\n        TypeError: Always.\n    \"\"\"\n    raise TypeError(\"CompressedSeqBatch.empty() requires an Alphabet. Use Alphabet.empty_compressed() instead.\")\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.CompressedSeqBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Not supported directly \u2014 use <code>Alphabet.zeros_compressed(n)</code> instead.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Always.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'CompressedSeqBatch':\n    \"\"\"Not supported directly \u2014 use ``Alphabet.zeros_compressed(n)`` instead.\n\n    Raises:\n        TypeError: Always.\n    \"\"\"\n    raise TypeError(\"CompressedSeqBatch.zeros() requires an Alphabet. Use Alphabet.zeros_compressed(n) instead.\")\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq","title":"<code>Seq</code>","text":"<p>               Bases: <code>HasAlphabet</code>, <code>Batchable</code></p> <p>Immutable, alphabet-aware sequence container storing encoded integers (uint8).</p> <p><code>Seq</code> objects should be created via <code>Alphabet.seq()</code> or <code>Alphabet.random()</code> rather than directly, to ensure encoding consistency.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>A numpy uint8 array of encoded symbol indices.</p> required <code>alphabet</code> <code>Alphabet</code> <p>The <code>Alphabet</code> that owns this sequence.</p> required <code>_validation_token</code> <code>object</code> <p>Internal token (must be the alphabet) to prevent direct construction.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = Alphabet.DNA.seq(b'ATGCGA')\n&gt;&gt;&gt; len(seq)\n6\n&gt;&gt;&gt; bytes(seq)\nb'ATGCGA'\n&gt;&gt;&gt; seq[1:4]\nTGC\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>class Seq(HasAlphabet, Batchable):\n    \"\"\"\n    Immutable, alphabet-aware sequence container storing encoded integers (uint8).\n\n    ``Seq`` objects should be created via ``Alphabet.seq()`` or ``Alphabet.random()``\n    rather than directly, to ensure encoding consistency.\n\n    Args:\n        data: A numpy uint8 array of encoded symbol indices.\n        alphabet: The ``Alphabet`` that owns this sequence.\n        _validation_token: Internal token (must be the alphabet) to prevent\n            direct construction.\n\n    Examples:\n        &gt;&gt;&gt; seq = Alphabet.DNA.seq(b'ATGCGA')\n        &gt;&gt;&gt; len(seq)\n        6\n        &gt;&gt;&gt; bytes(seq)\n        b'ATGCGA'\n        &gt;&gt;&gt; seq[1:4]\n        TGC\n    \"\"\"\n    __slots__ = ('_data', '_alphabet', '_hash')\n    def __init__(self, data: np.ndarray, alphabet: 'Alphabet', _validation_token: object = None):\n        if _validation_token is not alphabet:\n            raise PermissionError(\"Seq objects must be created via an Alphabet\")\n        self._alphabet = alphabet\n        self._data = data\n        self._hash = None\n        self._data.flags.writeable = False  # Enforce immutability for hashing safety\n\n    @property\n    def batch(self) -&gt; type['Batch']:\n        \"\"\"Returns the batch type for this class.\n\n        Returns:\n            The ``SeqBatch`` class.\n        \"\"\"\n        return SeqBatch\n\n    @property\n    def alphabet(self) -&gt; 'Alphabet':\n        \"\"\"Returns the alphabet used for encoding/decoding.\n\n        Returns:\n            The owning ``Alphabet`` singleton.\n        \"\"\"\n        return self._alphabet\n\n    @property\n    def encoded(self) -&gt; np.ndarray:\n        \"\"\"Returns the underlying encoded integer array (zero-copy).\n\n        Returns:\n            A read-only ``uint8`` numpy array.\n        \"\"\"\n        return self._data\n\n    def __array__(self, dtype=None):\n        \"\"\"Allows the Seq to be treated as a numpy array.\"\"\"\n        return self._data.astype(dtype, copy=False) if dtype else self._data\n\n    def __bytes__(self) -&gt; bytes: return self._alphabet.decode(self._data)\n\n    def tobytes(self) -&gt; bytes:\n        \"\"\"Decodes the sequence to raw bytes.\n\n        Returns:\n            The decoded byte string.\n\n        Examples:\n            &gt;&gt;&gt; seq.tobytes()\n            b'ATGCGA'\n        \"\"\"\n        return self.__bytes__()\n\n    def __len__(self): return self._data.shape[0]\n    def __str__(self): return self.__bytes__().decode('ascii')\n    def __iter__(self): return iter(self._data)\n    def __repr__(self):\n        if len(self) &lt;= 14: return str(self)\n        # Optimization: Decode only the parts we show\n        head = self._alphabet.decode(self._data[:7]).decode('ascii')\n        tail = self._alphabet.decode(self._data[-7:]).decode('ascii')\n        return f\"{head}...{tail}\"\n    # Use numpy flip for reversal (Zero Copy view if possible)\n    def __reversed__(self) -&gt; 'Seq': return self._alphabet.seq_from(np.flip(self._data))\n\n    def __contains__(self, item):\n        # 1. Handle Integer (Raw code check)\n        if isinstance(item, (int, np.integer)): return item in self._data\n        # 2. Handle Subsequence (Seq, bytes, str)\n        query = None\n        if isinstance(item, Seq):\n            if item.alphabet is not self._alphabet: return False\n            query = item._data.tobytes()\n        elif isinstance(item, (bytes, str)):\n            if isinstance(item, str): item = item.encode('ascii')\n            query = self._alphabet.encode(item).tobytes()\n        # Use bytes substring search (fast C implementation) on raw encoded data\n        if query is not None: return query in self._data.tobytes()\n        return False\n\n    def __eq__(self, other):\n        if self is other: return True\n        if not isinstance(other, Seq): return False\n        if self._alphabet is not other._alphabet: return False\n\n        # Optimization: Fast hash check if available\n        if self._hash is not None and other._hash is not None:\n            if self._hash != other._hash: return False\n\n        return np.array_equal(self._data, other._data)\n\n    def __hash__(self):\n        # Optimization: Hash the raw encoded integers directly\n        if self._hash is None: self._hash = hash(memoryview(self._data))\n        return self._hash\n\n    def __add__(self, other: 'Seq') -&gt; 'Seq':\n        if self._alphabet is not other._alphabet:\n            raise ValueError(\"Cannot concatenate sequences with different alphabets\")\n        # Fast Int concatenation\n        return self._alphabet.seq_from(np.concatenate((self._data, other._data), axis=0))\n\n    def __mul__(self, other: int) -&gt; 'Seq':\n        if not isinstance(other, int): return NotImplemented\n        if other &lt;= 0: return self._alphabet.empty_seq()\n        return self._alphabet.seq_from(np.tile(self._data, other))\n\n    def __rmul__(self, other: int) -&gt; 'Seq': return self.__mul__(other)\n    def __bool__(self): return len(self._data) &gt; 0\n\n    # Comparisons (Lexicographical)\n    # We must decode to bytes because internal integer order (e.g. T=0, A=2)\n    # might not match alphabetical order (A &lt; T).\n    def __lt__(self, other):\n        if not isinstance(other, Seq): return NotImplemented\n        return bytes(self) &lt; bytes(other)\n\n    def __le__(self, other):\n        if not isinstance(other, Seq): return NotImplemented\n        return bytes(self) &lt;= bytes(other)\n\n    def __gt__(self, other):\n        if not isinstance(other, Seq): return NotImplemented\n        return bytes(self) &gt; bytes(other)\n\n    def __ge__(self, other):\n        if not isinstance(other, Seq): return NotImplemented\n        return bytes(self) &gt;= bytes(other)\n\n    def __getitem__(self, item: Union[slice, int, Interval]) -&gt; 'Seq':\n        \"\"\"Extracts a subsequence by index, slice, or ``Interval``.\n\n        When an ``Interval`` with ``strand == -1`` is used, the result is\n        automatically reverse-complemented.\n\n        Args:\n            item: An integer index, a Python slice, or an ``Interval``.\n\n        Returns:\n            A new ``Seq`` representing the subsequence.\n\n        Examples:\n            &gt;&gt;&gt; seq = Alphabet.DNA.seq(b'ATGCGA')\n            &gt;&gt;&gt; seq[1:4]\n            TGC\n            &gt;&gt;&gt; seq[Interval(1, 4, -1)]  # reverse complement\n            GCA\n        \"\"\"\n        # 1. Standard Slicing (Fastest)\n        if isinstance(item, slice):\n            # Numpy handles the slicing logic/views\n            return self._alphabet.seq_from(self._data[item])\n\n        # 2. Integer Access\n        if isinstance(item, int): return self._alphabet.seq_from(self._data[item:item + 1])\n\n        # 3. Interval Object\n        # Only import Interval overhead if needed\n        item = Interval.from_item(item, length=len(self))\n        chunk_encoded = self._data[item.start:item.end]\n\n        if item.strand == -1:\n            # Resolve RC. If Alphabet supports int-based RC, usage is cleaner.\n            # Optimization: Use cached RC table directly if available\n            if self._alphabet.complement is not None:\n                # [::-1] creates a view, fancy indexing creates a copy\n                rc_data = self._alphabet.complement[chunk_encoded[::-1]]\n                return self._alphabet.seq_from(rc_data)\n\n            # For now, we assume we must decode-&gt;trans-&gt;encode or use a cached RC table\n            # Ideally: return self._alphabet.core(encoded=self._alphabet.rc_code(chunk_encoded))\n            return self._alphabet.reverse_complement(self._alphabet.seq_from(chunk_encoded))\n\n        return self._alphabet.seq_from(chunk_encoded)\n\n    def generate_id(self, digest_size: int = 8) -&gt; bytes:\n        \"\"\"Generates a deterministic ID from the sequence content using BLAKE2b.\n\n        Args:\n            digest_size: Hash digest size in bytes (output is ``2 * digest_size``\n                hex characters).\n\n        Returns:\n            The hex digest as bytes.\n\n        Examples:\n            &gt;&gt;&gt; seq.generate_id()\n            b'a1b2c3d4e5f6a7b8'\n        \"\"\"\n        return hexlify(blake2b(self._data.tobytes(), digest_size=digest_size, usedforsecurity=False).digest())\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq.alphabet","title":"<code>alphabet</code>  <code>property</code>","text":"<p>Returns the alphabet used for encoding/decoding.</p> <p>Returns:</p> Type Description <code>Alphabet</code> <p>The owning <code>Alphabet</code> singleton.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq.batch","title":"<code>batch</code>  <code>property</code>","text":"<p>Returns the batch type for this class.</p> <p>Returns:</p> Type Description <code>type[Batch]</code> <p>The <code>SeqBatch</code> class.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq.encoded","title":"<code>encoded</code>  <code>property</code>","text":"<p>Returns the underlying encoded integer array (zero-copy).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A read-only <code>uint8</code> numpy array.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq.__array__","title":"<code>__array__(dtype=None)</code>","text":"<p>Allows the Seq to be treated as a numpy array.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>def __array__(self, dtype=None):\n    \"\"\"Allows the Seq to be treated as a numpy array.\"\"\"\n    return self._data.astype(dtype, copy=False) if dtype else self._data\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq.__getitem__","title":"<code>__getitem__(item)</code>","text":"<p>Extracts a subsequence by index, slice, or <code>Interval</code>.</p> <p>When an <code>Interval</code> with <code>strand == -1</code> is used, the result is automatically reverse-complemented.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[slice, int, Interval]</code> <p>An integer index, a Python slice, or an <code>Interval</code>.</p> required <p>Returns:</p> Type Description <code>Seq</code> <p>A new <code>Seq</code> representing the subsequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq = Alphabet.DNA.seq(b'ATGCGA')\n&gt;&gt;&gt; seq[1:4]\nTGC\n&gt;&gt;&gt; seq[Interval(1, 4, -1)]  # reverse complement\nGCA\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>def __getitem__(self, item: Union[slice, int, Interval]) -&gt; 'Seq':\n    \"\"\"Extracts a subsequence by index, slice, or ``Interval``.\n\n    When an ``Interval`` with ``strand == -1`` is used, the result is\n    automatically reverse-complemented.\n\n    Args:\n        item: An integer index, a Python slice, or an ``Interval``.\n\n    Returns:\n        A new ``Seq`` representing the subsequence.\n\n    Examples:\n        &gt;&gt;&gt; seq = Alphabet.DNA.seq(b'ATGCGA')\n        &gt;&gt;&gt; seq[1:4]\n        TGC\n        &gt;&gt;&gt; seq[Interval(1, 4, -1)]  # reverse complement\n        GCA\n    \"\"\"\n    # 1. Standard Slicing (Fastest)\n    if isinstance(item, slice):\n        # Numpy handles the slicing logic/views\n        return self._alphabet.seq_from(self._data[item])\n\n    # 2. Integer Access\n    if isinstance(item, int): return self._alphabet.seq_from(self._data[item:item + 1])\n\n    # 3. Interval Object\n    # Only import Interval overhead if needed\n    item = Interval.from_item(item, length=len(self))\n    chunk_encoded = self._data[item.start:item.end]\n\n    if item.strand == -1:\n        # Resolve RC. If Alphabet supports int-based RC, usage is cleaner.\n        # Optimization: Use cached RC table directly if available\n        if self._alphabet.complement is not None:\n            # [::-1] creates a view, fancy indexing creates a copy\n            rc_data = self._alphabet.complement[chunk_encoded[::-1]]\n            return self._alphabet.seq_from(rc_data)\n\n        # For now, we assume we must decode-&gt;trans-&gt;encode or use a cached RC table\n        # Ideally: return self._alphabet.core(encoded=self._alphabet.rc_code(chunk_encoded))\n        return self._alphabet.reverse_complement(self._alphabet.seq_from(chunk_encoded))\n\n    return self._alphabet.seq_from(chunk_encoded)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq.generate_id","title":"<code>generate_id(digest_size=8)</code>","text":"<p>Generates a deterministic ID from the sequence content using BLAKE2b.</p> <p>Parameters:</p> Name Type Description Default <code>digest_size</code> <code>int</code> <p>Hash digest size in bytes (output is <code>2 * digest_size</code> hex characters).</p> <code>8</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The hex digest as bytes.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq.generate_id()\nb'a1b2c3d4e5f6a7b8'\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>def generate_id(self, digest_size: int = 8) -&gt; bytes:\n    \"\"\"Generates a deterministic ID from the sequence content using BLAKE2b.\n\n    Args:\n        digest_size: Hash digest size in bytes (output is ``2 * digest_size``\n            hex characters).\n\n    Returns:\n        The hex digest as bytes.\n\n    Examples:\n        &gt;&gt;&gt; seq.generate_id()\n        b'a1b2c3d4e5f6a7b8'\n    \"\"\"\n    return hexlify(blake2b(self._data.tobytes(), digest_size=digest_size, usedforsecurity=False).digest())\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.Seq.tobytes","title":"<code>tobytes()</code>","text":"<p>Decodes the sequence to raw bytes.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>The decoded byte string.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; seq.tobytes()\nb'ATGCGA'\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>def tobytes(self) -&gt; bytes:\n    \"\"\"Decodes the sequence to raw bytes.\n\n    Returns:\n        The decoded byte string.\n\n    Examples:\n        &gt;&gt;&gt; seq.tobytes()\n        b'ATGCGA'\n    \"\"\"\n    return self.__bytes__()\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch","title":"<code>SeqBatch</code>","text":"<p>               Bases: <code>Batch</code>, <code>HasAlphabet</code></p> <p>Flattened batch of sequences for Numba-accelerated parallel processing.</p> <p>Stores all encoded symbols in a single contiguous <code>uint8</code> array with per-sequence start/length metadata, enabling zero-copy slicing and parallel kernels.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>Contiguous <code>uint8</code> array of all encoded symbols.</p> required <code>starts</code> <code>ndarray</code> <p><code>int32</code> array of per-sequence start offsets into data.</p> required <code>lengths</code> <code>ndarray</code> <p><code>int32</code> array of per-sequence lengths.</p> required <code>alphabet</code> <code>Alphabet</code> <p>The shared <code>Alphabet</code>.</p> required <code>_validation_token</code> <code>object</code> <p>Internal token (must be the alphabet).</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = Alphabet.DNA.batch_from([seq1, seq2, seq3])\n&gt;&gt;&gt; len(batch)\n3\n&gt;&gt;&gt; batch[0]\nATGCGA\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>class SeqBatch(Batch, HasAlphabet):\n    \"\"\"\n    Flattened batch of sequences for Numba-accelerated parallel processing.\n\n    Stores all encoded symbols in a single contiguous ``uint8`` array with\n    per-sequence start/length metadata, enabling zero-copy slicing and\n    parallel kernels.\n\n    Args:\n        data: Contiguous ``uint8`` array of all encoded symbols.\n        starts: ``int32`` array of per-sequence start offsets into *data*.\n        lengths: ``int32`` array of per-sequence lengths.\n        alphabet: The shared ``Alphabet``.\n        _validation_token: Internal token (must be the alphabet).\n\n    Examples:\n        &gt;&gt;&gt; batch = Alphabet.DNA.batch_from([seq1, seq2, seq3])\n        &gt;&gt;&gt; len(batch)\n        3\n        &gt;&gt;&gt; batch[0]\n        ATGCGA\n    \"\"\"\n    __slots__ = ('_alphabet', '_data', '_starts', '_lengths', '_count')\n    def __init__(self, data: np.ndarray, starts: np.ndarray, lengths: np.ndarray, \n                 alphabet: 'Alphabet', _validation_token: object = None):\n        if _validation_token is not alphabet:\n            raise PermissionError(\"SeqBatch objects must be created via class methods or an Alphabet\")\n        self._data = data\n        self._starts = starts\n        self._lengths = lengths\n        self._alphabet = alphabet\n        self._count = len(starts)\n\n        # Lock arrays for safety\n        self._data.flags.writeable = False\n        self._starts.flags.writeable = False\n        self._lengths.flags.writeable = False\n\n    @classmethod\n    def build(cls, seqs: Iterable['Seq']) -&gt; 'SeqBatch':\n        \"\"\"Creates a SeqBatch from an iterable of Seq objects.\n\n        Infers the alphabet from the first sequence.\n\n        Args:\n            seqs: An iterable of ``Seq`` objects (must share the same alphabet).\n\n        Returns:\n            A new ``SeqBatch``.\n\n        Raises:\n            ValueError: If the iterable is empty (use ``Alphabet.empty_batch()``).\n\n        Examples:\n            &gt;&gt;&gt; batch = SeqBatch.build([seq1, seq2])\n            &gt;&gt;&gt; len(batch)\n            2\n        \"\"\"\n        seqs_list = list(seqs)\n        if not seqs_list:\n             raise ValueError(\"Cannot create SeqBatch from empty sequence list. Use Alphabet.empty_batch() instead.\")\n        return seqs_list[0].alphabet.batch_from(seqs_list)\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'SeqBatch':\n        \"\"\"Not supported directly \u2014 use ``Alphabet.zeros_batch(n)`` instead.\n\n        Raises:\n            TypeError: Always.\n        \"\"\"\n        raise TypeError(\"SeqBatch.zeros() requires an Alphabet. Use Alphabet.zeros_batch(n) instead.\")\n\n    @classmethod\n    def empty(cls) -&gt; 'SeqBatch':\n        \"\"\"Not supported directly \u2014 use ``Alphabet.empty_batch()`` instead.\n\n        Raises:\n            TypeError: Always.\n        \"\"\"\n        raise TypeError(\"SeqBatch.empty() requires an Alphabet. Use Alphabet.empty_batch() instead.\")\n\n    @classmethod\n    def concat(cls, batches: Iterable['SeqBatch']) -&gt; 'SeqBatch':\n        \"\"\"Concatenates multiple SeqBatch objects into one.\n\n        All batches must share the same alphabet.\n\n        Args:\n            batches: An iterable of ``SeqBatch`` objects.\n\n        Returns:\n            A single concatenated ``SeqBatch``.\n\n        Raises:\n            ValueError: If the list is empty or alphabets differ.\n\n        Examples:\n            &gt;&gt;&gt; combined = SeqBatch.concat([batch_a, batch_b])\n        \"\"\"\n        batches = list(batches)\n        if not batches: raise ValueError(\"Cannot concatenate empty list of batches\")\n\n        # Validate alphabet consistency\n        alphabet = batches[0].alphabet\n        for b in batches[1:]:\n            if b.alphabet != alphabet: raise ValueError(\"All batches must share the same alphabet\")\n\n        # Merge arrays\n        data = np.concatenate([b._data for b in batches])\n        lengths = np.concatenate([b._lengths for b in batches])\n\n        starts = np.zeros(len(lengths), dtype=np.int32)\n        if len(lengths) &gt; 1: np.cumsum(lengths[:-1], out=starts[1:])\n        return cls(data, starts, lengths, alphabet, _validation_token=alphabet)\n\n    def __add__(self, other: 'SeqBatch') -&gt; 'SeqBatch':\n        \"\"\"Concatenates two batches via the ``+`` operator.\"\"\"\n        return self.concat([self, other])\n\n    # --- Numba Accessors ---\n    # Properties to unpack into Numba function arguments: *batch.arrays\n    @property\n    def arrays(self):\n        \"\"\"Returns ``(data, starts, lengths)`` for Numba kernel unpacking.\n\n        Returns:\n            A 3-tuple of numpy arrays.\n        \"\"\"\n        return self._data, self._starts, self._lengths\n\n    @property\n    def component(self):\n        \"\"\"Returns the scalar type represented by this batch.\n\n        Returns:\n            The ``Seq`` class.\n        \"\"\"\n        return Seq\n\n    @property\n    def alphabet(self) -&gt; 'Alphabet':\n        \"\"\"Returns the shared alphabet.\n\n        Returns:\n            The ``Alphabet`` singleton.\n        \"\"\"\n        return self._alphabet\n\n    @property\n    def encoded(self) -&gt; np.ndarray:\n        \"\"\"Returns the flat encoded data array (zero-copy).\n\n        Returns:\n            A read-only ``uint8`` numpy array.\n        \"\"\"\n        return self._data\n\n    @property\n    def starts(self) -&gt; np.ndarray:\n        \"\"\"Returns the per-sequence start offsets.\n\n        Returns:\n            An ``int32`` numpy array.\n        \"\"\"\n        return self._starts\n\n    @property\n    def lengths(self) -&gt; np.ndarray:\n        \"\"\"Returns the per-sequence lengths.\n\n        Returns:\n            An ``int32`` numpy array.\n        \"\"\"\n        return self._lengths\n\n    @property\n    def nbytes(self) -&gt; int:\n        \"\"\"Returns the total memory usage in bytes.\n\n        Returns:\n            Total bytes consumed by data, starts, and lengths arrays.\n        \"\"\"\n        return self._data.nbytes + self._starts.nbytes + self._lengths.nbytes\n\n    def copy(self) -&gt; 'SeqBatch':\n        \"\"\"Returns a deep copy of this batch.\n\n        Returns:\n            A new ``SeqBatch`` with copied arrays.\n        \"\"\"\n        return self.__class__(self._data.copy(), self._starts.copy(), self._lengths.copy(), self._alphabet, _validation_token=self._alphabet)\n\n    def empty(self) -&gt; 'SeqBatch':\n        \"\"\"Returns an empty batch with the same alphabet.\n\n        Returns:\n            An empty ``SeqBatch``.\n        \"\"\"\n        return self._alphabet.empty()\n\n    def __repr__(self): return f\"&lt;SeqBatch: {len(self)} sequences&gt;\"\n\n    def __eq__(self, other):\n        if self is other: return True\n        if not isinstance(other, SeqBatch): return False\n        if self._alphabet is not other._alphabet: return False\n        if len(self) != len(other): return False\n        return (np.array_equal(self._lengths, other._lengths) and\n                np.array_equal(self._starts, other._starts) and\n                np.array_equal(self._data, other._data))\n\n    def __len__(self): return self._count\n    def __getitem__(self, idx):\n        if isinstance(idx, (int, np.integer)):\n            if idx &lt; 0: idx += self._count\n            if not 0 &lt;= idx &lt; self._count: raise IndexError(\"SeqBatch index out of range\")\n            start = self._starts[idx]\n            length = self._lengths[idx]\n            return self._alphabet.seq_from(self._data[start:start + length])\n        elif isinstance(idx, (slice, np.ndarray, list)):\n            # Optimized slicing: Gather arrays directly without creating Seq objects\n            if isinstance(idx, slice):\n                start, stop, step = idx.indices(self._count)\n                if step == 1:\n                    # Contiguous slice (Fastest)\n                    new_count = max(0, stop - start)\n                    if new_count == 0: return self._alphabet.empty()\n\n                    s_start = self._starts[start]\n                    s_end = self._starts[stop] if stop &lt; self._count else len(self._data)\n\n                    new_lengths = self._lengths[start:stop]\n\n                    # Check for physical contiguity to allow zero-copy slicing\n                    # If deduplicated, s_end - s_start != sum(lengths)\n                    if (s_end - s_start) == new_lengths.sum():\n                        new_data = self._data[s_start:s_end]\n                        new_starts = self._starts[start:stop] - s_start\n                        return self._alphabet.new_batch(new_data, new_starts, new_lengths)\n\n                # Non-contiguous slice -&gt; Convert to array indices\n                indices = np.arange(start, stop, step)\n            else:\n                # Array/List indices\n                indices = np.asanyarray(idx)\n                if indices.dtype == bool:\n                    indices = np.flatnonzero(indices)\n\n            return self._gather(indices)\n\n        raise TypeError(f\"Invalid index type: {type(idx)}\")\n\n    def __iter__(self) -&gt; Generator[Seq, None, None]:\n        for i in range(self._count):\n            start = self._starts[i]\n            length = self._lengths[i]\n            yield self._alphabet.seq_from(self._data[start:start + length])\n\n    def generate_id(self, digest_size: int = 8) -&gt; bytes:\n        \"\"\"Generates a deterministic ID for the entire batch using BLAKE2b.\n\n        Args:\n            digest_size: Hash digest size in bytes.\n\n        Returns:\n            The hex digest as bytes.\n        \"\"\"\n        return hexlify(blake2b(self._data.tobytes(), digest_size=digest_size, usedforsecurity=False).digest())\n\n    def generate_ids(self, digest_size: int = 8) -&gt; np.ndarray:\n        \"\"\"Generates a deterministic ID for each sequence in the batch.\n\n        Args:\n            digest_size: Hash digest size in bytes per ID.\n\n        Returns:\n            An object array of hex digest bytes, one per sequence.\n\n        Examples:\n            &gt;&gt;&gt; ids = batch.generate_ids()\n            &gt;&gt;&gt; ids[0]\n            b'a1b2c3d4e5f6a7b8'\n        \"\"\"\n        ids = np.empty(self._count, dtype=object)\n        for i in range(self._count):\n            start = self._starts[i]\n            length = self._lengths[i]\n            ids[i] = hexlify(blake2b(self._data[start:start + length], digest_size=digest_size, usedforsecurity=False).digest())\n        return ids\n\n    def _gather(self, indices: np.ndarray) -&gt; 'SeqBatch':\n        \"\"\"Internal method to gather sequences by index array.\n\n        Args:\n            indices: Integer array of sequence indices to gather.\n\n        Returns:\n            A new ``SeqBatch`` containing only the selected sequences.\n        \"\"\"\n        if len(indices) == 0: return self._alphabet.empty()\n\n        new_lengths = self._lengths[indices]\n        total_len = new_lengths.sum()\n\n        new_data = np.empty(total_len, dtype=np.uint8)\n        new_starts = np.zeros(len(indices), dtype=np.int32)\n        if len(indices) &gt; 1:\n            np.cumsum(new_lengths[:-1], out=new_starts[1:])\n\n        _batch_gather_kernel(self._data, self._starts, self._lengths, indices, new_data, new_starts)\n        return self._alphabet.new_batch(new_data, new_starts, new_lengths)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.alphabet","title":"<code>alphabet</code>  <code>property</code>","text":"<p>Returns the shared alphabet.</p> <p>Returns:</p> Type Description <code>Alphabet</code> <p>The <code>Alphabet</code> singleton.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.arrays","title":"<code>arrays</code>  <code>property</code>","text":"<p>Returns <code>(data, starts, lengths)</code> for Numba kernel unpacking.</p> <p>Returns:</p> Type Description <p>A 3-tuple of numpy arrays.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.component","title":"<code>component</code>  <code>property</code>","text":"<p>Returns the scalar type represented by this batch.</p> <p>Returns:</p> Type Description <p>The <code>Seq</code> class.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.encoded","title":"<code>encoded</code>  <code>property</code>","text":"<p>Returns the flat encoded data array (zero-copy).</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>A read-only <code>uint8</code> numpy array.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>Returns the per-sequence lengths.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An <code>int32</code> numpy array.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.nbytes","title":"<code>nbytes</code>  <code>property</code>","text":"<p>Returns the total memory usage in bytes.</p> <p>Returns:</p> Type Description <code>int</code> <p>Total bytes consumed by data, starts, and lengths arrays.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>Returns the per-sequence start offsets.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>An <code>int32</code> numpy array.</p>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.__add__","title":"<code>__add__(other)</code>","text":"<p>Concatenates two batches via the <code>+</code> operator.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>def __add__(self, other: 'SeqBatch') -&gt; 'SeqBatch':\n    \"\"\"Concatenates two batches via the ``+`` operator.\"\"\"\n    return self.concat([self, other])\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.build","title":"<code>build(seqs)</code>  <code>classmethod</code>","text":"<p>Creates a SeqBatch from an iterable of Seq objects.</p> <p>Infers the alphabet from the first sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seqs</code> <code>Iterable[Seq]</code> <p>An iterable of <code>Seq</code> objects (must share the same alphabet).</p> required <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A new <code>SeqBatch</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the iterable is empty (use <code>Alphabet.empty_batch()</code>).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; batch = SeqBatch.build([seq1, seq2])\n&gt;&gt;&gt; len(batch)\n2\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>@classmethod\ndef build(cls, seqs: Iterable['Seq']) -&gt; 'SeqBatch':\n    \"\"\"Creates a SeqBatch from an iterable of Seq objects.\n\n    Infers the alphabet from the first sequence.\n\n    Args:\n        seqs: An iterable of ``Seq`` objects (must share the same alphabet).\n\n    Returns:\n        A new ``SeqBatch``.\n\n    Raises:\n        ValueError: If the iterable is empty (use ``Alphabet.empty_batch()``).\n\n    Examples:\n        &gt;&gt;&gt; batch = SeqBatch.build([seq1, seq2])\n        &gt;&gt;&gt; len(batch)\n        2\n    \"\"\"\n    seqs_list = list(seqs)\n    if not seqs_list:\n         raise ValueError(\"Cannot create SeqBatch from empty sequence list. Use Alphabet.empty_batch() instead.\")\n    return seqs_list[0].alphabet.batch_from(seqs_list)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple SeqBatch objects into one.</p> <p>All batches must share the same alphabet.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[SeqBatch]</code> <p>An iterable of <code>SeqBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A single concatenated <code>SeqBatch</code>.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the list is empty or alphabets differ.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combined = SeqBatch.concat([batch_a, batch_b])\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['SeqBatch']) -&gt; 'SeqBatch':\n    \"\"\"Concatenates multiple SeqBatch objects into one.\n\n    All batches must share the same alphabet.\n\n    Args:\n        batches: An iterable of ``SeqBatch`` objects.\n\n    Returns:\n        A single concatenated ``SeqBatch``.\n\n    Raises:\n        ValueError: If the list is empty or alphabets differ.\n\n    Examples:\n        &gt;&gt;&gt; combined = SeqBatch.concat([batch_a, batch_b])\n    \"\"\"\n    batches = list(batches)\n    if not batches: raise ValueError(\"Cannot concatenate empty list of batches\")\n\n    # Validate alphabet consistency\n    alphabet = batches[0].alphabet\n    for b in batches[1:]:\n        if b.alphabet != alphabet: raise ValueError(\"All batches must share the same alphabet\")\n\n    # Merge arrays\n    data = np.concatenate([b._data for b in batches])\n    lengths = np.concatenate([b._lengths for b in batches])\n\n    starts = np.zeros(len(lengths), dtype=np.int32)\n    if len(lengths) &gt; 1: np.cumsum(lengths[:-1], out=starts[1:])\n    return cls(data, starts, lengths, alphabet, _validation_token=alphabet)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of this batch.</p> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A new <code>SeqBatch</code> with copied arrays.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>def copy(self) -&gt; 'SeqBatch':\n    \"\"\"Returns a deep copy of this batch.\n\n    Returns:\n        A new ``SeqBatch`` with copied arrays.\n    \"\"\"\n    return self.__class__(self._data.copy(), self._starts.copy(), self._lengths.copy(), self._alphabet, _validation_token=self._alphabet)\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.empty","title":"<code>empty()</code>","text":"<p>Returns an empty batch with the same alphabet.</p> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>An empty <code>SeqBatch</code>.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>def empty(self) -&gt; 'SeqBatch':\n    \"\"\"Returns an empty batch with the same alphabet.\n\n    Returns:\n        An empty ``SeqBatch``.\n    \"\"\"\n    return self._alphabet.empty()\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.generate_id","title":"<code>generate_id(digest_size=8)</code>","text":"<p>Generates a deterministic ID for the entire batch using BLAKE2b.</p> <p>Parameters:</p> Name Type Description Default <code>digest_size</code> <code>int</code> <p>Hash digest size in bytes.</p> <code>8</code> <p>Returns:</p> Type Description <code>bytes</code> <p>The hex digest as bytes.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>def generate_id(self, digest_size: int = 8) -&gt; bytes:\n    \"\"\"Generates a deterministic ID for the entire batch using BLAKE2b.\n\n    Args:\n        digest_size: Hash digest size in bytes.\n\n    Returns:\n        The hex digest as bytes.\n    \"\"\"\n    return hexlify(blake2b(self._data.tobytes(), digest_size=digest_size, usedforsecurity=False).digest())\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.generate_ids","title":"<code>generate_ids(digest_size=8)</code>","text":"<p>Generates a deterministic ID for each sequence in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>digest_size</code> <code>int</code> <p>Hash digest size in bytes per ID.</p> <code>8</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>An object array of hex digest bytes, one per sequence.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; ids = batch.generate_ids()\n&gt;&gt;&gt; ids[0]\nb'a1b2c3d4e5f6a7b8'\n</code></pre> Source code in <code>baclib/containers/seq.py</code> <pre><code>def generate_ids(self, digest_size: int = 8) -&gt; np.ndarray:\n    \"\"\"Generates a deterministic ID for each sequence in the batch.\n\n    Args:\n        digest_size: Hash digest size in bytes per ID.\n\n    Returns:\n        An object array of hex digest bytes, one per sequence.\n\n    Examples:\n        &gt;&gt;&gt; ids = batch.generate_ids()\n        &gt;&gt;&gt; ids[0]\n        b'a1b2c3d4e5f6a7b8'\n    \"\"\"\n    ids = np.empty(self._count, dtype=object)\n    for i in range(self._count):\n        start = self._starts[i]\n        length = self._lengths[i]\n        ids[i] = hexlify(blake2b(self._data[start:start + length], digest_size=digest_size, usedforsecurity=False).digest())\n    return ids\n</code></pre>"},{"location":"reference/baclib/containers/seq/#baclib.containers.seq.SeqBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Not supported directly \u2014 use <code>Alphabet.zeros_batch(n)</code> instead.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>Always.</p> Source code in <code>baclib/containers/seq.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'SeqBatch':\n    \"\"\"Not supported directly \u2014 use ``Alphabet.zeros_batch(n)`` instead.\n\n    Raises:\n        TypeError: Always.\n    \"\"\"\n    raise TypeError(\"SeqBatch.zeros() requires an Alphabet. Use Alphabet.zeros_batch(n) instead.\")\n</code></pre>"},{"location":"reference/baclib/core/","title":"core","text":""},{"location":"reference/baclib/core/#baclib.core","title":"<code>baclib.core</code>","text":"<p>Core data types: alphabets for biological sequences and genomic interval arithmetic.</p>"},{"location":"reference/baclib/core/alphabet/","title":"alphabet","text":""},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet","title":"<code>baclib.core.alphabet</code>","text":"<p>Module for representing ASCII biological alphabets</p>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet","title":"<code>Alphabet</code>","text":"<p>A class to represent an alphabet of ASCII symbols.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>class Alphabet:\n    \"\"\"\n    A class to represent an alphabet of ASCII symbols.\n    \"\"\"\n    __slots__ = ('_data', '_lookup_table', '_complement', '_trans_table', '_delete_bytes', '_decode_table')\n    DTYPE: Final = np.uint8\n    INVALID: Final = np.iinfo(DTYPE).max\n    MAX_LEN: Final = INVALID + 1\n    ENCODING: Final = 'ascii'\n\n    DNA: ClassVar['Alphabet']\n    RNA: ClassVar['Alphabet']\n    AMINO: ClassVar['Alphabet']\n    MURPHY_10: ClassVar['Alphabet']\n\n    def __init__(self, symbols: bytes, complement: bytes = None, aliases: dict[bytes, bytes] = None):\n        \"\"\"\n        Initializes an Alphabet.\n\n        Args:\n            symbols: The symbols in the alphabet as bytes.\n            complement: Optional complement symbols as bytes. Must be same length as symbols.\n            aliases: Optional mapping of invalid characters to valid ones (e.g. {b'N': b'A'}).\n\n        Raises:\n            AlphabetError: If symbols are not ASCII, too long, contain duplicates, or if complement is invalid.\n        \"\"\"\n        if not symbols.isascii(): raise AlphabetError('Alphabet symbols must be a valid ASCII string')\n        if len(symbols) &gt; self.MAX_LEN:\n            raise AlphabetError(f'Alphabet size cannot exceed {self.MAX_LEN} symbols ({self.DTYPE})')\n        if len(set(symbols.upper())) != len(symbols): raise AlphabetError('Alphabet contains duplicate symbols')\n\n        self._data: np.ndarray = np.frombuffer(symbols, dtype=self.DTYPE)\n\n        # Build Lookup Table\n        self._lookup_table = np.full(self.MAX_LEN, self.INVALID, dtype=self.DTYPE)\n        indices = np.arange(len(symbols), dtype=self.DTYPE)\n        self._lookup_table[np.frombuffer(symbols, dtype=self.DTYPE)] = indices\n        self._lookup_table[np.frombuffer(symbols.lower(), dtype=self.DTYPE)] = indices\n\n        # Apply Aliases (Map invalid chars to valid indices)\n        if aliases:\n            for src, dst in aliases.items():\n                if len(src) != 1 or len(dst) != 1: raise AlphabetError(\"Aliases must be single bytes\")\n\n                # Resolve destination index\n                dst_idx = self._lookup_table[ord(dst)]\n                if dst_idx == self.INVALID: raise AlphabetError(f\"Alias target {dst} not in alphabet\")\n\n                # Map source (both cases)\n                self._lookup_table[ord(src)] = dst_idx\n                self._lookup_table[ord(src.lower())] = dst_idx\n\n        # Build Translation Tables\n        self._trans_table = self._lookup_table.tobytes()\n        self._delete_bytes = np.where(self._lookup_table == self.INVALID)[0].astype(self.DTYPE).tobytes()\n\n        # Build Decode Table (for fast tobytes)\n        decode_map = np.zeros(256, dtype=self.DTYPE)\n        decode_map[:len(self._data)] = self._data\n        self._decode_table = decode_map.tobytes()\n\n        self._complement = None\n        if complement is not None:\n            if len(complement) != len(symbols):\n                raise AlphabetError(\"Complement must be the same length as symbols\")\n            comp_indices = self._lookup_table[np.frombuffer(complement, dtype=self.DTYPE)]\n            if np.any(comp_indices == self.INVALID):\n                raise AlphabetError(\"Complement contains symbols not in alphabet\")\n            self._complement = comp_indices\n\n    def __len__(self):\n        return len(self._data)\n\n    def __contains__(self, item):\n        # Fast O(1) lookup using the table\n        try:\n            if isinstance(item, (int, np.integer)):\n                return self._lookup_table[item] != self.INVALID\n            if isinstance(item, (str, bytes)):\n                if len(item) != 1: return False\n                val = ord(item) if isinstance(item, str) else item[0]\n                return self._lookup_table[val] != self.INVALID\n        except (IndexError, ValueError, TypeError):\n            pass\n        return False\n\n    def __iter__(self):\n        return iter(self._data)\n\n    def __getitem__(self, item):\n        return self._data[item]\n\n    def __array__(self, dtype=None):\n        return self._data.astype(dtype, copy=False) if dtype else self._data\n\n    def __repr__(self):\n        return repr(self._data)\n\n    def __eq__(self, other):\n        if self is other: return True\n        if not isinstance(other, Alphabet): return False\n        return np.array_equal(self._data, other._data)\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __hash__(self):\n        return hash(self._data.tobytes())\n\n    @property\n    def bits_per_symbol(self) -&gt; int:\n        \"\"\"Returns the number of bits required to represent a symbol in this alphabet.\n\n        Returns:\n            The number of bits (integer).\n        \"\"\"\n        return (len(self._data) - 1).bit_length()\n\n    @property\n    def complement(self):\n        \"\"\"Returns the complement lookup table if available.\"\"\"\n        return self._complement\n\n    @classmethod\n    def detect(cls, text: bytes) -&gt; 'Alphabet':\n        \"\"\"\n        Detects the most likely alphabet for the given byte string.\n\n        Candidates are checked in the following priority order:\n        1. DNA\n        2. RNA\n        3. AMINO\n        4. MURPHY_10\n\n        Scoring is based on:\n        1. Canonical Count: Number of characters strictly in the alphabet definition (case-insensitive).\n        2. Valid Count: Number of characters valid in the alphabet (including aliases).\n\n        Args:\n            text: The input byte string (or ASCII string).\n\n        Returns:\n            The most likely Alphabet singleton.\n        \"\"\"\n        if isinstance(text, str):\n            text = text.encode(cls.ENCODING)\n\n        data = np.frombuffer(text, dtype=cls.DTYPE)\n\n        # Optimization: Scan data once to get character counts\n        # This reduces complexity from O(K*N) to O(N + K*C) where K=num_alphabets, C=256\n        counts = np.bincount(data, minlength=cls.MAX_LEN)\n\n        # Candidate alphabets in priority order\n        candidates = [cls.DNA, cls.RNA, cls.AMINO, cls.MURPHY_10]\n\n        best_alpha = candidates[0]\n        best_score = (-1, -1)\n\n        for alpha in candidates:\n            # 1. Canonical Score\n            # Create mask for canonical symbols (both cases)\n            is_canonical = np.zeros(cls.MAX_LEN, dtype=bool)\n            is_canonical[alpha._data] = True\n\n            # Handle lowercase\n            lower_indices = np.frombuffer(alpha._data.tobytes().lower(), dtype=cls.DTYPE)\n            is_canonical[lower_indices] = True\n\n            # Sum counts of canonical characters\n            n_canonical = counts[is_canonical].sum()\n\n            # 2. Valid Score\n            # Use lookup table - anything not INVALID is valid\n            is_valid = (alpha._lookup_table != cls.INVALID)\n            n_valid = counts[is_valid].sum()\n\n            score = (n_canonical, n_valid)\n\n            if score &gt; best_score:\n                best_score = score\n                best_alpha = alpha\n\n        return best_alpha\n\n    def masker(self, k: int) -&gt; tuple[int, int, np.dtype]:\n        \"\"\"\n        Returns (bits_per_symbol, bit_mask, dtype) for a specific K.\n\n        Args:\n            k: The k-mer length.\n\n        Returns:\n            A tuple of (bits_per_symbol, bit_mask, dtype).\n\n        Raises:\n            ValueError: If K is too large for 64-bit hashing.\n        \"\"\"\n        bps = self.bits_per_symbol\n        total_bits = k * bps\n        if total_bits &lt;= 32:\n            dtype = np.uint32\n        elif total_bits &lt;= 64:\n            dtype = np.uint64\n        else:\n            raise ValueError(f\"K={k} is too large for 64-bit hashing with {self}\")\n        mask = (1 &lt;&lt; (bps * (k - 1))) - 1\n        return bps, mask, dtype\n\n    def encode(self, text: bytes) -&gt; np.ndarray:\n        \"\"\"\n        Zero-copy encoding from Byte String to Array.\n\n        Args:\n            text: The text to encode as bytes.\n\n        Returns:\n            A numpy array of encoded indices.\n        \"\"\"\n        return np.frombuffer(text.translate(self._trans_table, delete=self._delete_bytes), dtype=self.DTYPE)\n\n    def decode(self, encoded: np.ndarray) -&gt; bytes:\n        \"\"\"Decodes an array of indices back to bytes.\n\n        Args:\n            encoded: The numpy array of indices (uint8).\n\n        Returns:\n            The decoded bytes string.\n        \"\"\"\n        # Ensure uint8 for byte-wise translation\n        if encoded.dtype != self.DTYPE:\n            encoded = encoded.astype(self.DTYPE, copy=False)\n        return encoded.tobytes().translate(self._decode_table)\n\n    def entropy(self, seq: Union['Seq', 'SeqBatch']) -&gt; Union[float, np.ndarray]:\n        \"\"\"\n        Calculates the Shannon entropy of a sequence or batch.\n        H = -sum(p_i * log2(p_i))\n\n        Args:\n            seq: A Seq or SeqBatch object.\n\n        Returns:\n            The entropy in bits (float for Seq, ndarray for SeqBatch).\n        \"\"\"\n        if seq.alphabet != self:\n            raise ValueError(f\"Seq alphabet {seq.alphabet} does not match Alphabet {self}\")\n\n        if isinstance(seq, SeqBatch):\n            data, starts, lengths = seq.arrays\n            return _batch_entropy_kernel(data, starts, lengths, len(self))\n\n        # Single Seq\n        if len(seq) == 0: return 0.0\n        counts = np.bincount(seq.encoded, minlength=len(self))\n        # Filter zero counts to avoid log(0)\n        counts = counts[counts &gt; 0]\n        probs = counts / len(seq)\n        return -np.sum(probs * np.log2(probs))\n\n    def compress(self, seq: Union['Seq', 'SeqBatch']) -&gt; Union['CompressedSeq', 'CompressedSeqBatch']:\n        \"\"\"\n        Compresses a Seq or SeqBatch using bit-packing.\n        Only supports alphabets with &lt;= 4 bits per symbol (e.g. DNA, RNA).\n\n        Args:\n            seq: The sequence or batch to compress.\n\n        Returns:\n            A CompressedSeq or CompressedSeqBatch.\n        \"\"\"\n        bits = max(1, self.bits_per_symbol)\n        if bits &gt; 4:\n            raise ValueError(f\"Compression not supported for alphabets with &gt; 4 bits per symbol (current: {bits})\")\n\n        if isinstance(seq, Seq):\n            packed = _pack_seq_kernel(seq.encoded, len(seq), bits)\n            return self.new_compressed_seq(packed, len(seq), bits)\n\n        if isinstance(seq, SeqBatch):\n            data, starts, lengths = seq.arrays\n            per_byte = 8 // bits\n            byte_lengths = (lengths + per_byte - 1) // per_byte\n            total_bytes = byte_lengths.sum()\n\n            packed_data = np.zeros(total_bytes, dtype=np.uint8)\n            packed_starts = np.zeros(len(lengths), dtype=np.int32)\n            if len(lengths) &gt; 1:\n                np.cumsum(byte_lengths[:-1], out=packed_starts[1:])\n\n            _pack_batch_fill_kernel(data, starts, lengths, packed_data, packed_starts, bits)\n            return self.new_compressed_batch(packed_data, packed_starts, lengths, bits)\n\n        raise TypeError(f\"Cannot compress {type(seq)}\")\n\n    def decompress(self, compressed: 'CompressedSeq') -&gt; 'Seq':\n        \"\"\"Decompresses a CompressedSeq back to a standard Seq.\"\"\"\n        decoded = _unpack_seq_kernel(compressed._data, compressed._length, compressed._bits)\n        return self.seq_from(decoded)\n\n    def decompress_batch(self, batch: 'CompressedSeqBatch') -&gt; 'SeqBatch':\n        \"\"\"Decompresses a CompressedSeqBatch back to a standard SeqBatch.\"\"\"\n        total_len = batch._lengths.sum()\n        out_data = np.empty(total_len, dtype=np.uint8)\n        out_starts = np.zeros(len(batch), dtype=np.int32)\n        if len(batch) &gt; 1:\n            np.cumsum(batch._lengths[:-1], out=out_starts[1:])\n\n        _unpack_batch_kernel(batch._data, batch._starts, batch._lengths, out_data, out_starts, batch._bits)\n        return self.new_batch(out_data, out_starts, batch._lengths)\n\n    def new_seq(self, data: np.ndarray) -&gt; 'Seq':\n        \"\"\"\n        Factory method. The ONLY valid way to create a Seq.\n        \"\"\"\n        return Seq(data, self, _validation_token=self)\n\n    def seq_from(self, data: Union['Seq', str, bytes, np.ndarray]) -&gt; 'Seq':\n        \"\"\"Creates a Seq object from various input types, ensuring correct encoding.\n\n        Args:\n            data: The input data. Can be a ``Seq``, string, bytes, or numpy array.\n\n        Returns:\n            A new ``Seq`` object with this alphabet.\n\n        Raises:\n            AlphabetError: If the input data contains symbols not in the alphabet.\n        \"\"\"\n        # 1. Handle Pre-encoded (Optimization for internal use)\n        if isinstance(data, Seq):\n            if data.alphabet != self: raise AlphabetError(f'Sequence has a different alphabet \"{data.alphabet}\"')\n            return data\n        # 2. Handle Numpy Array (Assume it is uint8 text or encoded?)\n        # Convention: If it's uint8 array passed as 'core', treat as encoded indices\n        if isinstance(data, np.ndarray): return self.new_seq(data)\n        # 3. Handle Text/Bytes\n        if isinstance(data, str): data = data.encode(self.ENCODING)\n        # Use the vectorized encoder\n        return self.new_seq(self.encode(data))\n\n    def empty_seq(self) -&gt; 'Seq':\n        \"\"\"Returns an empty sequence with this alphabet.\n\n        Returns:\n            An empty ``Seq``.\n        \"\"\"\n        return self.new_seq(np.empty(0, dtype=self.DTYPE))\n\n    def random_seq(self, rng: np.random.Generator = None, length: int = None, min_len: int = 5, max_len: int = 5000,\n                   weights=None) -&gt; 'Seq':\n        \"\"\"\n        Generates a random sequence from this alphabet and coerces it to a Seq object.\n\n        Args:\n            rng: Random number generator (optional).\n            length: Exact length of sequence to generate.\n            min_len: Minimum length if length is not specified.\n            max_len: Maximum length if length is not specified.\n            weights: Weights for each symbol (optional).\n\n        Returns:\n            A random Seq object.\n\n        Examples:\n            &gt;&gt;&gt; dna = Alphabet.dna()\n            &gt;&gt;&gt; s = dna.random_seq(length=10)\n            &gt;&gt;&gt; len(s)\n            10\n        \"\"\"\n        if rng is None: rng = RESOURCES.rng\n        length = length or rng.integers(min_len, max_len)\n        # Optimization: Generate encoded indices directly (avoiding bytes round-trip)\n        n_sym = len(self._data)\n        if weights is None:\n            indices = rng.integers(0, n_sym, size=length, dtype=self.DTYPE)\n        else:\n            indices = rng.choice(n_sym, size=length, p=weights)\n        return self.seq_from(indices.astype(self.DTYPE))\n\n    def new_batch(self, data: np.ndarray, starts: np.ndarray, lengths: np.ndarray):\n        \"\"\"\n        Factory method. The ONLY valid way to create a SeqBatch.\n        \"\"\"\n        return SeqBatch(data, starts, lengths, self, _validation_token=self)\n\n    def batch_from(self, data: Iterable['Seq'], deduplicate: bool = False) -&gt; 'SeqBatch':\n        \"\"\"Creates a SeqBatch from an iterable of sequences.\n\n        Args:\n            data: An iterable of ``Seq`` objects (must have this alphabet).\n            deduplicate: If ``True``, deduplicates identical sequences to save memory.\n\n        Returns:\n            A new ``SeqBatch``.\n\n        Raises:\n            AlphabetError: If any sequence has a different alphabet.\n        \"\"\"\n        # Optimization: Fast path for existing SeqBatch (Clone)\n        if isinstance(data, SeqBatch):\n            if data.alphabet != self: raise AlphabetError(\n                \"Can only create a batch from batches with the same alphabet.\")\n            return self.new_batch(data.encoded.copy(), data.starts.copy(), data.lengths.copy())\n\n        items = data if isinstance(data, (list, tuple)) else list(data)\n        if not items: return self.empty_batch()\n\n        # Pass 1: Lengths &amp; Validation\n        count = len(items)\n        lengths = np.empty(count, dtype=np.int32)\n        for i, s in enumerate(items):\n            lengths[i] = len(s)\n            if s.alphabet != self: raise AlphabetError(\"Can only create a batch from sequences with the same alphabet.\")\n\n        # Pass 2: Fill Data (Optimized with C-level concatenation)\n        starts = np.zeros(count, dtype=np.int32)\n\n        if deduplicate and count &gt; 1:\n            # Deduplication Logic\n            unique_map = {} # Seq -&gt; (offset, length)\n            unique_parts = []\n            current_offset = 0\n\n            for i, s in enumerate(items):\n                if s in unique_map:\n                    offset, _ = unique_map[s]\n                    starts[i] = offset\n                else:\n                    starts[i] = current_offset\n                    unique_map[s] = (current_offset, lengths[i])\n                    unique_parts.append(s.encoded)\n                    current_offset += lengths[i]\n\n            data = np.concatenate(unique_parts) if unique_parts else np.empty(0, dtype=self.DTYPE)\n        else:\n            # Standard Contiguous Logic\n            if count &gt; 0:\n                data = items[0].encoded if count == 1 else np.concatenate([s.encoded for s in items])\n            else:\n                data = np.empty(0, dtype=self.DTYPE)\n\n            if count &gt; 1: np.cumsum(lengths[:-1], out=starts[1:])\n\n        return self.new_batch(data, starts, lengths)\n\n    def empty_batch(self) -&gt; 'SeqBatch':\n        \"\"\"Returns an empty sequence batch with this alphabet.\n\n        Returns:\n            An empty ``SeqBatch``.\n        \"\"\"\n        return self.new_batch(\n            np.empty(0, dtype=self.DTYPE), np.empty(0, dtype=np.int32), np.empty(0, dtype=np.int32))\n\n    def zeros_batch(self, n: int) -&gt; 'SeqBatch':\n        \"\"\"Returns a SeqBatch of n zero-length sequences.\"\"\"\n        return self.new_batch(\n            np.empty(0, dtype=self.DTYPE), np.zeros(n, dtype=np.int32), np.zeros(n, dtype=np.int32))\n\n    def new_compressed_seq(self, data: np.ndarray, length: int, bits: int) -&gt; 'CompressedSeq':\n        \"\"\"\n        Factory method. The ONLY valid way to create a CompressedSeq.\n        \"\"\"\n        return CompressedSeq(data, length, self, bits, _validation_token=self)\n\n    def new_compressed_batch(self, data: np.ndarray, starts: np.ndarray, lengths: np.ndarray, bits: int) -&gt; 'CompressedSeqBatch':\n        \"\"\"\n        Factory method. The ONLY valid way to create a CompressedSeqBatch.\n        \"\"\"\n        return CompressedSeqBatch(data, starts, lengths, self, bits, _validation_token=self)\n\n    def empty_compressed(self, bits: int = 2) -&gt; 'CompressedSeqBatch':\n        \"\"\"Returns an empty CompressedSeqBatch.\"\"\"\n        return self.new_compressed_batch(\n            np.empty(0, dtype=np.uint8), np.empty(0, dtype=np.int32), np.empty(0, dtype=np.int32), bits\n        )\n\n    def zeros_compressed(self, n: int, bits: int = 2) -&gt; 'CompressedSeqBatch':\n        \"\"\"Returns a CompressedSeqBatch of n zero-length sequences.\"\"\"\n        return self.new_compressed_batch(\n            np.empty(0, dtype=np.uint8),\n            np.zeros(n, dtype=np.int32),\n            np.zeros(n, dtype=np.int32),\n            bits\n        )\n\n\n    def random_batch(self, rng: np.random.Generator = None, n_seqs: int = None, min_seqs: int = 1,\n                     max_seqs: int = 1000, length: int = None, min_len: int = 10, max_len: int = 5_000_000,\n                     weights=None) -&gt; 'SeqBatch':\n        \"\"\"\n        Generates a SeqBatch of random sequences efficiently.\n\n        Args:\n            rng: Random number generator (optional).\n            n_seqs: Number of sequences to generate.\n            min_seqs: Minimum number of sequences to generate.\n            max_seqs: Maximum number of sequences to generate.\n            length: Exact length of sequences to generate.\n            min_len: Minimum length of sequences to generate.\n            max_len: Maximum length of sequences to generate.\n            weights: Weights for each symbol (optional).\n\n        Returns:\n            A SeqBatch containing the random sequences.\n        \"\"\"\n        if rng is None: rng = RESOURCES.rng\n        if n_seqs is None: n_seqs = int(rng.integers(min_seqs, max_seqs))\n\n        if length is not None:\n            if n_seqs &gt; length:\n                raise ValueError(f\"Cannot partition length {length} into {n_seqs} sequences (min_len=1)\")\n            if n_seqs &gt; 1:\n                # Use choice without replacement to ensure distinct cuts (no zero-length seqs)\n                cuts = np.sort(rng.choice(length - 1, size=n_seqs - 1, replace=False) + 1)\n                bounds = np.concatenate(([0], cuts, [length]))\n                lengths_arr = np.diff(bounds).astype(np.int32)\n            else:\n                lengths_arr = np.array([length], dtype=np.int32)\n        else:\n            lengths_arr = rng.integers(min_len, max_len, size=n_seqs, dtype=np.int32)\n            np.maximum(1, lengths_arr, out=lengths_arr)\n\n        if lengths_arr.size == 0: return self.empty_batch()\n\n        total_len = lengths_arr.sum()\n        n_sym = len(self._data)\n\n        # Optimization: Generate encoded indices directly\n        if weights is None:\n            indices = rng.integers(0, n_sym, size=total_len, dtype=self.DTYPE)\n        else:\n            indices = rng.choice(n_sym, size=total_len, p=weights)\n\n        starts = np.zeros(len(lengths_arr), dtype=np.int32)\n        if len(lengths_arr) &gt; 1:\n            np.cumsum(lengths_arr[:-1], out=starts[1:])\n\n        return self.new_batch(indices.astype(self.DTYPE, copy=False), starts, lengths_arr)\n\n    def reverse_complement(self, seq: 'Seq') -&gt; 'Seq':\n        \"\"\"Returns the reverse complement of the sequence.\n\n        Args:\n            seq: The input sequence.\n\n        Returns:\n            A new ``Seq`` object (reverse complemented). Returns the input sequence unchanged if no complement is defined.\n        \"\"\"\n        if self._complement is None: return seq\n        # Use .encoded for direct numpy access (much faster than iterating reversed(seq))\n        return self.seq_from(self._complement[seq.encoded[::-1]])\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.bits_per_symbol","title":"<code>bits_per_symbol</code>  <code>property</code>","text":"<p>Returns the number of bits required to represent a symbol in this alphabet.</p> <p>Returns:</p> Type Description <code>int</code> <p>The number of bits (integer).</p>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.complement","title":"<code>complement</code>  <code>property</code>","text":"<p>Returns the complement lookup table if available.</p>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.__init__","title":"<code>__init__(symbols, complement=None, aliases=None)</code>","text":"<p>Initializes an Alphabet.</p> <p>Parameters:</p> Name Type Description Default <code>symbols</code> <code>bytes</code> <p>The symbols in the alphabet as bytes.</p> required <code>complement</code> <code>bytes</code> <p>Optional complement symbols as bytes. Must be same length as symbols.</p> <code>None</code> <code>aliases</code> <code>dict[bytes, bytes]</code> <p>Optional mapping of invalid characters to valid ones (e.g. {b'N': b'A'}).</p> <code>None</code> <p>Raises:</p> Type Description <code>AlphabetError</code> <p>If symbols are not ASCII, too long, contain duplicates, or if complement is invalid.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def __init__(self, symbols: bytes, complement: bytes = None, aliases: dict[bytes, bytes] = None):\n    \"\"\"\n    Initializes an Alphabet.\n\n    Args:\n        symbols: The symbols in the alphabet as bytes.\n        complement: Optional complement symbols as bytes. Must be same length as symbols.\n        aliases: Optional mapping of invalid characters to valid ones (e.g. {b'N': b'A'}).\n\n    Raises:\n        AlphabetError: If symbols are not ASCII, too long, contain duplicates, or if complement is invalid.\n    \"\"\"\n    if not symbols.isascii(): raise AlphabetError('Alphabet symbols must be a valid ASCII string')\n    if len(symbols) &gt; self.MAX_LEN:\n        raise AlphabetError(f'Alphabet size cannot exceed {self.MAX_LEN} symbols ({self.DTYPE})')\n    if len(set(symbols.upper())) != len(symbols): raise AlphabetError('Alphabet contains duplicate symbols')\n\n    self._data: np.ndarray = np.frombuffer(symbols, dtype=self.DTYPE)\n\n    # Build Lookup Table\n    self._lookup_table = np.full(self.MAX_LEN, self.INVALID, dtype=self.DTYPE)\n    indices = np.arange(len(symbols), dtype=self.DTYPE)\n    self._lookup_table[np.frombuffer(symbols, dtype=self.DTYPE)] = indices\n    self._lookup_table[np.frombuffer(symbols.lower(), dtype=self.DTYPE)] = indices\n\n    # Apply Aliases (Map invalid chars to valid indices)\n    if aliases:\n        for src, dst in aliases.items():\n            if len(src) != 1 or len(dst) != 1: raise AlphabetError(\"Aliases must be single bytes\")\n\n            # Resolve destination index\n            dst_idx = self._lookup_table[ord(dst)]\n            if dst_idx == self.INVALID: raise AlphabetError(f\"Alias target {dst} not in alphabet\")\n\n            # Map source (both cases)\n            self._lookup_table[ord(src)] = dst_idx\n            self._lookup_table[ord(src.lower())] = dst_idx\n\n    # Build Translation Tables\n    self._trans_table = self._lookup_table.tobytes()\n    self._delete_bytes = np.where(self._lookup_table == self.INVALID)[0].astype(self.DTYPE).tobytes()\n\n    # Build Decode Table (for fast tobytes)\n    decode_map = np.zeros(256, dtype=self.DTYPE)\n    decode_map[:len(self._data)] = self._data\n    self._decode_table = decode_map.tobytes()\n\n    self._complement = None\n    if complement is not None:\n        if len(complement) != len(symbols):\n            raise AlphabetError(\"Complement must be the same length as symbols\")\n        comp_indices = self._lookup_table[np.frombuffer(complement, dtype=self.DTYPE)]\n        if np.any(comp_indices == self.INVALID):\n            raise AlphabetError(\"Complement contains symbols not in alphabet\")\n        self._complement = comp_indices\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.batch_from","title":"<code>batch_from(data, deduplicate=False)</code>","text":"<p>Creates a SeqBatch from an iterable of sequences.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Iterable[Seq]</code> <p>An iterable of <code>Seq</code> objects (must have this alphabet).</p> required <code>deduplicate</code> <code>bool</code> <p>If <code>True</code>, deduplicates identical sequences to save memory.</p> <code>False</code> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A new <code>SeqBatch</code>.</p> <p>Raises:</p> Type Description <code>AlphabetError</code> <p>If any sequence has a different alphabet.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def batch_from(self, data: Iterable['Seq'], deduplicate: bool = False) -&gt; 'SeqBatch':\n    \"\"\"Creates a SeqBatch from an iterable of sequences.\n\n    Args:\n        data: An iterable of ``Seq`` objects (must have this alphabet).\n        deduplicate: If ``True``, deduplicates identical sequences to save memory.\n\n    Returns:\n        A new ``SeqBatch``.\n\n    Raises:\n        AlphabetError: If any sequence has a different alphabet.\n    \"\"\"\n    # Optimization: Fast path for existing SeqBatch (Clone)\n    if isinstance(data, SeqBatch):\n        if data.alphabet != self: raise AlphabetError(\n            \"Can only create a batch from batches with the same alphabet.\")\n        return self.new_batch(data.encoded.copy(), data.starts.copy(), data.lengths.copy())\n\n    items = data if isinstance(data, (list, tuple)) else list(data)\n    if not items: return self.empty_batch()\n\n    # Pass 1: Lengths &amp; Validation\n    count = len(items)\n    lengths = np.empty(count, dtype=np.int32)\n    for i, s in enumerate(items):\n        lengths[i] = len(s)\n        if s.alphabet != self: raise AlphabetError(\"Can only create a batch from sequences with the same alphabet.\")\n\n    # Pass 2: Fill Data (Optimized with C-level concatenation)\n    starts = np.zeros(count, dtype=np.int32)\n\n    if deduplicate and count &gt; 1:\n        # Deduplication Logic\n        unique_map = {} # Seq -&gt; (offset, length)\n        unique_parts = []\n        current_offset = 0\n\n        for i, s in enumerate(items):\n            if s in unique_map:\n                offset, _ = unique_map[s]\n                starts[i] = offset\n            else:\n                starts[i] = current_offset\n                unique_map[s] = (current_offset, lengths[i])\n                unique_parts.append(s.encoded)\n                current_offset += lengths[i]\n\n        data = np.concatenate(unique_parts) if unique_parts else np.empty(0, dtype=self.DTYPE)\n    else:\n        # Standard Contiguous Logic\n        if count &gt; 0:\n            data = items[0].encoded if count == 1 else np.concatenate([s.encoded for s in items])\n        else:\n            data = np.empty(0, dtype=self.DTYPE)\n\n        if count &gt; 1: np.cumsum(lengths[:-1], out=starts[1:])\n\n    return self.new_batch(data, starts, lengths)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.compress","title":"<code>compress(seq)</code>","text":"<p>Compresses a Seq or SeqBatch using bit-packing. Only supports alphabets with &lt;= 4 bits per symbol (e.g. DNA, RNA).</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Union[Seq, SeqBatch]</code> <p>The sequence or batch to compress.</p> required <p>Returns:</p> Type Description <code>Union[CompressedSeq, CompressedSeqBatch]</code> <p>A CompressedSeq or CompressedSeqBatch.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def compress(self, seq: Union['Seq', 'SeqBatch']) -&gt; Union['CompressedSeq', 'CompressedSeqBatch']:\n    \"\"\"\n    Compresses a Seq or SeqBatch using bit-packing.\n    Only supports alphabets with &lt;= 4 bits per symbol (e.g. DNA, RNA).\n\n    Args:\n        seq: The sequence or batch to compress.\n\n    Returns:\n        A CompressedSeq or CompressedSeqBatch.\n    \"\"\"\n    bits = max(1, self.bits_per_symbol)\n    if bits &gt; 4:\n        raise ValueError(f\"Compression not supported for alphabets with &gt; 4 bits per symbol (current: {bits})\")\n\n    if isinstance(seq, Seq):\n        packed = _pack_seq_kernel(seq.encoded, len(seq), bits)\n        return self.new_compressed_seq(packed, len(seq), bits)\n\n    if isinstance(seq, SeqBatch):\n        data, starts, lengths = seq.arrays\n        per_byte = 8 // bits\n        byte_lengths = (lengths + per_byte - 1) // per_byte\n        total_bytes = byte_lengths.sum()\n\n        packed_data = np.zeros(total_bytes, dtype=np.uint8)\n        packed_starts = np.zeros(len(lengths), dtype=np.int32)\n        if len(lengths) &gt; 1:\n            np.cumsum(byte_lengths[:-1], out=packed_starts[1:])\n\n        _pack_batch_fill_kernel(data, starts, lengths, packed_data, packed_starts, bits)\n        return self.new_compressed_batch(packed_data, packed_starts, lengths, bits)\n\n    raise TypeError(f\"Cannot compress {type(seq)}\")\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.decode","title":"<code>decode(encoded)</code>","text":"<p>Decodes an array of indices back to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>encoded</code> <code>ndarray</code> <p>The numpy array of indices (uint8).</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>The decoded bytes string.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def decode(self, encoded: np.ndarray) -&gt; bytes:\n    \"\"\"Decodes an array of indices back to bytes.\n\n    Args:\n        encoded: The numpy array of indices (uint8).\n\n    Returns:\n        The decoded bytes string.\n    \"\"\"\n    # Ensure uint8 for byte-wise translation\n    if encoded.dtype != self.DTYPE:\n        encoded = encoded.astype(self.DTYPE, copy=False)\n    return encoded.tobytes().translate(self._decode_table)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.decompress","title":"<code>decompress(compressed)</code>","text":"<p>Decompresses a CompressedSeq back to a standard Seq.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def decompress(self, compressed: 'CompressedSeq') -&gt; 'Seq':\n    \"\"\"Decompresses a CompressedSeq back to a standard Seq.\"\"\"\n    decoded = _unpack_seq_kernel(compressed._data, compressed._length, compressed._bits)\n    return self.seq_from(decoded)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.decompress_batch","title":"<code>decompress_batch(batch)</code>","text":"<p>Decompresses a CompressedSeqBatch back to a standard SeqBatch.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def decompress_batch(self, batch: 'CompressedSeqBatch') -&gt; 'SeqBatch':\n    \"\"\"Decompresses a CompressedSeqBatch back to a standard SeqBatch.\"\"\"\n    total_len = batch._lengths.sum()\n    out_data = np.empty(total_len, dtype=np.uint8)\n    out_starts = np.zeros(len(batch), dtype=np.int32)\n    if len(batch) &gt; 1:\n        np.cumsum(batch._lengths[:-1], out=out_starts[1:])\n\n    _unpack_batch_kernel(batch._data, batch._starts, batch._lengths, out_data, out_starts, batch._bits)\n    return self.new_batch(out_data, out_starts, batch._lengths)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.detect","title":"<code>detect(text)</code>  <code>classmethod</code>","text":"<p>Detects the most likely alphabet for the given byte string.</p> <p>Candidates are checked in the following priority order: 1. DNA 2. RNA 3. AMINO 4. MURPHY_10</p> <p>Scoring is based on: 1. Canonical Count: Number of characters strictly in the alphabet definition (case-insensitive). 2. Valid Count: Number of characters valid in the alphabet (including aliases).</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>bytes</code> <p>The input byte string (or ASCII string).</p> required <p>Returns:</p> Type Description <code>Alphabet</code> <p>The most likely Alphabet singleton.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>@classmethod\ndef detect(cls, text: bytes) -&gt; 'Alphabet':\n    \"\"\"\n    Detects the most likely alphabet for the given byte string.\n\n    Candidates are checked in the following priority order:\n    1. DNA\n    2. RNA\n    3. AMINO\n    4. MURPHY_10\n\n    Scoring is based on:\n    1. Canonical Count: Number of characters strictly in the alphabet definition (case-insensitive).\n    2. Valid Count: Number of characters valid in the alphabet (including aliases).\n\n    Args:\n        text: The input byte string (or ASCII string).\n\n    Returns:\n        The most likely Alphabet singleton.\n    \"\"\"\n    if isinstance(text, str):\n        text = text.encode(cls.ENCODING)\n\n    data = np.frombuffer(text, dtype=cls.DTYPE)\n\n    # Optimization: Scan data once to get character counts\n    # This reduces complexity from O(K*N) to O(N + K*C) where K=num_alphabets, C=256\n    counts = np.bincount(data, minlength=cls.MAX_LEN)\n\n    # Candidate alphabets in priority order\n    candidates = [cls.DNA, cls.RNA, cls.AMINO, cls.MURPHY_10]\n\n    best_alpha = candidates[0]\n    best_score = (-1, -1)\n\n    for alpha in candidates:\n        # 1. Canonical Score\n        # Create mask for canonical symbols (both cases)\n        is_canonical = np.zeros(cls.MAX_LEN, dtype=bool)\n        is_canonical[alpha._data] = True\n\n        # Handle lowercase\n        lower_indices = np.frombuffer(alpha._data.tobytes().lower(), dtype=cls.DTYPE)\n        is_canonical[lower_indices] = True\n\n        # Sum counts of canonical characters\n        n_canonical = counts[is_canonical].sum()\n\n        # 2. Valid Score\n        # Use lookup table - anything not INVALID is valid\n        is_valid = (alpha._lookup_table != cls.INVALID)\n        n_valid = counts[is_valid].sum()\n\n        score = (n_canonical, n_valid)\n\n        if score &gt; best_score:\n            best_score = score\n            best_alpha = alpha\n\n    return best_alpha\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.empty_batch","title":"<code>empty_batch()</code>","text":"<p>Returns an empty sequence batch with this alphabet.</p> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>An empty <code>SeqBatch</code>.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def empty_batch(self) -&gt; 'SeqBatch':\n    \"\"\"Returns an empty sequence batch with this alphabet.\n\n    Returns:\n        An empty ``SeqBatch``.\n    \"\"\"\n    return self.new_batch(\n        np.empty(0, dtype=self.DTYPE), np.empty(0, dtype=np.int32), np.empty(0, dtype=np.int32))\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.empty_compressed","title":"<code>empty_compressed(bits=2)</code>","text":"<p>Returns an empty CompressedSeqBatch.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def empty_compressed(self, bits: int = 2) -&gt; 'CompressedSeqBatch':\n    \"\"\"Returns an empty CompressedSeqBatch.\"\"\"\n    return self.new_compressed_batch(\n        np.empty(0, dtype=np.uint8), np.empty(0, dtype=np.int32), np.empty(0, dtype=np.int32), bits\n    )\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.empty_seq","title":"<code>empty_seq()</code>","text":"<p>Returns an empty sequence with this alphabet.</p> <p>Returns:</p> Type Description <code>Seq</code> <p>An empty <code>Seq</code>.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def empty_seq(self) -&gt; 'Seq':\n    \"\"\"Returns an empty sequence with this alphabet.\n\n    Returns:\n        An empty ``Seq``.\n    \"\"\"\n    return self.new_seq(np.empty(0, dtype=self.DTYPE))\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.encode","title":"<code>encode(text)</code>","text":"<p>Zero-copy encoding from Byte String to Array.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>bytes</code> <p>The text to encode as bytes.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of encoded indices.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def encode(self, text: bytes) -&gt; np.ndarray:\n    \"\"\"\n    Zero-copy encoding from Byte String to Array.\n\n    Args:\n        text: The text to encode as bytes.\n\n    Returns:\n        A numpy array of encoded indices.\n    \"\"\"\n    return np.frombuffer(text.translate(self._trans_table, delete=self._delete_bytes), dtype=self.DTYPE)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.entropy","title":"<code>entropy(seq)</code>","text":"<p>Calculates the Shannon entropy of a sequence or batch. H = -sum(p_i * log2(p_i))</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Union[Seq, SeqBatch]</code> <p>A Seq or SeqBatch object.</p> required <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>The entropy in bits (float for Seq, ndarray for SeqBatch).</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def entropy(self, seq: Union['Seq', 'SeqBatch']) -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Calculates the Shannon entropy of a sequence or batch.\n    H = -sum(p_i * log2(p_i))\n\n    Args:\n        seq: A Seq or SeqBatch object.\n\n    Returns:\n        The entropy in bits (float for Seq, ndarray for SeqBatch).\n    \"\"\"\n    if seq.alphabet != self:\n        raise ValueError(f\"Seq alphabet {seq.alphabet} does not match Alphabet {self}\")\n\n    if isinstance(seq, SeqBatch):\n        data, starts, lengths = seq.arrays\n        return _batch_entropy_kernel(data, starts, lengths, len(self))\n\n    # Single Seq\n    if len(seq) == 0: return 0.0\n    counts = np.bincount(seq.encoded, minlength=len(self))\n    # Filter zero counts to avoid log(0)\n    counts = counts[counts &gt; 0]\n    probs = counts / len(seq)\n    return -np.sum(probs * np.log2(probs))\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.masker","title":"<code>masker(k)</code>","text":"<p>Returns (bits_per_symbol, bit_mask, dtype) for a specific K.</p> <p>Parameters:</p> Name Type Description Default <code>k</code> <code>int</code> <p>The k-mer length.</p> required <p>Returns:</p> Type Description <code>tuple[int, int, dtype]</code> <p>A tuple of (bits_per_symbol, bit_mask, dtype).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If K is too large for 64-bit hashing.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def masker(self, k: int) -&gt; tuple[int, int, np.dtype]:\n    \"\"\"\n    Returns (bits_per_symbol, bit_mask, dtype) for a specific K.\n\n    Args:\n        k: The k-mer length.\n\n    Returns:\n        A tuple of (bits_per_symbol, bit_mask, dtype).\n\n    Raises:\n        ValueError: If K is too large for 64-bit hashing.\n    \"\"\"\n    bps = self.bits_per_symbol\n    total_bits = k * bps\n    if total_bits &lt;= 32:\n        dtype = np.uint32\n    elif total_bits &lt;= 64:\n        dtype = np.uint64\n    else:\n        raise ValueError(f\"K={k} is too large for 64-bit hashing with {self}\")\n    mask = (1 &lt;&lt; (bps * (k - 1))) - 1\n    return bps, mask, dtype\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.new_batch","title":"<code>new_batch(data, starts, lengths)</code>","text":"<p>Factory method. The ONLY valid way to create a SeqBatch.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def new_batch(self, data: np.ndarray, starts: np.ndarray, lengths: np.ndarray):\n    \"\"\"\n    Factory method. The ONLY valid way to create a SeqBatch.\n    \"\"\"\n    return SeqBatch(data, starts, lengths, self, _validation_token=self)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.new_compressed_batch","title":"<code>new_compressed_batch(data, starts, lengths, bits)</code>","text":"<p>Factory method. The ONLY valid way to create a CompressedSeqBatch.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def new_compressed_batch(self, data: np.ndarray, starts: np.ndarray, lengths: np.ndarray, bits: int) -&gt; 'CompressedSeqBatch':\n    \"\"\"\n    Factory method. The ONLY valid way to create a CompressedSeqBatch.\n    \"\"\"\n    return CompressedSeqBatch(data, starts, lengths, self, bits, _validation_token=self)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.new_compressed_seq","title":"<code>new_compressed_seq(data, length, bits)</code>","text":"<p>Factory method. The ONLY valid way to create a CompressedSeq.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def new_compressed_seq(self, data: np.ndarray, length: int, bits: int) -&gt; 'CompressedSeq':\n    \"\"\"\n    Factory method. The ONLY valid way to create a CompressedSeq.\n    \"\"\"\n    return CompressedSeq(data, length, self, bits, _validation_token=self)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.new_seq","title":"<code>new_seq(data)</code>","text":"<p>Factory method. The ONLY valid way to create a Seq.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def new_seq(self, data: np.ndarray) -&gt; 'Seq':\n    \"\"\"\n    Factory method. The ONLY valid way to create a Seq.\n    \"\"\"\n    return Seq(data, self, _validation_token=self)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.random_batch","title":"<code>random_batch(rng=None, n_seqs=None, min_seqs=1, max_seqs=1000, length=None, min_len=10, max_len=5000000, weights=None)</code>","text":"<p>Generates a SeqBatch of random sequences efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Random number generator (optional).</p> <code>None</code> <code>n_seqs</code> <code>int</code> <p>Number of sequences to generate.</p> <code>None</code> <code>min_seqs</code> <code>int</code> <p>Minimum number of sequences to generate.</p> <code>1</code> <code>max_seqs</code> <code>int</code> <p>Maximum number of sequences to generate.</p> <code>1000</code> <code>length</code> <code>int</code> <p>Exact length of sequences to generate.</p> <code>None</code> <code>min_len</code> <code>int</code> <p>Minimum length of sequences to generate.</p> <code>10</code> <code>max_len</code> <code>int</code> <p>Maximum length of sequences to generate.</p> <code>5000000</code> <code>weights</code> <p>Weights for each symbol (optional).</p> <code>None</code> <p>Returns:</p> Type Description <code>SeqBatch</code> <p>A SeqBatch containing the random sequences.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def random_batch(self, rng: np.random.Generator = None, n_seqs: int = None, min_seqs: int = 1,\n                 max_seqs: int = 1000, length: int = None, min_len: int = 10, max_len: int = 5_000_000,\n                 weights=None) -&gt; 'SeqBatch':\n    \"\"\"\n    Generates a SeqBatch of random sequences efficiently.\n\n    Args:\n        rng: Random number generator (optional).\n        n_seqs: Number of sequences to generate.\n        min_seqs: Minimum number of sequences to generate.\n        max_seqs: Maximum number of sequences to generate.\n        length: Exact length of sequences to generate.\n        min_len: Minimum length of sequences to generate.\n        max_len: Maximum length of sequences to generate.\n        weights: Weights for each symbol (optional).\n\n    Returns:\n        A SeqBatch containing the random sequences.\n    \"\"\"\n    if rng is None: rng = RESOURCES.rng\n    if n_seqs is None: n_seqs = int(rng.integers(min_seqs, max_seqs))\n\n    if length is not None:\n        if n_seqs &gt; length:\n            raise ValueError(f\"Cannot partition length {length} into {n_seqs} sequences (min_len=1)\")\n        if n_seqs &gt; 1:\n            # Use choice without replacement to ensure distinct cuts (no zero-length seqs)\n            cuts = np.sort(rng.choice(length - 1, size=n_seqs - 1, replace=False) + 1)\n            bounds = np.concatenate(([0], cuts, [length]))\n            lengths_arr = np.diff(bounds).astype(np.int32)\n        else:\n            lengths_arr = np.array([length], dtype=np.int32)\n    else:\n        lengths_arr = rng.integers(min_len, max_len, size=n_seqs, dtype=np.int32)\n        np.maximum(1, lengths_arr, out=lengths_arr)\n\n    if lengths_arr.size == 0: return self.empty_batch()\n\n    total_len = lengths_arr.sum()\n    n_sym = len(self._data)\n\n    # Optimization: Generate encoded indices directly\n    if weights is None:\n        indices = rng.integers(0, n_sym, size=total_len, dtype=self.DTYPE)\n    else:\n        indices = rng.choice(n_sym, size=total_len, p=weights)\n\n    starts = np.zeros(len(lengths_arr), dtype=np.int32)\n    if len(lengths_arr) &gt; 1:\n        np.cumsum(lengths_arr[:-1], out=starts[1:])\n\n    return self.new_batch(indices.astype(self.DTYPE, copy=False), starts, lengths_arr)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.random_seq","title":"<code>random_seq(rng=None, length=None, min_len=5, max_len=5000, weights=None)</code>","text":"<p>Generates a random sequence from this alphabet and coerces it to a Seq object.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Random number generator (optional).</p> <code>None</code> <code>length</code> <code>int</code> <p>Exact length of sequence to generate.</p> <code>None</code> <code>min_len</code> <code>int</code> <p>Minimum length if length is not specified.</p> <code>5</code> <code>max_len</code> <code>int</code> <p>Maximum length if length is not specified.</p> <code>5000</code> <code>weights</code> <p>Weights for each symbol (optional).</p> <code>None</code> <p>Returns:</p> Type Description <code>Seq</code> <p>A random Seq object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dna = Alphabet.dna()\n&gt;&gt;&gt; s = dna.random_seq(length=10)\n&gt;&gt;&gt; len(s)\n10\n</code></pre> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def random_seq(self, rng: np.random.Generator = None, length: int = None, min_len: int = 5, max_len: int = 5000,\n               weights=None) -&gt; 'Seq':\n    \"\"\"\n    Generates a random sequence from this alphabet and coerces it to a Seq object.\n\n    Args:\n        rng: Random number generator (optional).\n        length: Exact length of sequence to generate.\n        min_len: Minimum length if length is not specified.\n        max_len: Maximum length if length is not specified.\n        weights: Weights for each symbol (optional).\n\n    Returns:\n        A random Seq object.\n\n    Examples:\n        &gt;&gt;&gt; dna = Alphabet.dna()\n        &gt;&gt;&gt; s = dna.random_seq(length=10)\n        &gt;&gt;&gt; len(s)\n        10\n    \"\"\"\n    if rng is None: rng = RESOURCES.rng\n    length = length or rng.integers(min_len, max_len)\n    # Optimization: Generate encoded indices directly (avoiding bytes round-trip)\n    n_sym = len(self._data)\n    if weights is None:\n        indices = rng.integers(0, n_sym, size=length, dtype=self.DTYPE)\n    else:\n        indices = rng.choice(n_sym, size=length, p=weights)\n    return self.seq_from(indices.astype(self.DTYPE))\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.reverse_complement","title":"<code>reverse_complement(seq)</code>","text":"<p>Returns the reverse complement of the sequence.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Seq</code> <p>The input sequence.</p> required <p>Returns:</p> Type Description <code>Seq</code> <p>A new <code>Seq</code> object (reverse complemented). Returns the input sequence unchanged if no complement is defined.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def reverse_complement(self, seq: 'Seq') -&gt; 'Seq':\n    \"\"\"Returns the reverse complement of the sequence.\n\n    Args:\n        seq: The input sequence.\n\n    Returns:\n        A new ``Seq`` object (reverse complemented). Returns the input sequence unchanged if no complement is defined.\n    \"\"\"\n    if self._complement is None: return seq\n    # Use .encoded for direct numpy access (much faster than iterating reversed(seq))\n    return self.seq_from(self._complement[seq.encoded[::-1]])\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.seq_from","title":"<code>seq_from(data)</code>","text":"<p>Creates a Seq object from various input types, ensuring correct encoding.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[Seq, str, bytes, ndarray]</code> <p>The input data. Can be a <code>Seq</code>, string, bytes, or numpy array.</p> required <p>Returns:</p> Type Description <code>Seq</code> <p>A new <code>Seq</code> object with this alphabet.</p> <p>Raises:</p> Type Description <code>AlphabetError</code> <p>If the input data contains symbols not in the alphabet.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def seq_from(self, data: Union['Seq', str, bytes, np.ndarray]) -&gt; 'Seq':\n    \"\"\"Creates a Seq object from various input types, ensuring correct encoding.\n\n    Args:\n        data: The input data. Can be a ``Seq``, string, bytes, or numpy array.\n\n    Returns:\n        A new ``Seq`` object with this alphabet.\n\n    Raises:\n        AlphabetError: If the input data contains symbols not in the alphabet.\n    \"\"\"\n    # 1. Handle Pre-encoded (Optimization for internal use)\n    if isinstance(data, Seq):\n        if data.alphabet != self: raise AlphabetError(f'Sequence has a different alphabet \"{data.alphabet}\"')\n        return data\n    # 2. Handle Numpy Array (Assume it is uint8 text or encoded?)\n    # Convention: If it's uint8 array passed as 'core', treat as encoded indices\n    if isinstance(data, np.ndarray): return self.new_seq(data)\n    # 3. Handle Text/Bytes\n    if isinstance(data, str): data = data.encode(self.ENCODING)\n    # Use the vectorized encoder\n    return self.new_seq(self.encode(data))\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.zeros_batch","title":"<code>zeros_batch(n)</code>","text":"<p>Returns a SeqBatch of n zero-length sequences.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def zeros_batch(self, n: int) -&gt; 'SeqBatch':\n    \"\"\"Returns a SeqBatch of n zero-length sequences.\"\"\"\n    return self.new_batch(\n        np.empty(0, dtype=self.DTYPE), np.zeros(n, dtype=np.int32), np.zeros(n, dtype=np.int32))\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.Alphabet.zeros_compressed","title":"<code>zeros_compressed(n, bits=2)</code>","text":"<p>Returns a CompressedSeqBatch of n zero-length sequences.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def zeros_compressed(self, n: int, bits: int = 2) -&gt; 'CompressedSeqBatch':\n    \"\"\"Returns a CompressedSeqBatch of n zero-length sequences.\"\"\"\n    return self.new_compressed_batch(\n        np.empty(0, dtype=np.uint8),\n        np.zeros(n, dtype=np.int32),\n        np.zeros(n, dtype=np.int32),\n        bits\n    )\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetConverter","title":"<code>AlphabetConverter</code>","text":"<p>Converts sequences from one Alphabet to another.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>class AlphabetConverter:\n    \"\"\"\n    Converts sequences from one Alphabet to another.\n    \"\"\"\n    __slots__ = ('_source', '_target', '_table', '_rev_table')\n    TRANSCRIBE: ClassVar['AlphabetConverter']\n    TO_MURPHY_10: ClassVar['AlphabetConverter']\n\n    def __init__(self, source: Alphabet, target: Alphabet, mapping: dict[bytes, bytes] = None, default: bytes = None):\n        \"\"\"Initializes a converter between two alphabets.\n\n        Args:\n            source: Source alphabet.\n            target: Target alphabet.\n            mapping: Optional dictionary for custom symbol mapping.\n            default: Optional default value for unmapped symbols.\n        \"\"\"\n        self._source = source\n        self._target = target\n        self._table = self._build_table(source, target, mapping, default)\n\n        # Build Reverse Table (target -&gt; source)\n        rev_mapping = {}\n        if mapping:\n            for k, v in mapping.items():\n                # Heuristic to invert mapping:\n                # 1. If v is not in source, we must map it back (e.g. U-&gt;T if U not in DNA).\n                # 2. If v is in mapping (as a key), it means v was remapped in forward (e.g. Swap A-&gt;C, C-&gt;A),\n                #    so we should map it back (C-&gt;A).\n                # 3. Otherwise (v is in source and not remapped), we assume v-&gt;v identity is preferred\n                #    (e.g. V-&gt;L, L-&gt;L identity preferred over L-&gt;V).\n                if v not in source or v in mapping: rev_mapping[v] = k\n\n        self._rev_table = self._build_table(target, source, rev_mapping, None)\n\n    @staticmethod\n    def _build_table(source, target, mapping, default):\n        final_mapping = {}\n        for i in range(len(source)):\n            sym = source.decode(np.array([i], dtype=source.DTYPE))\n            if sym in target:\n                final_mapping[sym] = sym\n\n        if mapping:\n            final_mapping.update(mapping)\n\n        table = np.full(len(source), target.INVALID, dtype=target.DTYPE)\n\n        if default is not None:\n            def_encoded = target.encode(default)\n            if len(def_encoded) &gt; 0:\n                table[:] = def_encoded[0]\n\n        for src, dst in final_mapping.items():\n            s_idx = source.encode(src)\n            d_idx = target.encode(dst)\n            if len(s_idx) &gt; 0 and len(d_idx) &gt; 0:\n                table[s_idx[0]] = d_idx[0]\n\n        table.flags.writeable = False\n        return table\n\n    def convert(self, seq: Union[Seq, SeqBatch]) -&gt; Union[Seq, SeqBatch]:\n        \"\"\"Converts a sequence or batch to the target alphabet.\n\n        Args:\n            seq: Input sequence or batch.\n\n        Returns:\n            The converted sequence or batch.\n\n        Raises:\n            ValueError: If the input alphabet does not match source or target (for reverse conversion).\n            ValueError: If conversion results in invalid symbols.\n        \"\"\"\n        if seq.alphabet == self._source:\n            table = self._table\n            target_alpha = self._target\n        elif seq.alphabet == self._target:\n            table = self._rev_table\n            target_alpha = self._source\n        else:\n            raise ValueError(f\"Input sequence alphabet {seq.alphabet} does not match converter source {self._source} or target {self._target}\")\n\n        new_data = table[seq.encoded]\n\n        if np.any(new_data == target_alpha.INVALID):\n             raise ValueError(\"Conversion resulted in invalid symbols (missing mapping for some characters)\")\n\n        if isinstance(seq, SeqBatch):\n            return target_alpha.new_batch(new_data, seq.starts.copy(), seq.lengths.copy())\n        return target_alpha.new_seq(new_data)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetConverter.__init__","title":"<code>__init__(source, target, mapping=None, default=None)</code>","text":"<p>Initializes a converter between two alphabets.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Alphabet</code> <p>Source alphabet.</p> required <code>target</code> <code>Alphabet</code> <p>Target alphabet.</p> required <code>mapping</code> <code>dict[bytes, bytes]</code> <p>Optional dictionary for custom symbol mapping.</p> <code>None</code> <code>default</code> <code>bytes</code> <p>Optional default value for unmapped symbols.</p> <code>None</code> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def __init__(self, source: Alphabet, target: Alphabet, mapping: dict[bytes, bytes] = None, default: bytes = None):\n    \"\"\"Initializes a converter between two alphabets.\n\n    Args:\n        source: Source alphabet.\n        target: Target alphabet.\n        mapping: Optional dictionary for custom symbol mapping.\n        default: Optional default value for unmapped symbols.\n    \"\"\"\n    self._source = source\n    self._target = target\n    self._table = self._build_table(source, target, mapping, default)\n\n    # Build Reverse Table (target -&gt; source)\n    rev_mapping = {}\n    if mapping:\n        for k, v in mapping.items():\n            # Heuristic to invert mapping:\n            # 1. If v is not in source, we must map it back (e.g. U-&gt;T if U not in DNA).\n            # 2. If v is in mapping (as a key), it means v was remapped in forward (e.g. Swap A-&gt;C, C-&gt;A),\n            #    so we should map it back (C-&gt;A).\n            # 3. Otherwise (v is in source and not remapped), we assume v-&gt;v identity is preferred\n            #    (e.g. V-&gt;L, L-&gt;L identity preferred over L-&gt;V).\n            if v not in source or v in mapping: rev_mapping[v] = k\n\n    self._rev_table = self._build_table(target, source, rev_mapping, None)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetConverter.convert","title":"<code>convert(seq)</code>","text":"<p>Converts a sequence or batch to the target alphabet.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Union[Seq, SeqBatch]</code> <p>Input sequence or batch.</p> required <p>Returns:</p> Type Description <code>Union[Seq, SeqBatch]</code> <p>The converted sequence or batch.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input alphabet does not match source or target (for reverse conversion).</p> <code>ValueError</code> <p>If conversion results in invalid symbols.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def convert(self, seq: Union[Seq, SeqBatch]) -&gt; Union[Seq, SeqBatch]:\n    \"\"\"Converts a sequence or batch to the target alphabet.\n\n    Args:\n        seq: Input sequence or batch.\n\n    Returns:\n        The converted sequence or batch.\n\n    Raises:\n        ValueError: If the input alphabet does not match source or target (for reverse conversion).\n        ValueError: If conversion results in invalid symbols.\n    \"\"\"\n    if seq.alphabet == self._source:\n        table = self._table\n        target_alpha = self._target\n    elif seq.alphabet == self._target:\n        table = self._rev_table\n        target_alpha = self._source\n    else:\n        raise ValueError(f\"Input sequence alphabet {seq.alphabet} does not match converter source {self._source} or target {self._target}\")\n\n    new_data = table[seq.encoded]\n\n    if np.any(new_data == target_alpha.INVALID):\n         raise ValueError(\"Conversion resulted in invalid symbols (missing mapping for some characters)\")\n\n    if isinstance(seq, SeqBatch):\n        return target_alpha.new_batch(new_data, seq.starts.copy(), seq.lengths.copy())\n    return target_alpha.new_seq(new_data)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetError","title":"<code>AlphabetError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when an alphabet is invalid or an operation is incompatible with the alphabet.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>class AlphabetError(Exception):\n    \"\"\"Raised when an alphabet is invalid or an operation is incompatible with the alphabet.\"\"\"\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetProperty","title":"<code>AlphabetProperty</code>","text":"<p>Maps sequence symbols to numerical properties (e.g., Hydrophobicity). Calculates average scores for Seqs and SeqBatches.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>class AlphabetProperty:\n    \"\"\"\n    Maps sequence symbols to numerical properties (e.g., Hydrophobicity).\n    Calculates average scores for Seqs and SeqBatches.\n    \"\"\"\n    __slots__ = ('_data', '_alphabet', '_default')\n    HYDROPHOBICITY: ClassVar['AlphabetProperty']\n    GC: ClassVar['AlphabetProperty']\n\n    def __init__(self, alphabet: 'Alphabet', mapping: dict[Union[bytes, str], float], default: float = np.nan):\n        \"\"\"\n        Args:\n            mapping: Dictionary mapping characters (str) to values (float).\n            alphabet: The Alphabet instance to align with.\n            default: Value to assign to symbols not in the mapping (default: NaN).\n        \"\"\"\n        self._alphabet = alphabet\n        self._default = default\n        self._data = np.full(len(alphabet), default, dtype=np.float32)\n        for char, val in mapping.items():\n            if isinstance(char, str): char = char.encode(Alphabet.ENCODING)\n            encoded = alphabet.encode(char)\n            if len(encoded) &gt; 0: self._data[encoded[0]] = val\n        self._data.flags.writeable = False\n\n    def encode(self, seq: Union[Seq, SeqBatch]) -&gt; np.ndarray:\n        \"\"\"Encodes a sequence or batch into property values.\n\n        Args:\n            seq: The sequence or batch to encode.\n\n        Returns:\n            A numpy array of float values.\n\n        Raises:\n            ValueError: If the sequence alphabet does not match.\n        \"\"\"\n        if seq.alphabet != self._alphabet:\n            raise ValueError(f\"Seq alphabet {seq.alphabet} does not match alphabet {self._alphabet}\")\n        return self._data[seq.encoded]\n\n    def score(self, seq: Union[Seq, SeqBatch], aggregator: str = 'mean') -&gt; Union[float, np.ndarray]:\n        \"\"\"\n        Calculates the aggregate score (mean/sum) for the sequence(s).\n        Ignores NaN values (unmapped symbols).\n\n        Args:\n            seq: Seq or SeqBatch.\n            aggregator: 'mean' or 'sum'.\n\n        Returns:\n            Float score (for Seq) or Array of scores (for SeqBatch).\n        \"\"\"\n        mapped = self.encode(seq)\n        if isinstance(seq, SeqBatch):\n            return _batch_score_kernel(mapped, seq.starts, seq.lengths, aggregator == 'mean')\n        else:\n            if aggregator == 'mean': return np.nanmean(mapped)\n            if aggregator == 'sum': return np.nansum(mapped)\n            raise ValueError(f\"Unknown aggregator: {aggregator}\")\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetProperty.__init__","title":"<code>__init__(alphabet, mapping, default=np.nan)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mapping</code> <code>dict[Union[bytes, str], float]</code> <p>Dictionary mapping characters (str) to values (float).</p> required <code>alphabet</code> <code>Alphabet</code> <p>The Alphabet instance to align with.</p> required <code>default</code> <code>float</code> <p>Value to assign to symbols not in the mapping (default: NaN).</p> <code>nan</code> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def __init__(self, alphabet: 'Alphabet', mapping: dict[Union[bytes, str], float], default: float = np.nan):\n    \"\"\"\n    Args:\n        mapping: Dictionary mapping characters (str) to values (float).\n        alphabet: The Alphabet instance to align with.\n        default: Value to assign to symbols not in the mapping (default: NaN).\n    \"\"\"\n    self._alphabet = alphabet\n    self._default = default\n    self._data = np.full(len(alphabet), default, dtype=np.float32)\n    for char, val in mapping.items():\n        if isinstance(char, str): char = char.encode(Alphabet.ENCODING)\n        encoded = alphabet.encode(char)\n        if len(encoded) &gt; 0: self._data[encoded[0]] = val\n    self._data.flags.writeable = False\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetProperty.encode","title":"<code>encode(seq)</code>","text":"<p>Encodes a sequence or batch into property values.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Union[Seq, SeqBatch]</code> <p>The sequence or batch to encode.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of float values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the sequence alphabet does not match.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def encode(self, seq: Union[Seq, SeqBatch]) -&gt; np.ndarray:\n    \"\"\"Encodes a sequence or batch into property values.\n\n    Args:\n        seq: The sequence or batch to encode.\n\n    Returns:\n        A numpy array of float values.\n\n    Raises:\n        ValueError: If the sequence alphabet does not match.\n    \"\"\"\n    if seq.alphabet != self._alphabet:\n        raise ValueError(f\"Seq alphabet {seq.alphabet} does not match alphabet {self._alphabet}\")\n    return self._data[seq.encoded]\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.AlphabetProperty.score","title":"<code>score(seq, aggregator='mean')</code>","text":"<p>Calculates the aggregate score (mean/sum) for the sequence(s). Ignores NaN values (unmapped symbols).</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Union[Seq, SeqBatch]</code> <p>Seq or SeqBatch.</p> required <code>aggregator</code> <code>str</code> <p>'mean' or 'sum'.</p> <code>'mean'</code> <p>Returns:</p> Type Description <code>Union[float, ndarray]</code> <p>Float score (for Seq) or Array of scores (for SeqBatch).</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def score(self, seq: Union[Seq, SeqBatch], aggregator: str = 'mean') -&gt; Union[float, np.ndarray]:\n    \"\"\"\n    Calculates the aggregate score (mean/sum) for the sequence(s).\n    Ignores NaN values (unmapped symbols).\n\n    Args:\n        seq: Seq or SeqBatch.\n        aggregator: 'mean' or 'sum'.\n\n    Returns:\n        Float score (for Seq) or Array of scores (for SeqBatch).\n    \"\"\"\n    mapped = self.encode(seq)\n    if isinstance(seq, SeqBatch):\n        return _batch_score_kernel(mapped, seq.starts, seq.lengths, aggregator == 'mean')\n    else:\n        if aggregator == 'mean': return np.nanmean(mapped)\n        if aggregator == 'sum': return np.nansum(mapped)\n        raise ValueError(f\"Unknown aggregator: {aggregator}\")\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode","title":"<code>GeneticCode</code>","text":"<p>Represents a genetic code table for translation.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>class GeneticCode:\n    \"\"\"\n    Represents a genetic code table for translation.\n    \"\"\"\n    __slots__ = ('_data', '_starts', '_stops')\n    _DNA = Alphabet.DNA\n    _AMINO = Alphabet.AMINO\n    BACTERIA: ClassVar['GeneticCode']\n\n    def __init__(self, table: bytes, starts: Iterable[bytes] = ()):\n        \"\"\"Initializes a genetic code.\n\n        Args:\n            table: 64-byte ASCII string representing the translation table.\n            starts: Iterable of start codons (e.g. ``[b'ATG', b'GTG']``).\n        \"\"\"\n        # Optimization: Pre-encode the table to indices using lookup table directly\n        self._data = self._AMINO._lookup_table[np.frombuffer(table, dtype=Alphabet.DTYPE)]\n        # Populate stops and starts\n        # We derive stops from the table string\n        self._stops = np.frombuffer(table, dtype=Alphabet.DTYPE) == ord('*')\n        # Populate starts\n        self._starts = np.zeros(64, dtype=bool)\n\n        valid_starts = [s for s in starts if len(s) == 3]\n        if valid_starts:\n            joined = b\"\".join(valid_starts)\n            encoded = self._DNA.encode(joined)\n\n            if len(encoded) == len(joined):\n                # Fast path: Vectorized calculation (No invalid chars dropped)\n                indices = (encoded[0::3] &lt;&lt; 4) | (encoded[1::3] &lt;&lt; 2) | encoded[2::3]\n                self._starts[indices] = True\n            else:  # Fallback: Process individually to handle invalid chars safely without frame shifts\n                for s in valid_starts:\n                    enc = self._DNA.encode(s)\n                    if len(enc) == 3:\n                        idx = (enc[0] &lt;&lt; 4) | (enc[1] &lt;&lt; 2) | enc[2]\n                        self._starts[idx] = True\n\n    @property\n    def starts(self) -&gt; np.ndarray:\n        \"\"\"Boolean array indicating valid start codons (size 64).\"\"\"\n        return self._starts\n\n    @property\n    def stops(self) -&gt; np.ndarray:\n        \"\"\"Boolean array indicating stop codons (size 64).\"\"\"\n        return self._stops\n\n    def __iter__(self):\n        return iter(self._data)\n\n    def __getitem__(self, item):\n        return self._data[item]\n\n    def __array__(self, dtype=None):\n        return self._data.astype(dtype, copy=False) if dtype else self._data\n\n    def __repr__(self):\n        return repr(self._data)\n\n    def find_starts(self, seq: 'Seq') -&gt; np.ndarray:\n        \"\"\"Returns indices of all start codons in the sequence (0-based).\"\"\"\n        if seq.alphabet != self._DNA: raise ValueError(\"Sequence must use the DNA alphabet\")\n        return _find_codons_kernel(seq.encoded, self._starts)\n\n    def find_stops(self, seq: 'Seq') -&gt; np.ndarray:\n        \"\"\"Returns indices of all stop codons in the sequence (0-based).\"\"\"\n        if seq.alphabet != self._DNA: raise ValueError(\"Sequence must use the DNA alphabet\")\n        return _find_codons_kernel(seq.encoded, self._stops)\n\n    def find_orfs(self, seq: 'Seq', strand: Literal[0, 1, -1] = 0, min_len: int = 30, max_len: int = 3000,\n                  include_partials: bool = False) -&gt; IntervalBatch:\n        \"\"\"\n        Finds all Open Reading Frames (ORFs) in the sequence.\n        Returns all in-frame start-stop pairs with no intervening stops.\n\n        Args:\n            seq: The input DNA sequence.\n            strand: 1 (forward), -1 (reverse), or 0 (both).\n            min_len: Minimum length in bases (inclusive).\n            max_len: Maximum length in bases (inclusive).\n            include_partials: If True, includes ORFs that are truncated at the sequence boundaries.\n\n        Returns:\n            An IntervalBatch of the found ORFs.\n        \"\"\"\n        if seq.alphabet != self._DNA: raise ValueError(\"Sequence must use the DNA alphabet\")\n        starts, ends, strands = [], [], []\n        # Forward Strand\n        if strand &gt;= 0:\n            s, e = _find_orfs_kernel(seq.encoded, self._starts, self._stops, min_len, max_len, include_partials)\n            if len(s) &gt; 0:\n                starts.append(np.array(s, dtype=np.int32))\n                ends.append(np.array(e, dtype=np.int32))\n                strands.append(np.ones(len(s), dtype=np.int32))\n        # Reverse Strand\n        if strand &lt;= 0:\n            rc_seq = seq.alphabet.reverse_complement(seq)\n            s_rc, e_rc = _find_orfs_kernel(rc_seq.encoded, self._starts, self._stops, min_len, max_len,\n                                           include_partials)\n            if len(s_rc) &gt; 0:\n                s_rc = np.array(s_rc, dtype=np.int32)\n                e_rc = np.array(e_rc, dtype=np.int32)\n                L = len(seq)\n                starts.append(L - e_rc)\n                ends.append(L - s_rc)\n                strands.append(np.full(len(s_rc), -1, dtype=np.int32))\n        if not starts: return IntervalBatch()\n        return IntervalBatch(np.concatenate(starts), np.concatenate(ends), np.concatenate(strands))\n\n    def is_complete_cds(self, seq: 'Seq') -&gt; bool:\n        \"\"\"Checks if the sequence starts with a start codon and ends with a stop codon.\"\"\"\n        if len(seq) &lt; 3 or len(seq) % 3 != 0: return False\n        return _check_cds_kernel(seq.encoded, self._starts, self._stops)\n\n    def translate(self, seq: Union['Seq', 'SeqBatch'], frame: Literal[0, 1, 2] = 0, to_stop: bool = True) -&gt; Union[\n        'Seq', 'SeqBatch']:\n        \"\"\"\n        Translates a DNA sequence or SeqBatch to Amino Acids.\n\n        Args:\n            seq: The DNA sequence.\n            frame: The reading frame (0, 1, or 2).\n            to_stop: If True, translation terminates at the first stop codon.\n\n        Returns:\n            The translated protein sequence.\n\n        Raises:\n            TranslationError: If the sequence is too short.\n        \"\"\"\n        if isinstance(seq, SeqBatch):\n            return self._translate_batch(seq, frame, to_stop)\n\n        # Use encoded sequence (0-3 integers) for faster lookup in small table\n        # This avoids cache misses associated with the large 16MB lookup table\n        n = len(seq)\n        start = frame\n        n_codons = (n - start) // 3\n        if n_codons &lt;= 0:\n            if n &lt; 3 - frame: raise TranslationError('Cannot translate sequence with less than 1 codon')\n        translation = _translate_kernel(seq.encoded, self._data, self._stops, start, n_codons, to_stop, Alphabet.DTYPE)\n        return self._AMINO.seq_from(translation)\n\n    def _translate_batch(self, batch: 'SeqBatch', frame: int, to_stop: bool) -&gt; 'SeqBatch':\n        \"\"\"\n        Internal batch translation logic.\n\n        Args:\n            batch: The batch of sequences to translate.\n            frame: The reading frame.\n            to_stop: Whether to stop at stop codons.\n\n        Returns:\n            A new SeqBatch containing translated sequences.\n        \"\"\"\n        if len(batch) == 0: return self._AMINO.empty_batch()\n\n        data, starts, lengths = batch.arrays\n        # 1. Calculate lengths (Pass 1)\n        new_lengths = _batch_translate_len_kernel(\n            data, starts, lengths, self._data, self._stops, frame, to_stop\n        )\n\n        # 2. Calculate offsets\n        new_count = len(lengths)\n        new_starts = np.zeros(new_count, dtype=np.int32)\n        if new_count &gt; 0:\n            np.cumsum(new_lengths[:-1], out=new_starts[1:])\n            total_len = new_starts[-1] + new_lengths[-1]\n        else:\n            total_len = 0\n\n        new_data = np.empty(total_len, dtype=Alphabet.DTYPE)\n\n        # 3. Fill (Pass 2)\n        if total_len &gt; 0:\n            _batch_translate_fill_kernel(\n                data, starts, self._data, frame, new_data, new_starts, new_lengths\n            )\n\n        return self._AMINO.new_batch(new_data, new_starts, new_lengths)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.starts","title":"<code>starts</code>  <code>property</code>","text":"<p>Boolean array indicating valid start codons (size 64).</p>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.stops","title":"<code>stops</code>  <code>property</code>","text":"<p>Boolean array indicating stop codons (size 64).</p>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.__init__","title":"<code>__init__(table, starts=())</code>","text":"<p>Initializes a genetic code.</p> <p>Parameters:</p> Name Type Description Default <code>table</code> <code>bytes</code> <p>64-byte ASCII string representing the translation table.</p> required <code>starts</code> <code>Iterable[bytes]</code> <p>Iterable of start codons (e.g. <code>[b'ATG', b'GTG']</code>).</p> <code>()</code> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def __init__(self, table: bytes, starts: Iterable[bytes] = ()):\n    \"\"\"Initializes a genetic code.\n\n    Args:\n        table: 64-byte ASCII string representing the translation table.\n        starts: Iterable of start codons (e.g. ``[b'ATG', b'GTG']``).\n    \"\"\"\n    # Optimization: Pre-encode the table to indices using lookup table directly\n    self._data = self._AMINO._lookup_table[np.frombuffer(table, dtype=Alphabet.DTYPE)]\n    # Populate stops and starts\n    # We derive stops from the table string\n    self._stops = np.frombuffer(table, dtype=Alphabet.DTYPE) == ord('*')\n    # Populate starts\n    self._starts = np.zeros(64, dtype=bool)\n\n    valid_starts = [s for s in starts if len(s) == 3]\n    if valid_starts:\n        joined = b\"\".join(valid_starts)\n        encoded = self._DNA.encode(joined)\n\n        if len(encoded) == len(joined):\n            # Fast path: Vectorized calculation (No invalid chars dropped)\n            indices = (encoded[0::3] &lt;&lt; 4) | (encoded[1::3] &lt;&lt; 2) | encoded[2::3]\n            self._starts[indices] = True\n        else:  # Fallback: Process individually to handle invalid chars safely without frame shifts\n            for s in valid_starts:\n                enc = self._DNA.encode(s)\n                if len(enc) == 3:\n                    idx = (enc[0] &lt;&lt; 4) | (enc[1] &lt;&lt; 2) | enc[2]\n                    self._starts[idx] = True\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.find_orfs","title":"<code>find_orfs(seq, strand=0, min_len=30, max_len=3000, include_partials=False)</code>","text":"<p>Finds all Open Reading Frames (ORFs) in the sequence. Returns all in-frame start-stop pairs with no intervening stops.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Seq</code> <p>The input DNA sequence.</p> required <code>strand</code> <code>Literal[0, 1, -1]</code> <p>1 (forward), -1 (reverse), or 0 (both).</p> <code>0</code> <code>min_len</code> <code>int</code> <p>Minimum length in bases (inclusive).</p> <code>30</code> <code>max_len</code> <code>int</code> <p>Maximum length in bases (inclusive).</p> <code>3000</code> <code>include_partials</code> <code>bool</code> <p>If True, includes ORFs that are truncated at the sequence boundaries.</p> <code>False</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An IntervalBatch of the found ORFs.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def find_orfs(self, seq: 'Seq', strand: Literal[0, 1, -1] = 0, min_len: int = 30, max_len: int = 3000,\n              include_partials: bool = False) -&gt; IntervalBatch:\n    \"\"\"\n    Finds all Open Reading Frames (ORFs) in the sequence.\n    Returns all in-frame start-stop pairs with no intervening stops.\n\n    Args:\n        seq: The input DNA sequence.\n        strand: 1 (forward), -1 (reverse), or 0 (both).\n        min_len: Minimum length in bases (inclusive).\n        max_len: Maximum length in bases (inclusive).\n        include_partials: If True, includes ORFs that are truncated at the sequence boundaries.\n\n    Returns:\n        An IntervalBatch of the found ORFs.\n    \"\"\"\n    if seq.alphabet != self._DNA: raise ValueError(\"Sequence must use the DNA alphabet\")\n    starts, ends, strands = [], [], []\n    # Forward Strand\n    if strand &gt;= 0:\n        s, e = _find_orfs_kernel(seq.encoded, self._starts, self._stops, min_len, max_len, include_partials)\n        if len(s) &gt; 0:\n            starts.append(np.array(s, dtype=np.int32))\n            ends.append(np.array(e, dtype=np.int32))\n            strands.append(np.ones(len(s), dtype=np.int32))\n    # Reverse Strand\n    if strand &lt;= 0:\n        rc_seq = seq.alphabet.reverse_complement(seq)\n        s_rc, e_rc = _find_orfs_kernel(rc_seq.encoded, self._starts, self._stops, min_len, max_len,\n                                       include_partials)\n        if len(s_rc) &gt; 0:\n            s_rc = np.array(s_rc, dtype=np.int32)\n            e_rc = np.array(e_rc, dtype=np.int32)\n            L = len(seq)\n            starts.append(L - e_rc)\n            ends.append(L - s_rc)\n            strands.append(np.full(len(s_rc), -1, dtype=np.int32))\n    if not starts: return IntervalBatch()\n    return IntervalBatch(np.concatenate(starts), np.concatenate(ends), np.concatenate(strands))\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.find_starts","title":"<code>find_starts(seq)</code>","text":"<p>Returns indices of all start codons in the sequence (0-based).</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def find_starts(self, seq: 'Seq') -&gt; np.ndarray:\n    \"\"\"Returns indices of all start codons in the sequence (0-based).\"\"\"\n    if seq.alphabet != self._DNA: raise ValueError(\"Sequence must use the DNA alphabet\")\n    return _find_codons_kernel(seq.encoded, self._starts)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.find_stops","title":"<code>find_stops(seq)</code>","text":"<p>Returns indices of all stop codons in the sequence (0-based).</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def find_stops(self, seq: 'Seq') -&gt; np.ndarray:\n    \"\"\"Returns indices of all stop codons in the sequence (0-based).\"\"\"\n    if seq.alphabet != self._DNA: raise ValueError(\"Sequence must use the DNA alphabet\")\n    return _find_codons_kernel(seq.encoded, self._stops)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.is_complete_cds","title":"<code>is_complete_cds(seq)</code>","text":"<p>Checks if the sequence starts with a start codon and ends with a stop codon.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def is_complete_cds(self, seq: 'Seq') -&gt; bool:\n    \"\"\"Checks if the sequence starts with a start codon and ends with a stop codon.\"\"\"\n    if len(seq) &lt; 3 or len(seq) % 3 != 0: return False\n    return _check_cds_kernel(seq.encoded, self._starts, self._stops)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.GeneticCode.translate","title":"<code>translate(seq, frame=0, to_stop=True)</code>","text":"<p>Translates a DNA sequence or SeqBatch to Amino Acids.</p> <p>Parameters:</p> Name Type Description Default <code>seq</code> <code>Union[Seq, SeqBatch]</code> <p>The DNA sequence.</p> required <code>frame</code> <code>Literal[0, 1, 2]</code> <p>The reading frame (0, 1, or 2).</p> <code>0</code> <code>to_stop</code> <code>bool</code> <p>If True, translation terminates at the first stop codon.</p> <code>True</code> <p>Returns:</p> Type Description <code>Union[Seq, SeqBatch]</code> <p>The translated protein sequence.</p> <p>Raises:</p> Type Description <code>TranslationError</code> <p>If the sequence is too short.</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>def translate(self, seq: Union['Seq', 'SeqBatch'], frame: Literal[0, 1, 2] = 0, to_stop: bool = True) -&gt; Union[\n    'Seq', 'SeqBatch']:\n    \"\"\"\n    Translates a DNA sequence or SeqBatch to Amino Acids.\n\n    Args:\n        seq: The DNA sequence.\n        frame: The reading frame (0, 1, or 2).\n        to_stop: If True, translation terminates at the first stop codon.\n\n    Returns:\n        The translated protein sequence.\n\n    Raises:\n        TranslationError: If the sequence is too short.\n    \"\"\"\n    if isinstance(seq, SeqBatch):\n        return self._translate_batch(seq, frame, to_stop)\n\n    # Use encoded sequence (0-3 integers) for faster lookup in small table\n    # This avoids cache misses associated with the large 16MB lookup table\n    n = len(seq)\n    start = frame\n    n_codons = (n - start) // 3\n    if n_codons &lt;= 0:\n        if n &lt; 3 - frame: raise TranslationError('Cannot translate sequence with less than 1 codon')\n    translation = _translate_kernel(seq.encoded, self._data, self._stops, start, n_codons, to_stop, Alphabet.DTYPE)\n    return self._AMINO.seq_from(translation)\n</code></pre>"},{"location":"reference/baclib/core/alphabet/#baclib.core.alphabet.TranslationError","title":"<code>TranslationError</code>","text":"<p>               Bases: <code>AlphabetError</code></p> <p>Raised when nucleotide-to-amino-acid translation fails (e.g. invalid codon).</p> Source code in <code>baclib/core/alphabet.py</code> <pre><code>class TranslationError(AlphabetError):\n    \"\"\"Raised when nucleotide-to-amino-acid translation fails (e.g. invalid codon).\"\"\"\n</code></pre>"},{"location":"reference/baclib/core/interval/","title":"interval","text":""},{"location":"reference/baclib/core/interval/#baclib.core.interval","title":"<code>baclib.core.interval</code>","text":"<p>Genomic interval representation with strand and context, plus batched interval operations.</p>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Context","title":"<code>Context</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Spatial relationship between two genomic intervals.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>class Context(IntEnum):\n    \"\"\"Spatial relationship between two genomic intervals.\"\"\"\n    UPSTREAM = auto()\n    DOWNSTREAM = auto()\n    INSIDE = auto()\n    OVERLAPPING = auto()\n    OVERLAPPING_START = auto()\n    OVERLAPPING_END = auto()\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval","title":"<code>Interval</code>","text":"<p>Immutable genomic interval. Safe for hashing and use in sets/dicts.</p> <p>Attributes:</p> Name Type Description <code>start</code> <p>The start position (0-based, inclusive).</p> <code>end</code> <p>The end position (0-based, exclusive).</p> <code>strand</code> <code>Strand</code> <p>The strand (FORWARD, REVERSE, or UNSTRANDED).</p> Source code in <code>baclib/core/interval.py</code> <pre><code>class Interval:\n    \"\"\"\n    Immutable genomic interval. Safe for hashing and use in sets/dicts.\n\n    Attributes:\n        start: The start position (0-based, inclusive).\n        end: The end position (0-based, exclusive).\n        strand: The strand (FORWARD, REVERSE, or UNSTRANDED).\n    \"\"\"\n    __slots__ = ('_start', '_end', '_strand')\n\n    def __init__(self, start: int, end: int, strand: Any = None):\n        \"\"\"\n        Initializes an Interval.\n\n        Args:\n            start: Start position.\n            end: End position.\n            strand: Strand symbol or enum.\n        \"\"\"\n        self._start: int = int(start)\n        self._end: int = int(end)\n        self._strand: Strand = Strand.from_symbol(strand)\n\n    @property\n    def start(self): return self._start\n    @property\n    def end(self): return self._end\n    @property\n    def strand(self) -&gt; Strand: return self._strand\n    def __hash__(self): return hash((self._start, self._end, self._strand))\n    def __repr__(self): return f\"{self._start}:{self._end}({self._strand})\"\n    def __len__(self): return max(0, self._end - self._start)\n    def __iter__(self): return iter((self._start, self._end, self._strand))\n\n    def __array__(self, dtype=None):\n        \"\"\"Allows the Interval to be treated as a numpy array (e.g. np.array(interval)).\"\"\"\n        return np.array([self._start, self._end, self._strand], dtype=dtype or np.int32)\n\n    def __eq__(self, other):\n        if not isinstance(other, Interval): return False\n        # Direct slot access is fastest\n        return (self._start == other._start and\n                self._end == other._end and\n                self._strand == other._strand)\n\n    def __contains__(self, item: Union[slice, int, 'Interval']):\n        if isinstance(item, int): return self._start &lt;= item &lt; self._end\n        item = Interval.from_item(item)\n        return self._start &lt;= item.start and self._end &gt;= item.end\n\n    def overlap(self, other: Union[slice, int, 'Interval']) -&gt; int:\n        \"\"\"\n        Calculates the overlap length with another interval.\n\n        Args:\n            other: The other interval.\n\n        Returns:\n            The number of overlapping bases.\n        \"\"\"\n        other = Interval.from_item(other)\n        return max(0, min(self._end, other.end) - max(self._start, other.start))\n\n    # --- Hull Arithmetic ---\n    def __add__(self, other: Union[slice, int, 'Interval']) -&gt; 'Interval':\n        \"\"\"Returns the convex hull (union extent) as a NEW Interval.\n\n        Args:\n            other: The other interval.\n\n        Returns:\n            A new ``Interval`` spanning from the minimum start to maximum end.\n        \"\"\"\n        other = Interval.from_item(other)\n        new_strand = self._strand if self._strand == other.strand else 0\n        return Interval(min(self._start, other.start), max(self._end, other.end), new_strand)\n\n    def __radd__(self, other: Union[slice, int, 'Interval']) -&gt; 'Interval':\n        return self.__add__(other)\n\n    def intersection(self, other: Union[slice, int, 'Interval']) -&gt; 'Interval':\n        \"\"\"Returns the intersection of this interval and another as a NEW Interval.\n\n        Args:\n            other: The other interval.\n\n        Returns:\n            A new ``Interval`` representing the overlapping region.\n        \"\"\"\n        other = Interval.from_item(other)\n        new_start = max(self._start, other.start)\n        new_end = min(self._end, other.end)\n        # Result inherits self's strand? Or 0? Usually self's strand for intersection.\n        if new_start &gt;= new_end: return Interval(new_start, new_start, self._strand)\n        return Interval(new_start, new_end, self._strand)\n\n    def shift(self, x: int, y: int = None) -&gt; 'Interval':\n        \"\"\"\n        Shifts the interval coordinates.\n\n        Args:\n            x: Amount to shift start (and end if y is None).\n            y: Amount to shift end (optional).\n\n        Returns:\n            A new shifted Interval.\n        \"\"\"\n        return Interval(self._start + x, self._end + (y if y is not None else x), self._strand)\n\n    def reverse_complement(self, length: int = None) -&gt; 'Interval':\n        \"\"\"\n        Returns the interval coordinates on the reverse complement strand.\n\n        Args:\n            length: The length of the parent sequence.\n\n        Returns:\n            A new Interval on the opposite strand.\n        \"\"\"\n        if length is None: length = self._end  # - self._start\n        return Interval(length - self._end, length - self._start, self._strand * -1)\n\n    def relate(self, other: Union[slice, int, 'Interval']) -&gt; Context:\n        \"\"\"\n        Determines the spatial relationship of another interval relative to this one,\n        respecting the strand of this interval.\n\n        Args:\n            other: The other interval to compare.\n\n        Returns:\n            A Context enum member.\n        \"\"\"\n        other = Interval.from_item(other)\n\n        # Determine absolute orientation\n        if other.end &lt;= self._start:\n            return Context.UPSTREAM if self._strand &gt;= 0 else Context.DOWNSTREAM\n        if other.start &gt;= self._end:\n            return Context.DOWNSTREAM if self._strand &gt;= 0 else Context.UPSTREAM\n\n        # Overlap cases\n        if other.start &gt;= self._start and other.end &lt;= self._end: return Context.INSIDE\n\n        if other.start &lt; self._start:\n            if other.end &gt; self._end: return Context.OVERLAPPING\n            return Context.OVERLAPPING_START if self._strand &gt;= 0 else Context.OVERLAPPING_END\n\n        return Context.OVERLAPPING_END if self._strand &gt;= 0 else Context.OVERLAPPING_START\n\n    @classmethod\n    def random(cls, rng: np.random.Generator = None, length: int = None, min_len: int = 1, max_len: int = 10_000,\n               min_start: int = 0, max_start: int = 1_000_000):\n        \"\"\"\n        Generates a random Interval.\n\n        Args:\n            rng: Random number generator.\n            length: Fixed length (optional).\n            min_len: Minimum length.\n            max_len: Maximum length.\n            min_start: Minimum start position.\n            max_start: Maximum start position.\n\n        Returns:\n            A random Interval.\n        \"\"\"\n        if rng is None: rng = RESOURCES.rng\n        if not length: length = rng.integers(min_len, max_len)\n        # Ensure we don't go negative on the bounds\n        safe_max_start = max(min_start + 1, max_start - length)\n        start = rng.integers(min_start, safe_max_start)\n        return cls(start, start + length, rng.choice([1, -1]))\n\n    @classmethod\n    def from_match(cls, item: Match, strand: int = Strand.UNSTRANDED) -&gt; 'Interval': \n        return Interval(item.start(), item.end(), strand)\n\n    @classmethod\n    def from_int(cls, item: int, strand: int = Strand.UNSTRANDED, length: int = None) -&gt; 'Interval':\n        \"\"\"Creates an interval from a single integer index.\n\n        Args:\n            item: The integer index (can be negative).\n            strand: The strand.\n            length: Sequence length (required for negative indices).\n\n        Returns:\n            A new ``Interval`` of length 1.\n        \"\"\"\n        if item &lt; 0 and length is not None: item += length\n        return cls(item, item + 1, strand)\n\n    @classmethod\n    def from_slice(cls, item: slice, strand: int = Strand.UNSTRANDED, length: int = None) -&gt; 'Interval':\n        \"\"\"Creates an interval from a slice object.\n\n        Args:\n            item: The slice object.\n            strand: The strand.\n            length: Sequence length (required for slices with ``None`` stop).\n\n        Returns:\n            A new ``Interval``.\n        \"\"\"\n        start, stop, step = item.start, item.stop, item.step\n        if start is None: start = 0\n        if stop is None and length is not None: stop = length\n        if stop is None: raise ValueError(\"Cannot create Interval from slice with None stop without 'length'\")\n        if step == -1: return cls(stop + 1, start + 1, strand)\n        return cls(start, stop, strand)\n\n    @classmethod\n    def from_item(cls, item: Union[slice, int, 'Interval', Match], strand: int = Strand.UNSTRANDED, length: int = None) -&gt; 'Interval':\n        \"\"\"\n        Coerces various types into an Interval.\n\n        Args:\n            item: The item to coerce (slice, int, Interval, Match).\n            strand: Default strand if not present in item.\n            length: Length of the sequence (needed for slice with None stop).\n\n        Returns:\n            An Interval object.\n\n        Raises:\n            TypeError: If coercion is not possible.\n        \"\"\"\n        if isinstance(item, cls): return item\n        if interval := getattr(item, 'interval', None): return interval\n        if isinstance(item, Match): return cls.from_match(item, strand)\n        if isinstance(item, int): return cls.from_int(item, strand, length)\n        if isinstance(item, slice): return cls.from_slice(item, strand, length)\n        raise TypeError(f\"Cannot coerce {type(item)} to Interval\")\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.__add__","title":"<code>__add__(other)</code>","text":"<p>Returns the convex hull (union extent) as a NEW Interval.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[slice, int, Interval]</code> <p>The other interval.</p> required <p>Returns:</p> Type Description <code>Interval</code> <p>A new <code>Interval</code> spanning from the minimum start to maximum end.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def __add__(self, other: Union[slice, int, 'Interval']) -&gt; 'Interval':\n    \"\"\"Returns the convex hull (union extent) as a NEW Interval.\n\n    Args:\n        other: The other interval.\n\n    Returns:\n        A new ``Interval`` spanning from the minimum start to maximum end.\n    \"\"\"\n    other = Interval.from_item(other)\n    new_strand = self._strand if self._strand == other.strand else 0\n    return Interval(min(self._start, other.start), max(self._end, other.end), new_strand)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.__array__","title":"<code>__array__(dtype=None)</code>","text":"<p>Allows the Interval to be treated as a numpy array (e.g. np.array(interval)).</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def __array__(self, dtype=None):\n    \"\"\"Allows the Interval to be treated as a numpy array (e.g. np.array(interval)).\"\"\"\n    return np.array([self._start, self._end, self._strand], dtype=dtype or np.int32)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.__init__","title":"<code>__init__(start, end, strand=None)</code>","text":"<p>Initializes an Interval.</p> <p>Parameters:</p> Name Type Description Default <code>start</code> <code>int</code> <p>Start position.</p> required <code>end</code> <code>int</code> <p>End position.</p> required <code>strand</code> <code>Any</code> <p>Strand symbol or enum.</p> <code>None</code> Source code in <code>baclib/core/interval.py</code> <pre><code>def __init__(self, start: int, end: int, strand: Any = None):\n    \"\"\"\n    Initializes an Interval.\n\n    Args:\n        start: Start position.\n        end: End position.\n        strand: Strand symbol or enum.\n    \"\"\"\n    self._start: int = int(start)\n    self._end: int = int(end)\n    self._strand: Strand = Strand.from_symbol(strand)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.from_int","title":"<code>from_int(item, strand=Strand.UNSTRANDED, length=None)</code>  <code>classmethod</code>","text":"<p>Creates an interval from a single integer index.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>int</code> <p>The integer index (can be negative).</p> required <code>strand</code> <code>int</code> <p>The strand.</p> <code>UNSTRANDED</code> <code>length</code> <code>int</code> <p>Sequence length (required for negative indices).</p> <code>None</code> <p>Returns:</p> Type Description <code>Interval</code> <p>A new <code>Interval</code> of length 1.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef from_int(cls, item: int, strand: int = Strand.UNSTRANDED, length: int = None) -&gt; 'Interval':\n    \"\"\"Creates an interval from a single integer index.\n\n    Args:\n        item: The integer index (can be negative).\n        strand: The strand.\n        length: Sequence length (required for negative indices).\n\n    Returns:\n        A new ``Interval`` of length 1.\n    \"\"\"\n    if item &lt; 0 and length is not None: item += length\n    return cls(item, item + 1, strand)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.from_item","title":"<code>from_item(item, strand=Strand.UNSTRANDED, length=None)</code>  <code>classmethod</code>","text":"<p>Coerces various types into an Interval.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[slice, int, Interval, Match]</code> <p>The item to coerce (slice, int, Interval, Match).</p> required <code>strand</code> <code>int</code> <p>Default strand if not present in item.</p> <code>UNSTRANDED</code> <code>length</code> <code>int</code> <p>Length of the sequence (needed for slice with None stop).</p> <code>None</code> <p>Returns:</p> Type Description <code>Interval</code> <p>An Interval object.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If coercion is not possible.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef from_item(cls, item: Union[slice, int, 'Interval', Match], strand: int = Strand.UNSTRANDED, length: int = None) -&gt; 'Interval':\n    \"\"\"\n    Coerces various types into an Interval.\n\n    Args:\n        item: The item to coerce (slice, int, Interval, Match).\n        strand: Default strand if not present in item.\n        length: Length of the sequence (needed for slice with None stop).\n\n    Returns:\n        An Interval object.\n\n    Raises:\n        TypeError: If coercion is not possible.\n    \"\"\"\n    if isinstance(item, cls): return item\n    if interval := getattr(item, 'interval', None): return interval\n    if isinstance(item, Match): return cls.from_match(item, strand)\n    if isinstance(item, int): return cls.from_int(item, strand, length)\n    if isinstance(item, slice): return cls.from_slice(item, strand, length)\n    raise TypeError(f\"Cannot coerce {type(item)} to Interval\")\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.from_slice","title":"<code>from_slice(item, strand=Strand.UNSTRANDED, length=None)</code>  <code>classmethod</code>","text":"<p>Creates an interval from a slice object.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>slice</code> <p>The slice object.</p> required <code>strand</code> <code>int</code> <p>The strand.</p> <code>UNSTRANDED</code> <code>length</code> <code>int</code> <p>Sequence length (required for slices with <code>None</code> stop).</p> <code>None</code> <p>Returns:</p> Type Description <code>Interval</code> <p>A new <code>Interval</code>.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef from_slice(cls, item: slice, strand: int = Strand.UNSTRANDED, length: int = None) -&gt; 'Interval':\n    \"\"\"Creates an interval from a slice object.\n\n    Args:\n        item: The slice object.\n        strand: The strand.\n        length: Sequence length (required for slices with ``None`` stop).\n\n    Returns:\n        A new ``Interval``.\n    \"\"\"\n    start, stop, step = item.start, item.stop, item.step\n    if start is None: start = 0\n    if stop is None and length is not None: stop = length\n    if stop is None: raise ValueError(\"Cannot create Interval from slice with None stop without 'length'\")\n    if step == -1: return cls(stop + 1, start + 1, strand)\n    return cls(start, stop, strand)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.intersection","title":"<code>intersection(other)</code>","text":"<p>Returns the intersection of this interval and another as a NEW Interval.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[slice, int, Interval]</code> <p>The other interval.</p> required <p>Returns:</p> Type Description <code>Interval</code> <p>A new <code>Interval</code> representing the overlapping region.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def intersection(self, other: Union[slice, int, 'Interval']) -&gt; 'Interval':\n    \"\"\"Returns the intersection of this interval and another as a NEW Interval.\n\n    Args:\n        other: The other interval.\n\n    Returns:\n        A new ``Interval`` representing the overlapping region.\n    \"\"\"\n    other = Interval.from_item(other)\n    new_start = max(self._start, other.start)\n    new_end = min(self._end, other.end)\n    # Result inherits self's strand? Or 0? Usually self's strand for intersection.\n    if new_start &gt;= new_end: return Interval(new_start, new_start, self._strand)\n    return Interval(new_start, new_end, self._strand)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.overlap","title":"<code>overlap(other)</code>","text":"<p>Calculates the overlap length with another interval.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[slice, int, Interval]</code> <p>The other interval.</p> required <p>Returns:</p> Type Description <code>int</code> <p>The number of overlapping bases.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def overlap(self, other: Union[slice, int, 'Interval']) -&gt; int:\n    \"\"\"\n    Calculates the overlap length with another interval.\n\n    Args:\n        other: The other interval.\n\n    Returns:\n        The number of overlapping bases.\n    \"\"\"\n    other = Interval.from_item(other)\n    return max(0, min(self._end, other.end) - max(self._start, other.start))\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.random","title":"<code>random(rng=None, length=None, min_len=1, max_len=10000, min_start=0, max_start=1000000)</code>  <code>classmethod</code>","text":"<p>Generates a random Interval.</p> <p>Parameters:</p> Name Type Description Default <code>rng</code> <code>Generator</code> <p>Random number generator.</p> <code>None</code> <code>length</code> <code>int</code> <p>Fixed length (optional).</p> <code>None</code> <code>min_len</code> <code>int</code> <p>Minimum length.</p> <code>1</code> <code>max_len</code> <code>int</code> <p>Maximum length.</p> <code>10000</code> <code>min_start</code> <code>int</code> <p>Minimum start position.</p> <code>0</code> <code>max_start</code> <code>int</code> <p>Maximum start position.</p> <code>1000000</code> <p>Returns:</p> Type Description <p>A random Interval.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef random(cls, rng: np.random.Generator = None, length: int = None, min_len: int = 1, max_len: int = 10_000,\n           min_start: int = 0, max_start: int = 1_000_000):\n    \"\"\"\n    Generates a random Interval.\n\n    Args:\n        rng: Random number generator.\n        length: Fixed length (optional).\n        min_len: Minimum length.\n        max_len: Maximum length.\n        min_start: Minimum start position.\n        max_start: Maximum start position.\n\n    Returns:\n        A random Interval.\n    \"\"\"\n    if rng is None: rng = RESOURCES.rng\n    if not length: length = rng.integers(min_len, max_len)\n    # Ensure we don't go negative on the bounds\n    safe_max_start = max(min_start + 1, max_start - length)\n    start = rng.integers(min_start, safe_max_start)\n    return cls(start, start + length, rng.choice([1, -1]))\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.relate","title":"<code>relate(other)</code>","text":"<p>Determines the spatial relationship of another interval relative to this one, respecting the strand of this interval.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[slice, int, Interval]</code> <p>The other interval to compare.</p> required <p>Returns:</p> Type Description <code>Context</code> <p>A Context enum member.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def relate(self, other: Union[slice, int, 'Interval']) -&gt; Context:\n    \"\"\"\n    Determines the spatial relationship of another interval relative to this one,\n    respecting the strand of this interval.\n\n    Args:\n        other: The other interval to compare.\n\n    Returns:\n        A Context enum member.\n    \"\"\"\n    other = Interval.from_item(other)\n\n    # Determine absolute orientation\n    if other.end &lt;= self._start:\n        return Context.UPSTREAM if self._strand &gt;= 0 else Context.DOWNSTREAM\n    if other.start &gt;= self._end:\n        return Context.DOWNSTREAM if self._strand &gt;= 0 else Context.UPSTREAM\n\n    # Overlap cases\n    if other.start &gt;= self._start and other.end &lt;= self._end: return Context.INSIDE\n\n    if other.start &lt; self._start:\n        if other.end &gt; self._end: return Context.OVERLAPPING\n        return Context.OVERLAPPING_START if self._strand &gt;= 0 else Context.OVERLAPPING_END\n\n    return Context.OVERLAPPING_END if self._strand &gt;= 0 else Context.OVERLAPPING_START\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.reverse_complement","title":"<code>reverse_complement(length=None)</code>","text":"<p>Returns the interval coordinates on the reverse complement strand.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>The length of the parent sequence.</p> <code>None</code> <p>Returns:</p> Type Description <code>Interval</code> <p>A new Interval on the opposite strand.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def reverse_complement(self, length: int = None) -&gt; 'Interval':\n    \"\"\"\n    Returns the interval coordinates on the reverse complement strand.\n\n    Args:\n        length: The length of the parent sequence.\n\n    Returns:\n        A new Interval on the opposite strand.\n    \"\"\"\n    if length is None: length = self._end  # - self._start\n    return Interval(length - self._end, length - self._start, self._strand * -1)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Interval.shift","title":"<code>shift(x, y=None)</code>","text":"<p>Shifts the interval coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>int</code> <p>Amount to shift start (and end if y is None).</p> required <code>y</code> <code>int</code> <p>Amount to shift end (optional).</p> <code>None</code> <p>Returns:</p> Type Description <code>Interval</code> <p>A new shifted Interval.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def shift(self, x: int, y: int = None) -&gt; 'Interval':\n    \"\"\"\n    Shifts the interval coordinates.\n\n    Args:\n        x: Amount to shift start (and end if y is None).\n        y: Amount to shift end (optional).\n\n    Returns:\n        A new shifted Interval.\n    \"\"\"\n    return Interval(self._start + x, self._end + (y if y is not None else x), self._strand)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch","title":"<code>IntervalBatch</code>","text":"<p>               Bases: <code>Batch</code></p> <p>High-performance batch of genomic intervals, powered by NumPy.</p> <p>This class provides efficient querying, intersection, and manipulation of large sets of genomic intervals.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>class IntervalBatch(Batch):\n    \"\"\"\n    High-performance batch of genomic intervals, powered by NumPy.\n\n    This class provides efficient querying, intersection, and manipulation of\n    large sets of genomic intervals.\n    \"\"\"\n    __slots__ = ('_starts', '_ends', '_strands', '_original_indices', '_max_len')\n    _DTYPE = np.int32  # int32 allows up to 2.14 Billion bp (fits all bacterial genomes)\n\n    def __init__(self, starts: np.ndarray = None, ends: np.ndarray = None, strands: np.ndarray = None,\n                 original_indices: np.ndarray = None, sort: bool = True):\n        \"\"\"\n        Initializes an IntervalBatch.\n\n        Args:\n            starts: Array of start positions.\n            ends: Array of end positions.\n            strands: Array of strands.\n            original_indices: Array of original indices (for tracking after sort).\n            sort: Whether to sort the intervals (default: True).\n        \"\"\"\n        if starts is None:\n            self._starts = np.empty(0, dtype=self._DTYPE)\n            self._ends = np.empty(0, dtype=self._DTYPE)\n            self._strands = np.empty(0, dtype=self._DTYPE)\n            self._max_len = 0\n        else:\n            # Ensure contiguous arrays for Numba performance\n            self._starts = np.ascontiguousarray(starts, dtype=self._DTYPE)\n            self._ends = np.ascontiguousarray(ends, dtype=self._DTYPE)\n            self._strands = np.ascontiguousarray(strands, dtype=self._DTYPE) if strands is not None else np.zeros(len(starts), dtype=self._DTYPE)\n            # Calculate max length for query optimization\n            self._max_len = np.max(self._ends - self._starts) if len(self._starts) &gt; 0 else 0\n\n        self._original_indices = original_indices\n        if sort: self.sort()\n\n    def sort(self):\n        \"\"\"Sorts the intervals by start position, then end position.\"\"\"\n        # Sort both data and the index tracker\n        if len(self._starts) &gt; 1:\n            # Optimization: Check if already sorted to avoid expensive lexsort\n            if _is_sorted_kernel(self._starts, self._ends): return\n\n            # Lexsort: Primary key is last in the tuple (starts), Secondary is ends\n            order = np.lexsort((self._ends, self._starts))\n\n            self._starts = self._starts[order]\n            self._ends = self._ends[order]\n            self._strands = self._strands[order]\n\n            if self._original_indices is not None:\n                self._original_indices = self._original_indices[order]\n            else:\n                # Create the mapping only now that we know we are scrambling the order\n                self._original_indices = order.astype(np.int32)\n\n\n    @classmethod\n    def zeros(cls, n: int) -&gt; 'IntervalBatch':\n        \"\"\"Creates a batch of *n* 0-length intervals at position 0.\n\n        Args:\n            n: Number of intervals.\n\n        Returns:\n            A new ``IntervalBatch``.\n        \"\"\"\n        return cls(\n            np.zeros(n, dtype=np.int32),\n            np.zeros(n, dtype=np.int32),\n            np.ones(n, dtype=np.int32) # Default strand? Or 0? Or 1 (FORWARD)? Strand(1) is standard.\n        )\n\n    @classmethod\n    def random(cls, n: int, rng: np.random.Generator = None, length: int = None, min_len: int = 1, max_len: int = 1000,\n               min_start: int = 0, max_start: int = 1_000_000) -&gt; 'IntervalBatch':\n        \"\"\"\n        Creates a batch of n random intervals.\n\n        Args:\n            n: Number of intervals to generate.\n            rng: Random number generator (optional).\n            length: Fixed length for all intervals (optional).\n            min_len: Minimum length (default: 1).\n            max_len: Maximum length (default: 1000).\n            min_start: Minimum start position (default: 0).\n            max_start: Maximum start position (default: 1,000,000).\n\n        Returns:\n            An IntervalBatch.\n        \"\"\"\n        if rng is None: rng = RESOURCES.rng\n        if n &lt;= 0: return cls.empty()\n\n        # 1. Generate Lengths\n        if length is not None:\n             lengths = np.full(n, length, dtype=np.int32)\n        else:\n             lengths = rng.integers(min_len, max_len, size=n, dtype=np.int32)\n\n        # 2. Generate Starts\n        # We treat max_start as the upper bound for the start coordinate (exclusive).\n        if max_start &lt;= min_start:\n             raise ValueError(f\"max_start ({max_start}) must be &gt; min_start ({min_start})\")\n\n        starts = rng.integers(min_start, max_start, size=n, dtype=np.int32)\n\n        # 3. Generate Ends\n        ends = starts + lengths\n\n        # 4. Generate Strands (-1, 0, 1)\n        strands = rng.choice([-1, 0, 1], size=n).astype(np.int32)\n\n        return cls(starts, ends, strands, sort=True)\n\n    @classmethod\n    def empty(cls) -&gt; 'IntervalBatch':\n        \"\"\"Creates an empty IntervalBatch.\n\n        Returns:\n            An empty ``IntervalBatch``.\n        \"\"\"\n        return cls.zeros(0)\n\n    @classmethod\n    def build(cls, *intervals: Union[Interval, Iterable[Interval]]) -&gt; 'IntervalBatch':\n        \"\"\"Creates an IntervalBatch from an iterable of Interval objects (or varargs).\n\n        Args:\n            *intervals: Iterable of intervals or individual ``Interval`` arguments.\n\n        Returns:\n            A new ``IntervalBatch``.\n        \"\"\"\n        if not intervals: return cls.empty()\n        # Handle single iterable argument\n        if len(intervals) == 1 and isinstance(intervals[0], Iterable) and not isinstance(intervals[0], Interval):\n            intervals = intervals[0]\n\n        # Vectorized construction using list comprehension is generally faster than explicit loops\n        data = [(x._start, x._end, x._strand) for x in intervals]\n        arr = np.array(data, dtype=cls._DTYPE)\n        if len(arr) == 0: return cls.empty()\n        return cls(arr[:, 0], arr[:, 1], arr[:, 2])\n\n    @classmethod\n    def from_features(cls, *features) -&gt; 'IntervalBatch':\n        \"\"\"\n        Creates an IntervalBatch from a list of Feature objects.\n\n        Args:\n            *features: Variable number of Feature objects (must have .interval attribute).\n\n        Returns:\n            An IntervalBatch.\n        \"\"\"\n        if not features: return cls.empty()\n\n        # Handle single argument (list, batch, iterator)\n        if len(features) == 1:\n            obj = features[0]\n\n            # Fast Path: Container with intervals property (e.g. FeatureList, Record, FeatureBatch)\n            if batch := getattr(obj, 'intervals', None):\n                if isinstance(batch, cls): return batch\n\n            # 3. Unwrap Iterable if it's not a single Feature (Features are iterable)\n            if isinstance(obj, Iterable) and not hasattr(obj, 'interval'):\n                features = list(obj)\n\n        # Fallback: Iteration (Optimized for list of objects)\n        # We assume objects have .interval attribute (duck typing) for speed\n        n = len(features)\n        starts = np.empty(n, dtype=cls._DTYPE)\n        ends = np.empty(n, dtype=cls._DTYPE)\n        strands = np.empty(n, dtype=cls._DTYPE)\n        for i, f in enumerate(features):\n            iv = f.interval\n            starts[i] = iv._start\n            ends[i] = iv._end\n            strands[i] = iv._strand\n        return cls(starts, ends, strands)\n\n    @classmethod\n    def from_items(cls, *items: Union[slice, int, 'Interval', Match]) -&gt; 'IntervalBatch':\n        \"\"\"\n        Creates an IntervalBatch from various items.\n\n        Args:\n            *items: Items to convert to intervals.\n\n        Returns:\n            An IntervalBatch.\n        \"\"\"\n        if not items: return cls.empty()\n        arr = np.array([tuple(Interval.from_item(i)) for i in items], dtype=cls._DTYPE)\n        return cls(arr[:, 0], arr[:, 1], arr[:, 2])\n\n    @classmethod\n    def concat(cls, batches: Iterable['IntervalBatch']) -&gt; 'IntervalBatch':\n        \"\"\"Concatenates multiple IntervalBatches.\n\n        Args:\n            batches: Iterable of ``IntervalBatch`` objects.\n\n        Returns:\n            A new concatenated ``IntervalBatch``.\n        \"\"\"\n        batches = list(batches)\n        if not batches: return cls.empty()\n\n        starts = np.concatenate([b._starts for b in batches])\n        ends = np.concatenate([b._ends for b in batches])\n        strands = np.concatenate([b._strands for b in batches])\n\n        return cls(starts, ends, strands, sort=True)\n\n\n\n    def __repr__(self): return f\"&lt;IntervalBatch: {len(self)} intervals&gt;\"\n\n    def __len__(self): return len(self._starts)\n\n    def __getitem__(self, item):\n        if isinstance(item, (int, np.integer)):\n            return Interval(self._starts[item], self._ends[item], self._strands[item])\n        elif isinstance(item, (slice, np.ndarray, list)):\n            # Slicing or boolean masking a sorted batch preserves sort order\n            is_slice = isinstance(item, slice)\n            is_mask = isinstance(item, np.ndarray) and item.dtype == bool\n            return IntervalBatch(self._starts[item], self._ends[item], self._strands[item], \n                                 sort=not (is_slice or is_mask))\n        raise TypeError(f\"Invalid index type: {type(item)}\")\n\n    def filter(self, mask: Union[np.ndarray, Iterable, slice]) -&gt; 'IntervalBatch':\n        \"\"\"\n        Filters the batch using a boolean mask, integer indices, or slice.\n        Always returns an IntervalBatch.\n\n        Args:\n            mask: Boolean array, integer indices, slice, or iterable.\n\n        Returns:\n            A new IntervalBatch.\n        \"\"\"\n        if isinstance(mask, (slice, int, np.integer)):\n            if isinstance(mask, (int, np.integer)):\n                mask = [mask]\n            return self[mask]\n\n        # Ensure mask is a numpy array (handles lists of bools correctly as masks)\n        return self[np.asarray(mask)]\n\n    def __iter__(self):\n        for i in range(len(self)):\n            yield Interval(self._starts[i], self._ends[i], self._strands[i])\n\n    def copy(self) -&gt; 'IntervalBatch':\n        \"\"\"Returns a deep copy of the IntervalBatch.\"\"\"\n        return IntervalBatch(\n            self._starts.copy(), self._ends.copy(), self._strands.copy(),\n            self._original_indices.copy() if self._original_indices is not None else None,\n            sort=False\n        )\n\n    @property\n    def starts(self): return self._starts\n    @property\n    def ends(self): return self._ends\n    @property\n    def strands(self): return self._strands\n\n    @property\n    def component(self): return Interval\n\n\n    @property\n    def nbytes(self) -&gt; int:\n        return self._starts.nbytes + self._ends.nbytes + self._strands.nbytes + (self._original_indices.nbytes if self._original_indices is not None else 0)\n\n    @property\n    def centers(self) -&gt; np.ndarray:\n        \"\"\"Returns the center points of the intervals (float array).\"\"\"\n        return (self._starts + self._ends) / 2\n\n    @property\n    def lengths(self) -&gt; np.ndarray:\n        \"\"\"Returns the lengths of the intervals (int array).\"\"\"\n        return self._ends - self._starts\n\n    def query(self, start: int, end: int) -&gt; np.ndarray:\n        \"\"\"Returns indices of intervals overlapping [start, end).\"\"\"\n        if len(self) == 0: return np.empty(0, dtype=np.int32)\n\n        if self._original_indices is not None:\n            return _query_kernel(self.starts, self.ends, self._original_indices, start, end, self._max_len)\n\n        return _query_kernel_identity(self.starts, self.ends,\n                             start, end, self._max_len)\n\n    def intersect(self, other: 'IntervalBatch', stranded: bool = False) -&gt; 'IntervalBatch':\n        \"\"\"Computes the intersection with another IntervalBatch.\n\n        Args:\n            other: The other IntervalBatch.\n            stranded: If ``True``, only intersects intervals on the same strand.\n\n        Returns:\n            A new ``IntervalBatch`` representing the intersection.\n        \"\"\"\n        if len(self) == 0 or len(other) == 0: return IntervalBatch.empty()\n        # Call Numba Kernel\n        out = _intersect_kernel(self.starts, self.ends, self.strands,\n                                other.starts, other.ends, other.strands, stranded)\n        # Kernel returns tuple of arrays\n        if len(out[0]) == 0: return IntervalBatch.empty()\n        return IntervalBatch(out[0], out[1], out[2], sort=False)\n\n    def subtract(self, other: 'IntervalBatch', stranded: bool = False) -&gt; 'IntervalBatch':\n        \"\"\"Subtracts regions in ``other`` from this index.\n\n        Args:\n            other: The IntervalBatch to subtract.\n            stranded: If ``True``, only subtracts intervals on the same strand.\n\n        Returns:\n            A new ``IntervalBatch`` with regions removed.\n        \"\"\"\n        if len(other) == 0: return self.copy()\n        # Merge B to simplify subtraction\n        b_merged = other.merge()\n\n        out = _subtract_kernel(self.starts, self.ends, self.strands,\n                               b_merged.starts, b_merged.ends, b_merged.strands, stranded)\n        if len(out[0]) == 0: return IntervalBatch.empty()\n        return IntervalBatch(out[0], out[1], out[2], sort=False)\n\n    def promoters(self, upstream: int = 100, downstream: int = 0) -&gt; 'IntervalBatch':\n        \"\"\"\n        Extracts promoter regions relative to strand.\n        Forward: [Start - Up, Start + Down]\n        Reverse: [End - Down, End + Up]\n\n        Args:\n            upstream: Bases upstream of the start.\n            downstream: Bases downstream of the start.\n\n        Returns:\n            A new IntervalBatch of promoters.\n        \"\"\"\n        if len(self) == 0: return IntervalBatch.empty()\n\n        s = self._starts\n        e = self._ends\n        st = self._strands\n\n        new_s = np.empty_like(s)\n        new_e = np.empty_like(e)\n\n        # Forward (+ or 0)\n        mask_fwd = st &gt;= 0\n        if np.any(mask_fwd):\n            fwd_s = s[mask_fwd]\n            new_s[mask_fwd] = fwd_s - upstream\n            new_e[mask_fwd] = fwd_s + downstream\n\n        # Reverse (-)\n        mask_rev = ~mask_fwd\n        if np.any(mask_rev):\n            rev_e = e[mask_rev]\n            new_s[mask_rev] = rev_e - downstream\n            new_e[mask_rev] = rev_e + upstream\n\n        np.maximum(new_s, 0, out=new_s)\n        return IntervalBatch(new_s, new_e, st.copy(), sort=True)\n\n    def flank(self, left: int, right: int = None, direction: str = 'both') -&gt; 'IntervalBatch':\n        \"\"\"\n        Generates flanking regions.\n\n        Args:\n            left: Bases to the left (relative to direction).\n            right: Bases to the right (relative to direction).\n            direction: 'both', 'upstream', or 'downstream'.\n\n        Returns:\n            A new IntervalBatch.\n        \"\"\"\n        if right is None: right = left\n        if len(self) == 0: return IntervalBatch.empty()\n\n        # Map string direction to int for Numba\n        d_code = 0  # both\n        if direction == 'upstream': d_code = 1\n        elif direction == 'downstream': d_code = 2\n\n        # Deterministic output size calculation\n        factor = 2 if d_code == 0 else 1\n        n = len(self)\n        out_s = np.empty(n * factor, dtype=self._DTYPE)\n        out_e = np.empty(n * factor, dtype=self._DTYPE)\n        out_st = np.empty(n * factor, dtype=self._DTYPE)\n\n        _flank_kernel(self.starts, self.ends, self.strands, left, right, d_code, out_s, out_e, out_st)\n\n        # Filter empty intervals (length &lt;= 0) resulting from boundary clipping\n        mask = out_e &gt; out_s\n        if not np.all(mask):\n            return IntervalBatch(out_s[mask], out_e[mask], out_st[mask], sort=True)\n\n        return IntervalBatch(out_s, out_e, out_st, sort=True)\n\n    def jaccard(self, other: 'IntervalBatch') -&gt; float:\n        \"\"\"\n        Calculates Jaccard Index (Intersection / Union) in bp.\n        Useful for comparing gene predictions or annotations.\n\n        Args:\n            other: The other IntervalBatch.\n\n        Returns:\n            The Jaccard index (0.0 to 1.0).\n        \"\"\"\n        union_len = self.merge().coverage() + other.merge().coverage()\n        if union_len == 0: return 0.0\n\n        intersect_len = self.intersect(other).coverage()\n        # Union = A + B - Intersection\n        real_union = union_len - intersect_len\n\n        if real_union == 0: return 0.0\n        return intersect_len / real_union\n\n    def relate(self, other: Union['Interval', 'IntervalBatch']) -&gt; np.ndarray:\n        \"\"\"\n        Vectorized determination of spatial relationships.\n\n        Args:\n            other: An Interval or IntervalBatch.\n\n        Returns:\n            np.ndarray: Array of Context values (int32).\n        \"\"\"\n        if isinstance(other, Interval):\n            s2 = np.full(len(self), other.start, dtype=self._DTYPE)\n            e2 = np.full(len(self), other.end, dtype=self._DTYPE)\n        elif isinstance(other, IntervalBatch):\n            if len(self) != len(other):\n                raise ValueError(f\"Batch length mismatch: {len(self)} vs {len(other)}\")\n            s2 = other.starts\n            e2 = other.ends\n        else:\n            try:\n                iv = Interval.from_item(other)\n                s2 = np.full(len(self), iv.start, dtype=self._DTYPE)\n                e2 = np.full(len(self), iv.end, dtype=self._DTYPE)\n            except TypeError:\n                raise TypeError(f\"Cannot relate IntervalBatch with {type(other)}\")\n\n        return _relate_kernel(self.starts, self.ends, self.strands, s2, e2)\n\n    def merge(self, tolerance: int = 0) -&gt; 'IntervalBatch':\n        \"\"\"\n        Merges overlapping or adjacent intervals.\n\n        Args:\n            tolerance: Maximum distance between intervals to merge.\n\n        Returns:\n            A new merged IntervalBatch.\n        \"\"\"\n        if len(self) == 0: return self\n        # Note: self.sort() must be guaranteed before calling this kernel\n        # Since we sort on init, we are usually safe, but you might want to ensure it.\n\n        out = _merge_kernel(self.starts, self.ends, self.strands, tolerance)\n\n        return IntervalBatch(out[0], out[1], out[2], sort=False)\n\n    def tile(self, width: int, step: int = None) -&gt; 'IntervalBatch':\n        \"\"\"\n        Splits intervals into sliding windows of 'width'.\n        Keeps windows strictly INSIDE the original intervals.\n\n        Args:\n            width: Window width.\n            step: Step size (defaults to width).\n\n        Returns:\n            A new IntervalBatch of tiles.\n        \"\"\"\n        if step is None: step = width\n        if len(self) == 0: return IntervalBatch.empty()\n\n        # Pass 1: Count tiles per interval (Parallel)\n        counts = _tile_count_kernel(self.starts, self.ends, width, step)\n        total_tiles = counts.sum()\n        if total_tiles == 0: return IntervalBatch.empty()\n\n        # Calculate Offsets\n        offsets = np.zeros(len(counts), dtype=np.int32)\n        np.cumsum(counts[:-1], out=offsets[1:])\n\n        # Pass 2: Fill (Parallel)\n        out_s = np.empty(total_tiles, dtype=self._DTYPE)\n        out_e = np.empty(total_tiles, dtype=self._DTYPE)\n        out_st = np.empty(total_tiles, dtype=self._DTYPE)\n\n        _tile_fill_kernel(self.starts, self.ends, self.strands, width, step, offsets, out_s, out_e, out_st)\n\n        return IntervalBatch(out_s, out_e, out_st, sort=False)\n\n    def coverage(self) -&gt; int:\n        \"\"\"Returns total unique bases covered.\"\"\"\n        return _coverage_kernel(self.starts, self.ends)\n\n    def pad(self, upstream: int, downstream: int = None) -&gt; 'IntervalBatch':\n        \"\"\"\n        Expands intervals by 'upstream' and 'downstream' bp.\n        Respects strand (upstream is 5', downstream is 3').\n\n        Args:\n            upstream: Bases to add upstream.\n            downstream: Bases to add downstream (defaults to upstream).\n\n        Returns:\n            A new padded IntervalBatch.\n        \"\"\"\n        if downstream is None: downstream = upstream\n        if len(self) == 0: return self\n\n        # Copy data to avoid mutating the current index\n        s, e, st = self._starts.copy(), self._ends.copy(), self._strands.copy()\n\n        # 1. Forward/Unstranded (+ or 0): Start - Up, End + Down\n        mask_fwd = st &gt;= 0\n        s[mask_fwd] -= upstream\n        e[mask_fwd] += downstream\n\n        # 2. Reverse (-): Start - Down, End + Up\n        mask_rev = st &lt; 0\n        s[mask_rev] -= downstream\n        e[mask_rev] += upstream\n\n        # 3. Clip negative values to 0\n        np.maximum(s, 0, out=s)\n\n        # 4. Re-sort required?\n        # Padding can cause re-ordering (e.g. a small interval expanding past a large one)\n        # It's safest to create a new index which enforces sort/validation\n        return IntervalBatch(s, e, st, sort=True)\n\n    def reverse_complement(self, length: Union[int, np.ndarray]) -&gt; 'IntervalBatch':\n        \"\"\"\n        Returns a new IntervalBatch on the reverse complement strand.\n\n        Args:\n            length: Length of the parent sequence(s). Can be a scalar or an array.\n\n        Returns:\n            A new IntervalBatch.\n        \"\"\"\n        if len(self) == 0: return IntervalBatch.empty()\n        return IntervalBatch(length - self._ends, length - self._starts, self._strands * -1, sort=True)\n\n    def complement(self, length: int = None) -&gt; 'IntervalBatch':\n        \"\"\"\n        Returns the 'gaps' in the index up to 'length'.\n        Essential for finding intergenic regions.\n\n        Args:\n            length: The total length of the region (e.g., genome size).\n                    If None, defaults to the maximum end position in the batch.\n\n        Returns:\n            A new IntervalBatch representing the gaps.\n        \"\"\"\n        if len(self) == 0:\n            if length is None: return IntervalBatch.empty()\n            return IntervalBatch.build(Interval(0, length))\n\n        # Merge first to remove overlaps and sort\n        merged = self.merge()\n\n        if length is None:\n            length = merged.ends[-1]\n\n        out = _complement_kernel(merged.starts, merged.ends, length)\n        return IntervalBatch(out[0], out[1], out[2], sort=False)\n\n    def span(self) -&gt; int:\n        \"\"\"Returns the distance from the very start to the very end.\"\"\"\n        if len(self) == 0: return 0\n        # Starts are sorted, but ends are not necessarily sorted by the last element\n        return np.max(self._ends) - self._starts[0]\n\n    def cover_linear(self, length: int = None, min_coverage: float = 0.0, max_overlap: float = 0.1) -&gt; np.ndarray:\n        \"\"\"\n        Identifies a linear chain of intervals that covers the range [0, length] with minimal overlap.\n        Useful for scaffolding or filtering overlapping features.\n\n        Args:\n            length: The total length of the region (e.g. contig length). Required if min_coverage &gt; 0.\n            min_coverage: Minimum fraction of 'length' covered by the chain.\n            max_overlap: Maximum allowed fractional overlap between adjacent intervals.\n\n        Returns:\n            Indices of the selected intervals (relative to the original unsorted input).\n        \"\"\"\n        kept_sorted_indices = _cover_linear_kernel(self.starts, self.ends, length or 0, min_coverage, max_overlap)\n        if self._original_indices is not None: return self._original_indices[kept_sorted_indices]\n        return kept_sorted_indices\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.centers","title":"<code>centers</code>  <code>property</code>","text":"<p>Returns the center points of the intervals (float array).</p>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.lengths","title":"<code>lengths</code>  <code>property</code>","text":"<p>Returns the lengths of the intervals (int array).</p>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.__init__","title":"<code>__init__(starts=None, ends=None, strands=None, original_indices=None, sort=True)</code>","text":"<p>Initializes an IntervalBatch.</p> <p>Parameters:</p> Name Type Description Default <code>starts</code> <code>ndarray</code> <p>Array of start positions.</p> <code>None</code> <code>ends</code> <code>ndarray</code> <p>Array of end positions.</p> <code>None</code> <code>strands</code> <code>ndarray</code> <p>Array of strands.</p> <code>None</code> <code>original_indices</code> <code>ndarray</code> <p>Array of original indices (for tracking after sort).</p> <code>None</code> <code>sort</code> <code>bool</code> <p>Whether to sort the intervals (default: True).</p> <code>True</code> Source code in <code>baclib/core/interval.py</code> <pre><code>def __init__(self, starts: np.ndarray = None, ends: np.ndarray = None, strands: np.ndarray = None,\n             original_indices: np.ndarray = None, sort: bool = True):\n    \"\"\"\n    Initializes an IntervalBatch.\n\n    Args:\n        starts: Array of start positions.\n        ends: Array of end positions.\n        strands: Array of strands.\n        original_indices: Array of original indices (for tracking after sort).\n        sort: Whether to sort the intervals (default: True).\n    \"\"\"\n    if starts is None:\n        self._starts = np.empty(0, dtype=self._DTYPE)\n        self._ends = np.empty(0, dtype=self._DTYPE)\n        self._strands = np.empty(0, dtype=self._DTYPE)\n        self._max_len = 0\n    else:\n        # Ensure contiguous arrays for Numba performance\n        self._starts = np.ascontiguousarray(starts, dtype=self._DTYPE)\n        self._ends = np.ascontiguousarray(ends, dtype=self._DTYPE)\n        self._strands = np.ascontiguousarray(strands, dtype=self._DTYPE) if strands is not None else np.zeros(len(starts), dtype=self._DTYPE)\n        # Calculate max length for query optimization\n        self._max_len = np.max(self._ends - self._starts) if len(self._starts) &gt; 0 else 0\n\n    self._original_indices = original_indices\n    if sort: self.sort()\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.build","title":"<code>build(*intervals)</code>  <code>classmethod</code>","text":"<p>Creates an IntervalBatch from an iterable of Interval objects (or varargs).</p> <p>Parameters:</p> Name Type Description Default <code>*intervals</code> <code>Union[Interval, Iterable[Interval]]</code> <p>Iterable of intervals or individual <code>Interval</code> arguments.</p> <code>()</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new <code>IntervalBatch</code>.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef build(cls, *intervals: Union[Interval, Iterable[Interval]]) -&gt; 'IntervalBatch':\n    \"\"\"Creates an IntervalBatch from an iterable of Interval objects (or varargs).\n\n    Args:\n        *intervals: Iterable of intervals or individual ``Interval`` arguments.\n\n    Returns:\n        A new ``IntervalBatch``.\n    \"\"\"\n    if not intervals: return cls.empty()\n    # Handle single iterable argument\n    if len(intervals) == 1 and isinstance(intervals[0], Iterable) and not isinstance(intervals[0], Interval):\n        intervals = intervals[0]\n\n    # Vectorized construction using list comprehension is generally faster than explicit loops\n    data = [(x._start, x._end, x._strand) for x in intervals]\n    arr = np.array(data, dtype=cls._DTYPE)\n    if len(arr) == 0: return cls.empty()\n    return cls(arr[:, 0], arr[:, 1], arr[:, 2])\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.complement","title":"<code>complement(length=None)</code>","text":"<p>Returns the 'gaps' in the index up to 'length'. Essential for finding intergenic regions.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>The total length of the region (e.g., genome size).     If None, defaults to the maximum end position in the batch.</p> <code>None</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new IntervalBatch representing the gaps.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def complement(self, length: int = None) -&gt; 'IntervalBatch':\n    \"\"\"\n    Returns the 'gaps' in the index up to 'length'.\n    Essential for finding intergenic regions.\n\n    Args:\n        length: The total length of the region (e.g., genome size).\n                If None, defaults to the maximum end position in the batch.\n\n    Returns:\n        A new IntervalBatch representing the gaps.\n    \"\"\"\n    if len(self) == 0:\n        if length is None: return IntervalBatch.empty()\n        return IntervalBatch.build(Interval(0, length))\n\n    # Merge first to remove overlaps and sort\n    merged = self.merge()\n\n    if length is None:\n        length = merged.ends[-1]\n\n    out = _complement_kernel(merged.starts, merged.ends, length)\n    return IntervalBatch(out[0], out[1], out[2], sort=False)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.concat","title":"<code>concat(batches)</code>  <code>classmethod</code>","text":"<p>Concatenates multiple IntervalBatches.</p> <p>Parameters:</p> Name Type Description Default <code>batches</code> <code>Iterable[IntervalBatch]</code> <p>Iterable of <code>IntervalBatch</code> objects.</p> required <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new concatenated <code>IntervalBatch</code>.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef concat(cls, batches: Iterable['IntervalBatch']) -&gt; 'IntervalBatch':\n    \"\"\"Concatenates multiple IntervalBatches.\n\n    Args:\n        batches: Iterable of ``IntervalBatch`` objects.\n\n    Returns:\n        A new concatenated ``IntervalBatch``.\n    \"\"\"\n    batches = list(batches)\n    if not batches: return cls.empty()\n\n    starts = np.concatenate([b._starts for b in batches])\n    ends = np.concatenate([b._ends for b in batches])\n    strands = np.concatenate([b._strands for b in batches])\n\n    return cls(starts, ends, strands, sort=True)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.copy","title":"<code>copy()</code>","text":"<p>Returns a deep copy of the IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def copy(self) -&gt; 'IntervalBatch':\n    \"\"\"Returns a deep copy of the IntervalBatch.\"\"\"\n    return IntervalBatch(\n        self._starts.copy(), self._ends.copy(), self._strands.copy(),\n        self._original_indices.copy() if self._original_indices is not None else None,\n        sort=False\n    )\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.cover_linear","title":"<code>cover_linear(length=None, min_coverage=0.0, max_overlap=0.1)</code>","text":"<p>Identifies a linear chain of intervals that covers the range [0, length] with minimal overlap. Useful for scaffolding or filtering overlapping features.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>int</code> <p>The total length of the region (e.g. contig length). Required if min_coverage &gt; 0.</p> <code>None</code> <code>min_coverage</code> <code>float</code> <p>Minimum fraction of 'length' covered by the chain.</p> <code>0.0</code> <code>max_overlap</code> <code>float</code> <p>Maximum allowed fractional overlap between adjacent intervals.</p> <code>0.1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Indices of the selected intervals (relative to the original unsorted input).</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def cover_linear(self, length: int = None, min_coverage: float = 0.0, max_overlap: float = 0.1) -&gt; np.ndarray:\n    \"\"\"\n    Identifies a linear chain of intervals that covers the range [0, length] with minimal overlap.\n    Useful for scaffolding or filtering overlapping features.\n\n    Args:\n        length: The total length of the region (e.g. contig length). Required if min_coverage &gt; 0.\n        min_coverage: Minimum fraction of 'length' covered by the chain.\n        max_overlap: Maximum allowed fractional overlap between adjacent intervals.\n\n    Returns:\n        Indices of the selected intervals (relative to the original unsorted input).\n    \"\"\"\n    kept_sorted_indices = _cover_linear_kernel(self.starts, self.ends, length or 0, min_coverage, max_overlap)\n    if self._original_indices is not None: return self._original_indices[kept_sorted_indices]\n    return kept_sorted_indices\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.coverage","title":"<code>coverage()</code>","text":"<p>Returns total unique bases covered.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def coverage(self) -&gt; int:\n    \"\"\"Returns total unique bases covered.\"\"\"\n    return _coverage_kernel(self.starts, self.ends)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.empty","title":"<code>empty()</code>  <code>classmethod</code>","text":"<p>Creates an empty IntervalBatch.</p> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An empty <code>IntervalBatch</code>.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef empty(cls) -&gt; 'IntervalBatch':\n    \"\"\"Creates an empty IntervalBatch.\n\n    Returns:\n        An empty ``IntervalBatch``.\n    \"\"\"\n    return cls.zeros(0)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.filter","title":"<code>filter(mask)</code>","text":"<p>Filters the batch using a boolean mask, integer indices, or slice. Always returns an IntervalBatch.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Union[ndarray, Iterable, slice]</code> <p>Boolean array, integer indices, slice, or iterable.</p> required <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def filter(self, mask: Union[np.ndarray, Iterable, slice]) -&gt; 'IntervalBatch':\n    \"\"\"\n    Filters the batch using a boolean mask, integer indices, or slice.\n    Always returns an IntervalBatch.\n\n    Args:\n        mask: Boolean array, integer indices, slice, or iterable.\n\n    Returns:\n        A new IntervalBatch.\n    \"\"\"\n    if isinstance(mask, (slice, int, np.integer)):\n        if isinstance(mask, (int, np.integer)):\n            mask = [mask]\n        return self[mask]\n\n    # Ensure mask is a numpy array (handles lists of bools correctly as masks)\n    return self[np.asarray(mask)]\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.flank","title":"<code>flank(left, right=None, direction='both')</code>","text":"<p>Generates flanking regions.</p> <p>Parameters:</p> Name Type Description Default <code>left</code> <code>int</code> <p>Bases to the left (relative to direction).</p> required <code>right</code> <code>int</code> <p>Bases to the right (relative to direction).</p> <code>None</code> <code>direction</code> <code>str</code> <p>'both', 'upstream', or 'downstream'.</p> <code>'both'</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def flank(self, left: int, right: int = None, direction: str = 'both') -&gt; 'IntervalBatch':\n    \"\"\"\n    Generates flanking regions.\n\n    Args:\n        left: Bases to the left (relative to direction).\n        right: Bases to the right (relative to direction).\n        direction: 'both', 'upstream', or 'downstream'.\n\n    Returns:\n        A new IntervalBatch.\n    \"\"\"\n    if right is None: right = left\n    if len(self) == 0: return IntervalBatch.empty()\n\n    # Map string direction to int for Numba\n    d_code = 0  # both\n    if direction == 'upstream': d_code = 1\n    elif direction == 'downstream': d_code = 2\n\n    # Deterministic output size calculation\n    factor = 2 if d_code == 0 else 1\n    n = len(self)\n    out_s = np.empty(n * factor, dtype=self._DTYPE)\n    out_e = np.empty(n * factor, dtype=self._DTYPE)\n    out_st = np.empty(n * factor, dtype=self._DTYPE)\n\n    _flank_kernel(self.starts, self.ends, self.strands, left, right, d_code, out_s, out_e, out_st)\n\n    # Filter empty intervals (length &lt;= 0) resulting from boundary clipping\n    mask = out_e &gt; out_s\n    if not np.all(mask):\n        return IntervalBatch(out_s[mask], out_e[mask], out_st[mask], sort=True)\n\n    return IntervalBatch(out_s, out_e, out_st, sort=True)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.from_features","title":"<code>from_features(*features)</code>  <code>classmethod</code>","text":"<p>Creates an IntervalBatch from a list of Feature objects.</p> <p>Parameters:</p> Name Type Description Default <code>*features</code> <p>Variable number of Feature objects (must have .interval attribute).</p> <code>()</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef from_features(cls, *features) -&gt; 'IntervalBatch':\n    \"\"\"\n    Creates an IntervalBatch from a list of Feature objects.\n\n    Args:\n        *features: Variable number of Feature objects (must have .interval attribute).\n\n    Returns:\n        An IntervalBatch.\n    \"\"\"\n    if not features: return cls.empty()\n\n    # Handle single argument (list, batch, iterator)\n    if len(features) == 1:\n        obj = features[0]\n\n        # Fast Path: Container with intervals property (e.g. FeatureList, Record, FeatureBatch)\n        if batch := getattr(obj, 'intervals', None):\n            if isinstance(batch, cls): return batch\n\n        # 3. Unwrap Iterable if it's not a single Feature (Features are iterable)\n        if isinstance(obj, Iterable) and not hasattr(obj, 'interval'):\n            features = list(obj)\n\n    # Fallback: Iteration (Optimized for list of objects)\n    # We assume objects have .interval attribute (duck typing) for speed\n    n = len(features)\n    starts = np.empty(n, dtype=cls._DTYPE)\n    ends = np.empty(n, dtype=cls._DTYPE)\n    strands = np.empty(n, dtype=cls._DTYPE)\n    for i, f in enumerate(features):\n        iv = f.interval\n        starts[i] = iv._start\n        ends[i] = iv._end\n        strands[i] = iv._strand\n    return cls(starts, ends, strands)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.from_items","title":"<code>from_items(*items)</code>  <code>classmethod</code>","text":"<p>Creates an IntervalBatch from various items.</p> <p>Parameters:</p> Name Type Description Default <code>*items</code> <code>Union[slice, int, Interval, Match]</code> <p>Items to convert to intervals.</p> <code>()</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef from_items(cls, *items: Union[slice, int, 'Interval', Match]) -&gt; 'IntervalBatch':\n    \"\"\"\n    Creates an IntervalBatch from various items.\n\n    Args:\n        *items: Items to convert to intervals.\n\n    Returns:\n        An IntervalBatch.\n    \"\"\"\n    if not items: return cls.empty()\n    arr = np.array([tuple(Interval.from_item(i)) for i in items], dtype=cls._DTYPE)\n    return cls(arr[:, 0], arr[:, 1], arr[:, 2])\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.intersect","title":"<code>intersect(other, stranded=False)</code>","text":"<p>Computes the intersection with another IntervalBatch.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>IntervalBatch</code> <p>The other IntervalBatch.</p> required <code>stranded</code> <code>bool</code> <p>If <code>True</code>, only intersects intervals on the same strand.</p> <code>False</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new <code>IntervalBatch</code> representing the intersection.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def intersect(self, other: 'IntervalBatch', stranded: bool = False) -&gt; 'IntervalBatch':\n    \"\"\"Computes the intersection with another IntervalBatch.\n\n    Args:\n        other: The other IntervalBatch.\n        stranded: If ``True``, only intersects intervals on the same strand.\n\n    Returns:\n        A new ``IntervalBatch`` representing the intersection.\n    \"\"\"\n    if len(self) == 0 or len(other) == 0: return IntervalBatch.empty()\n    # Call Numba Kernel\n    out = _intersect_kernel(self.starts, self.ends, self.strands,\n                            other.starts, other.ends, other.strands, stranded)\n    # Kernel returns tuple of arrays\n    if len(out[0]) == 0: return IntervalBatch.empty()\n    return IntervalBatch(out[0], out[1], out[2], sort=False)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.jaccard","title":"<code>jaccard(other)</code>","text":"<p>Calculates Jaccard Index (Intersection / Union) in bp. Useful for comparing gene predictions or annotations.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>IntervalBatch</code> <p>The other IntervalBatch.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The Jaccard index (0.0 to 1.0).</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def jaccard(self, other: 'IntervalBatch') -&gt; float:\n    \"\"\"\n    Calculates Jaccard Index (Intersection / Union) in bp.\n    Useful for comparing gene predictions or annotations.\n\n    Args:\n        other: The other IntervalBatch.\n\n    Returns:\n        The Jaccard index (0.0 to 1.0).\n    \"\"\"\n    union_len = self.merge().coverage() + other.merge().coverage()\n    if union_len == 0: return 0.0\n\n    intersect_len = self.intersect(other).coverage()\n    # Union = A + B - Intersection\n    real_union = union_len - intersect_len\n\n    if real_union == 0: return 0.0\n    return intersect_len / real_union\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.merge","title":"<code>merge(tolerance=0)</code>","text":"<p>Merges overlapping or adjacent intervals.</p> <p>Parameters:</p> Name Type Description Default <code>tolerance</code> <code>int</code> <p>Maximum distance between intervals to merge.</p> <code>0</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new merged IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def merge(self, tolerance: int = 0) -&gt; 'IntervalBatch':\n    \"\"\"\n    Merges overlapping or adjacent intervals.\n\n    Args:\n        tolerance: Maximum distance between intervals to merge.\n\n    Returns:\n        A new merged IntervalBatch.\n    \"\"\"\n    if len(self) == 0: return self\n    # Note: self.sort() must be guaranteed before calling this kernel\n    # Since we sort on init, we are usually safe, but you might want to ensure it.\n\n    out = _merge_kernel(self.starts, self.ends, self.strands, tolerance)\n\n    return IntervalBatch(out[0], out[1], out[2], sort=False)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.pad","title":"<code>pad(upstream, downstream=None)</code>","text":"<p>Expands intervals by 'upstream' and 'downstream' bp. Respects strand (upstream is 5', downstream is 3').</p> <p>Parameters:</p> Name Type Description Default <code>upstream</code> <code>int</code> <p>Bases to add upstream.</p> required <code>downstream</code> <code>int</code> <p>Bases to add downstream (defaults to upstream).</p> <code>None</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new padded IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def pad(self, upstream: int, downstream: int = None) -&gt; 'IntervalBatch':\n    \"\"\"\n    Expands intervals by 'upstream' and 'downstream' bp.\n    Respects strand (upstream is 5', downstream is 3').\n\n    Args:\n        upstream: Bases to add upstream.\n        downstream: Bases to add downstream (defaults to upstream).\n\n    Returns:\n        A new padded IntervalBatch.\n    \"\"\"\n    if downstream is None: downstream = upstream\n    if len(self) == 0: return self\n\n    # Copy data to avoid mutating the current index\n    s, e, st = self._starts.copy(), self._ends.copy(), self._strands.copy()\n\n    # 1. Forward/Unstranded (+ or 0): Start - Up, End + Down\n    mask_fwd = st &gt;= 0\n    s[mask_fwd] -= upstream\n    e[mask_fwd] += downstream\n\n    # 2. Reverse (-): Start - Down, End + Up\n    mask_rev = st &lt; 0\n    s[mask_rev] -= downstream\n    e[mask_rev] += upstream\n\n    # 3. Clip negative values to 0\n    np.maximum(s, 0, out=s)\n\n    # 4. Re-sort required?\n    # Padding can cause re-ordering (e.g. a small interval expanding past a large one)\n    # It's safest to create a new index which enforces sort/validation\n    return IntervalBatch(s, e, st, sort=True)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.promoters","title":"<code>promoters(upstream=100, downstream=0)</code>","text":"<p>Extracts promoter regions relative to strand. Forward: [Start - Up, Start + Down] Reverse: [End - Down, End + Up]</p> <p>Parameters:</p> Name Type Description Default <code>upstream</code> <code>int</code> <p>Bases upstream of the start.</p> <code>100</code> <code>downstream</code> <code>int</code> <p>Bases downstream of the start.</p> <code>0</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new IntervalBatch of promoters.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def promoters(self, upstream: int = 100, downstream: int = 0) -&gt; 'IntervalBatch':\n    \"\"\"\n    Extracts promoter regions relative to strand.\n    Forward: [Start - Up, Start + Down]\n    Reverse: [End - Down, End + Up]\n\n    Args:\n        upstream: Bases upstream of the start.\n        downstream: Bases downstream of the start.\n\n    Returns:\n        A new IntervalBatch of promoters.\n    \"\"\"\n    if len(self) == 0: return IntervalBatch.empty()\n\n    s = self._starts\n    e = self._ends\n    st = self._strands\n\n    new_s = np.empty_like(s)\n    new_e = np.empty_like(e)\n\n    # Forward (+ or 0)\n    mask_fwd = st &gt;= 0\n    if np.any(mask_fwd):\n        fwd_s = s[mask_fwd]\n        new_s[mask_fwd] = fwd_s - upstream\n        new_e[mask_fwd] = fwd_s + downstream\n\n    # Reverse (-)\n    mask_rev = ~mask_fwd\n    if np.any(mask_rev):\n        rev_e = e[mask_rev]\n        new_s[mask_rev] = rev_e - downstream\n        new_e[mask_rev] = rev_e + upstream\n\n    np.maximum(new_s, 0, out=new_s)\n    return IntervalBatch(new_s, new_e, st.copy(), sort=True)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.query","title":"<code>query(start, end)</code>","text":"<p>Returns indices of intervals overlapping [start, end).</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def query(self, start: int, end: int) -&gt; np.ndarray:\n    \"\"\"Returns indices of intervals overlapping [start, end).\"\"\"\n    if len(self) == 0: return np.empty(0, dtype=np.int32)\n\n    if self._original_indices is not None:\n        return _query_kernel(self.starts, self.ends, self._original_indices, start, end, self._max_len)\n\n    return _query_kernel_identity(self.starts, self.ends,\n                         start, end, self._max_len)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.random","title":"<code>random(n, rng=None, length=None, min_len=1, max_len=1000, min_start=0, max_start=1000000)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n random intervals.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of intervals to generate.</p> required <code>rng</code> <code>Generator</code> <p>Random number generator (optional).</p> <code>None</code> <code>length</code> <code>int</code> <p>Fixed length for all intervals (optional).</p> <code>None</code> <code>min_len</code> <code>int</code> <p>Minimum length (default: 1).</p> <code>1</code> <code>max_len</code> <code>int</code> <p>Maximum length (default: 1000).</p> <code>1000</code> <code>min_start</code> <code>int</code> <p>Minimum start position (default: 0).</p> <code>0</code> <code>max_start</code> <code>int</code> <p>Maximum start position (default: 1,000,000).</p> <code>1000000</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>An IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef random(cls, n: int, rng: np.random.Generator = None, length: int = None, min_len: int = 1, max_len: int = 1000,\n           min_start: int = 0, max_start: int = 1_000_000) -&gt; 'IntervalBatch':\n    \"\"\"\n    Creates a batch of n random intervals.\n\n    Args:\n        n: Number of intervals to generate.\n        rng: Random number generator (optional).\n        length: Fixed length for all intervals (optional).\n        min_len: Minimum length (default: 1).\n        max_len: Maximum length (default: 1000).\n        min_start: Minimum start position (default: 0).\n        max_start: Maximum start position (default: 1,000,000).\n\n    Returns:\n        An IntervalBatch.\n    \"\"\"\n    if rng is None: rng = RESOURCES.rng\n    if n &lt;= 0: return cls.empty()\n\n    # 1. Generate Lengths\n    if length is not None:\n         lengths = np.full(n, length, dtype=np.int32)\n    else:\n         lengths = rng.integers(min_len, max_len, size=n, dtype=np.int32)\n\n    # 2. Generate Starts\n    # We treat max_start as the upper bound for the start coordinate (exclusive).\n    if max_start &lt;= min_start:\n         raise ValueError(f\"max_start ({max_start}) must be &gt; min_start ({min_start})\")\n\n    starts = rng.integers(min_start, max_start, size=n, dtype=np.int32)\n\n    # 3. Generate Ends\n    ends = starts + lengths\n\n    # 4. Generate Strands (-1, 0, 1)\n    strands = rng.choice([-1, 0, 1], size=n).astype(np.int32)\n\n    return cls(starts, ends, strands, sort=True)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.relate","title":"<code>relate(other)</code>","text":"<p>Vectorized determination of spatial relationships.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Union[Interval, IntervalBatch]</code> <p>An Interval or IntervalBatch.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of Context values (int32).</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def relate(self, other: Union['Interval', 'IntervalBatch']) -&gt; np.ndarray:\n    \"\"\"\n    Vectorized determination of spatial relationships.\n\n    Args:\n        other: An Interval or IntervalBatch.\n\n    Returns:\n        np.ndarray: Array of Context values (int32).\n    \"\"\"\n    if isinstance(other, Interval):\n        s2 = np.full(len(self), other.start, dtype=self._DTYPE)\n        e2 = np.full(len(self), other.end, dtype=self._DTYPE)\n    elif isinstance(other, IntervalBatch):\n        if len(self) != len(other):\n            raise ValueError(f\"Batch length mismatch: {len(self)} vs {len(other)}\")\n        s2 = other.starts\n        e2 = other.ends\n    else:\n        try:\n            iv = Interval.from_item(other)\n            s2 = np.full(len(self), iv.start, dtype=self._DTYPE)\n            e2 = np.full(len(self), iv.end, dtype=self._DTYPE)\n        except TypeError:\n            raise TypeError(f\"Cannot relate IntervalBatch with {type(other)}\")\n\n    return _relate_kernel(self.starts, self.ends, self.strands, s2, e2)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.reverse_complement","title":"<code>reverse_complement(length)</code>","text":"<p>Returns a new IntervalBatch on the reverse complement strand.</p> <p>Parameters:</p> Name Type Description Default <code>length</code> <code>Union[int, ndarray]</code> <p>Length of the parent sequence(s). Can be a scalar or an array.</p> required <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new IntervalBatch.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def reverse_complement(self, length: Union[int, np.ndarray]) -&gt; 'IntervalBatch':\n    \"\"\"\n    Returns a new IntervalBatch on the reverse complement strand.\n\n    Args:\n        length: Length of the parent sequence(s). Can be a scalar or an array.\n\n    Returns:\n        A new IntervalBatch.\n    \"\"\"\n    if len(self) == 0: return IntervalBatch.empty()\n    return IntervalBatch(length - self._ends, length - self._starts, self._strands * -1, sort=True)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.sort","title":"<code>sort()</code>","text":"<p>Sorts the intervals by start position, then end position.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def sort(self):\n    \"\"\"Sorts the intervals by start position, then end position.\"\"\"\n    # Sort both data and the index tracker\n    if len(self._starts) &gt; 1:\n        # Optimization: Check if already sorted to avoid expensive lexsort\n        if _is_sorted_kernel(self._starts, self._ends): return\n\n        # Lexsort: Primary key is last in the tuple (starts), Secondary is ends\n        order = np.lexsort((self._ends, self._starts))\n\n        self._starts = self._starts[order]\n        self._ends = self._ends[order]\n        self._strands = self._strands[order]\n\n        if self._original_indices is not None:\n            self._original_indices = self._original_indices[order]\n        else:\n            # Create the mapping only now that we know we are scrambling the order\n            self._original_indices = order.astype(np.int32)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.span","title":"<code>span()</code>","text":"<p>Returns the distance from the very start to the very end.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def span(self) -&gt; int:\n    \"\"\"Returns the distance from the very start to the very end.\"\"\"\n    if len(self) == 0: return 0\n    # Starts are sorted, but ends are not necessarily sorted by the last element\n    return np.max(self._ends) - self._starts[0]\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.subtract","title":"<code>subtract(other, stranded=False)</code>","text":"<p>Subtracts regions in <code>other</code> from this index.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>IntervalBatch</code> <p>The IntervalBatch to subtract.</p> required <code>stranded</code> <code>bool</code> <p>If <code>True</code>, only subtracts intervals on the same strand.</p> <code>False</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new <code>IntervalBatch</code> with regions removed.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def subtract(self, other: 'IntervalBatch', stranded: bool = False) -&gt; 'IntervalBatch':\n    \"\"\"Subtracts regions in ``other`` from this index.\n\n    Args:\n        other: The IntervalBatch to subtract.\n        stranded: If ``True``, only subtracts intervals on the same strand.\n\n    Returns:\n        A new ``IntervalBatch`` with regions removed.\n    \"\"\"\n    if len(other) == 0: return self.copy()\n    # Merge B to simplify subtraction\n    b_merged = other.merge()\n\n    out = _subtract_kernel(self.starts, self.ends, self.strands,\n                           b_merged.starts, b_merged.ends, b_merged.strands, stranded)\n    if len(out[0]) == 0: return IntervalBatch.empty()\n    return IntervalBatch(out[0], out[1], out[2], sort=False)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.tile","title":"<code>tile(width, step=None)</code>","text":"<p>Splits intervals into sliding windows of 'width'. Keeps windows strictly INSIDE the original intervals.</p> <p>Parameters:</p> Name Type Description Default <code>width</code> <code>int</code> <p>Window width.</p> required <code>step</code> <code>int</code> <p>Step size (defaults to width).</p> <code>None</code> <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new IntervalBatch of tiles.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>def tile(self, width: int, step: int = None) -&gt; 'IntervalBatch':\n    \"\"\"\n    Splits intervals into sliding windows of 'width'.\n    Keeps windows strictly INSIDE the original intervals.\n\n    Args:\n        width: Window width.\n        step: Step size (defaults to width).\n\n    Returns:\n        A new IntervalBatch of tiles.\n    \"\"\"\n    if step is None: step = width\n    if len(self) == 0: return IntervalBatch.empty()\n\n    # Pass 1: Count tiles per interval (Parallel)\n    counts = _tile_count_kernel(self.starts, self.ends, width, step)\n    total_tiles = counts.sum()\n    if total_tiles == 0: return IntervalBatch.empty()\n\n    # Calculate Offsets\n    offsets = np.zeros(len(counts), dtype=np.int32)\n    np.cumsum(counts[:-1], out=offsets[1:])\n\n    # Pass 2: Fill (Parallel)\n    out_s = np.empty(total_tiles, dtype=self._DTYPE)\n    out_e = np.empty(total_tiles, dtype=self._DTYPE)\n    out_st = np.empty(total_tiles, dtype=self._DTYPE)\n\n    _tile_fill_kernel(self.starts, self.ends, self.strands, width, step, offsets, out_s, out_e, out_st)\n\n    return IntervalBatch(out_s, out_e, out_st, sort=False)\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.IntervalBatch.zeros","title":"<code>zeros(n)</code>  <code>classmethod</code>","text":"<p>Creates a batch of n 0-length intervals at position 0.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of intervals.</p> required <p>Returns:</p> Type Description <code>IntervalBatch</code> <p>A new <code>IntervalBatch</code>.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>@classmethod\ndef zeros(cls, n: int) -&gt; 'IntervalBatch':\n    \"\"\"Creates a batch of *n* 0-length intervals at position 0.\n\n    Args:\n        n: Number of intervals.\n\n    Returns:\n        A new ``IntervalBatch``.\n    \"\"\"\n    return cls(\n        np.zeros(n, dtype=np.int32),\n        np.zeros(n, dtype=np.int32),\n        np.ones(n, dtype=np.int32) # Default strand? Or 0? Or 1 (FORWARD)? Strand(1) is standard.\n    )\n</code></pre>"},{"location":"reference/baclib/core/interval/#baclib.core.interval.Strand","title":"<code>Strand</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Enumeration for genomic strands.</p> Source code in <code>baclib/core/interval.py</code> <pre><code>class Strand(IntEnum):\n    \"\"\"\n    Enumeration for genomic strands.\n    \"\"\"\n    FORWARD = 1\n    REVERSE = -1\n    UNSTRANDED = 0\n    _STR_CACHE: ClassVar[dict]\n    _BYTES_CACHE: ClassVar[dict]\n    _FROM_BYTES_CACHE: ClassVar[dict]\n\n    def __str__(self): return self._STR_CACHE[self]\n    @property\n    def bytes(self) -&gt; bytes: return self._BYTES_CACHE[self]\n\n    @classmethod\n    def from_bytes(cls, b: bytes) -&gt; 'Strand':\n        return cls._FROM_BYTES_CACHE.get(b, cls.UNSTRANDED)\n\n    @classmethod\n    def from_symbol(cls, s: Any) -&gt; 'Strand':\n        if s is None: return cls.UNSTRANDED\n        if isinstance(s, cls): return s\n        if isinstance(s, int):\n            try: return cls(s)\n            except ValueError: return cls.UNSTRANDED\n        if isinstance(s, bytes): return cls.from_bytes(s)\n        if isinstance(s, str): return cls.from_bytes(s.encode('ascii'))\n        return cls.UNSTRANDED\n\n    @classmethod\n    def _init_caches(cls):\n        cls._STR_CACHE = {cls.FORWARD: '+', cls.REVERSE: '-', cls.UNSTRANDED: '.'}\n        cls._BYTES_CACHE = {cls.FORWARD: b'+', cls.REVERSE: b'-', cls.UNSTRANDED: b'.'}\n        cls._FROM_BYTES_CACHE = { b'+': cls.FORWARD, b'-': cls.REVERSE, b'.': cls.UNSTRANDED}\n</code></pre>"},{"location":"reference/baclib/engines/","title":"engines","text":""},{"location":"reference/baclib/engines/#baclib.engines","title":"<code>baclib.engines</code>","text":"<p>Computational engines for alignment, indexing, motif discovery, and graph algorithms.</p>"},{"location":"reference/baclib/engines/index_py/","title":"index","text":""},{"location":"reference/baclib/engines/index_py/#baclib.engines.index","title":"<code>baclib.engines.index</code>","text":"<p>Sequence indexing engines: MinHash sketching and sparse-map minimizer/syncmer indexes.</p>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.BaseIndex","title":"<code>BaseIndex</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract Base Class for all Sequence Indexes. Acts as the configuration context for hashing (k, alphabet, masking).</p> <p>Attributes:</p> Name Type Description <code>k</code> <code>int</code> <p>K-mer length.</p> <code>alphabet</code> <code>Alphabet</code> <p>The alphabet used.</p> <code>canonical</code> <code>bool</code> <p>Whether to use canonical k-mers (min of forward/reverse).</p> Source code in <code>baclib/engines/index.py</code> <pre><code>class BaseIndex(ABC):\n    \"\"\"\n    Abstract Base Class for all Sequence Indexes.\n    Acts as the configuration context for hashing (k, alphabet, masking).\n\n    Attributes:\n        k (int): K-mer length.\n        alphabet (Alphabet): The alphabet used.\n        canonical (bool): Whether to use canonical k-mers (min of forward/reverse).\n    \"\"\"\n    __slots__ = ('_k', '_alphabet', '_canonical', '_built', '_bps', '_mask', '_dtype', '_rc_table')\n    _DEFAULT_ALPHABET = Alphabet.DNA\n    def __init__(self, k: int, alphabet: Alphabet = None, canonical: bool = True):\n        self._k = k\n        self._alphabet = alphabet or self._DEFAULT_ALPHABET\n        self._canonical = canonical\n        self._built = False\n\n        # --- Hashing Configuration ---\n        self._bps, self._mask, self._dtype = alphabet.masker(k)\n        self._bps = np.uint8(self._bps)\n        self._mask = self._dtype(self._mask)\n\n        # Reverse Complement Table\n        self._rc_table = None\n        if canonical:\n            if alphabet.complement is None:\n                raise ValueError(\"Canonical indexing requires an Alphabet with a complement table.\")\n            self._rc_table = alphabet.complement\n\n    @property\n    def k(self) -&gt; int: return self._k\n    @property\n    def alphabet(self) -&gt; Alphabet: return self._alphabet\n    @property\n    def canonical(self) -&gt; bool: return self._canonical\n    @property\n    def built(self) -&gt; bool: return self._built\n    @property\n    def bps(self) -&gt; int: return self._bps\n    @property\n    def mask(self) -&gt; int: return self._mask\n    @property\n    def dtype(self) -&gt; np.dtype: return self._dtype\n    @property\n    def rc_table(self) -&gt; np.ndarray: return self._rc_table\n    @abstractmethod\n    def query(self, queries: Union[list[Seq], SeqBatch] = None) -&gt; np.ndarray: ...\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.MinHashSketch","title":"<code>MinHashSketch</code>","text":"<p>               Bases: <code>BaseIndex</code></p> <p>Compression Index for Distance Estimation (Jaccard/ANI). Stores hashes in multiple memory blocks to allow instant updates.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; sketch = MinHashSketch(k=21, size=1000)\n&gt;&gt;&gt; sketch.build(batch)\n&gt;&gt;&gt; dists = sketch.query(query_seq)\n</code></pre> Source code in <code>baclib/engines/index.py</code> <pre><code>class MinHashSketch(BaseIndex):\n    \"\"\"\n    Compression Index for Distance Estimation (Jaccard/ANI).\n    Stores hashes in multiple memory blocks to allow instant updates.\n\n    Examples:\n        &gt;&gt;&gt; sketch = MinHashSketch(k=21, size=1000)\n        &gt;&gt;&gt; sketch.build(batch)\n        &gt;&gt;&gt; dists = sketch.query(query_seq)\n    \"\"\"\n\n    def __init__(self, k: int = 16, size: int = 1000, alphabet: Alphabet = None):\n        super().__init__(k, alphabet or Alphabet.dna(), canonical=True)\n        self._size = np.uint16(size)\n        self._sketches = np.empty((0, self._size), dtype=self._dtype)\n\n    @property\n    def size(self): return self._size\n\n    @property\n    def sketches(self) -&gt; np.ndarray:\n        if not self._built: raise RuntimeError(\"Index not built\")\n        return self._sketches\n\n    def build(self, batch: SeqBatch):\n        \"\"\"Builds the index from a single batch of sequences.\"\"\"\n        data, starts, lengths = batch.arrays\n        n = len(batch)\n        self._sketches = _minhash_kernel(\n            data, starts[:n], lengths[:n], self._k, self._size, self._bps, self._mask, self._dtype, self._rc_table\n        )\n        self._built = True\n\n    def query(self, queries: Union[list[Seq], SeqBatch] = None) -&gt; np.ndarray:\n        \"\"\"\n        Query the index.\n\n        Args:\n            queries: The query sequences. If None, computes all-vs-all distances.\n\n        Returns:\n            Array of Jaccard similarities.\n        \"\"\"\n        if not self._built: raise RuntimeError(\"Index not built\")\n        if queries is None: return _all_vs_all_jaccard_kernel(self._sketches)\n        if not isinstance(queries, SeqBatch): queries = self.alphabet.batch_from(queries)\n        if queries.alphabet != self.alphabet: raise ValueError(f\"Queries must have same alphabet as index: {self.alphabet}\")\n        # Compute sketches for the query batch\n        data, starts, lengths = queries.arrays\n        n = len(queries)\n        q_sketches = _minhash_kernel(\n            data, starts[:n], lengths[:n], self._k, self._size, self._bps, self._mask, self._dtype, self._rc_table\n        )\n        return _many_vs_many_jaccard_kernel(q_sketches, self._sketches)\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.MinHashSketch.build","title":"<code>build(batch)</code>","text":"<p>Builds the index from a single batch of sequences.</p> Source code in <code>baclib/engines/index.py</code> <pre><code>def build(self, batch: SeqBatch):\n    \"\"\"Builds the index from a single batch of sequences.\"\"\"\n    data, starts, lengths = batch.arrays\n    n = len(batch)\n    self._sketches = _minhash_kernel(\n        data, starts[:n], lengths[:n], self._k, self._size, self._bps, self._mask, self._dtype, self._rc_table\n    )\n    self._built = True\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.MinHashSketch.query","title":"<code>query(queries=None)</code>","text":"<p>Query the index.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[list[Seq], SeqBatch]</code> <p>The query sequences. If None, computes all-vs-all distances.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of Jaccard similarities.</p> Source code in <code>baclib/engines/index.py</code> <pre><code>def query(self, queries: Union[list[Seq], SeqBatch] = None) -&gt; np.ndarray:\n    \"\"\"\n    Query the index.\n\n    Args:\n        queries: The query sequences. If None, computes all-vs-all distances.\n\n    Returns:\n        Array of Jaccard similarities.\n    \"\"\"\n    if not self._built: raise RuntimeError(\"Index not built\")\n    if queries is None: return _all_vs_all_jaccard_kernel(self._sketches)\n    if not isinstance(queries, SeqBatch): queries = self.alphabet.batch_from(queries)\n    if queries.alphabet != self.alphabet: raise ValueError(f\"Queries must have same alphabet as index: {self.alphabet}\")\n    # Compute sketches for the query batch\n    data, starts, lengths = queries.arrays\n    n = len(queries)\n    q_sketches = _minhash_kernel(\n        data, starts[:n], lengths[:n], self._k, self._size, self._bps, self._mask, self._dtype, self._rc_table\n    )\n    return _many_vs_many_jaccard_kernel(q_sketches, self._sketches)\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.SparseMapIndex","title":"<code>SparseMapIndex</code>","text":"<p>               Bases: <code>BaseIndex</code></p> <p>Parent class for Mapping/Seeding indexes (Minimizers &amp; Syncmers). Manages the storage, sorting, and querying of sparse anchors.</p> Source code in <code>baclib/engines/index.py</code> <pre><code>class SparseMapIndex(BaseIndex):\n    \"\"\"\n    Parent class for Mapping/Seeding indexes (Minimizers &amp; Syncmers).\n    Manages the storage, sorting, and querying of sparse anchors.\n    \"\"\"\n\n    _KERNEL_REGISTRY = {}\n\n    @classmethod\n    def register_kernel(cls, mode: int):\n        def decorator(func):\n            cls._KERNEL_REGISTRY[mode] = func\n            return func\n        return decorator\n\n    def __init__(self, k: int = 15, alphabet: Alphabet = None,\n                 mode: Union[str, SparseMapIndexMode] = SparseMapIndexMode.MINIMIZER, **kwargs):\n        super().__init__(k, alphabet, canonical=True)\n        self._n_seqs = 0\n\n        self._mode = SparseMapIndexMode[mode.upper()] if isinstance(mode, str) else SparseMapIndexMode(mode)\n\n        # Setup strategy (Kernel + Args)\n        self._kernel, self._kernel_args = self._KERNEL_REGISTRY[self._mode](self, **kwargs)\n        # Use dtype from alphabet for hashes\n        self._hashes = np.empty(0, dtype=self._dtype)\n        self._positions = np.empty(0, dtype=np.uint32)\n        self._ids = np.empty(0, dtype=np.uint32)\n\n    def __len__(self) -&gt; int: return self._n_seqs\n\n    @property\n    def n_seqs(self) -&gt; int: return self._n_seqs\n    @property\n    def hashes(self) -&gt; np.ndarray: return self._hashes\n    @property\n    def positions(self) -&gt; np.ndarray: return self._positions\n    @property\n    def ids(self) -&gt; np.ndarray: return self._ids\n\n    def compute_seeds(self, queries: Union[list[Seq], SeqBatch]) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"Computes seeds (hashes, positions, ids) for a batch.\"\"\"\n        if not isinstance(queries, SeqBatch): queries = self.alphabet.batch_from(queries)\n        data, starts, lengths = queries.arrays\n        n = len(queries)\n        return self._kernel(data, starts[:n], lengths[:n], self._k, *self._kernel_args)\n\n    def build(self, seqs: Union[list[Seq], SeqBatch]):\n        \"\"\"Builds the index from a single batch of sequences.\"\"\"\n        # Ensure we work with a batch to get correct sequence count\n        if isinstance(seqs, SeqBatch):\n            batch = seqs\n        else:\n            batch = self.alphabet.batch_from(seqs)\n\n        hashes, pos, ids = self.compute_seeds(batch)\n        self._n_seqs = len(batch)\n        sorter = np.argsort(hashes)\n        self._hashes = hashes[sorter]\n        self._positions = pos[sorter]\n        self._ids = ids[sorter]\n        self._built = True\n\n    def query(self, queries: Union[list[Seq], SeqBatch] = None) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Queries the index\n\n        Args:\n            queries: The queries.\n\n        Returns:\n            (query_pos, target_pos, target_id) arrays.\n        \"\"\"\n        if not self._built: raise RuntimeError(\"Index not built\")\n        q_hashes, q_pos, _ = self.compute_seeds(queries)\n        return _find_hits_kernel(q_hashes, q_pos, self._hashes, self._positions, self._ids)\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.SparseMapIndex.build","title":"<code>build(seqs)</code>","text":"<p>Builds the index from a single batch of sequences.</p> Source code in <code>baclib/engines/index.py</code> <pre><code>def build(self, seqs: Union[list[Seq], SeqBatch]):\n    \"\"\"Builds the index from a single batch of sequences.\"\"\"\n    # Ensure we work with a batch to get correct sequence count\n    if isinstance(seqs, SeqBatch):\n        batch = seqs\n    else:\n        batch = self.alphabet.batch_from(seqs)\n\n    hashes, pos, ids = self.compute_seeds(batch)\n    self._n_seqs = len(batch)\n    sorter = np.argsort(hashes)\n    self._hashes = hashes[sorter]\n    self._positions = pos[sorter]\n    self._ids = ids[sorter]\n    self._built = True\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.SparseMapIndex.compute_seeds","title":"<code>compute_seeds(queries)</code>","text":"<p>Computes seeds (hashes, positions, ids) for a batch.</p> Source code in <code>baclib/engines/index.py</code> <pre><code>def compute_seeds(self, queries: Union[list[Seq], SeqBatch]) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Computes seeds (hashes, positions, ids) for a batch.\"\"\"\n    if not isinstance(queries, SeqBatch): queries = self.alphabet.batch_from(queries)\n    data, starts, lengths = queries.arrays\n    n = len(queries)\n    return self._kernel(data, starts[:n], lengths[:n], self._k, *self._kernel_args)\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.SparseMapIndex.query","title":"<code>query(queries=None)</code>","text":"<p>Queries the index</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[list[Seq], SeqBatch]</code> <p>The queries.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[ndarray, ndarray, ndarray]</code> <p>(query_pos, target_pos, target_id) arrays.</p> Source code in <code>baclib/engines/index.py</code> <pre><code>def query(self, queries: Union[list[Seq], SeqBatch] = None) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Queries the index\n\n    Args:\n        queries: The queries.\n\n    Returns:\n        (query_pos, target_pos, target_id) arrays.\n    \"\"\"\n    if not self._built: raise RuntimeError(\"Index not built\")\n    q_hashes, q_pos, _ = self.compute_seeds(queries)\n    return _find_hits_kernel(q_hashes, q_pos, self._hashes, self._positions, self._ids)\n</code></pre>"},{"location":"reference/baclib/engines/index_py/#baclib.engines.index.SparseMapIndexMode","title":"<code>SparseMapIndexMode</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Seeding strategy for sparse sequence indexing.</p> Source code in <code>baclib/engines/index.py</code> <pre><code>class SparseMapIndexMode(IntEnum):\n    \"\"\"Seeding strategy for sparse sequence indexing.\"\"\"\n    MINIMIZER = 0\n    SYNCMER = 1\n    DENSE = 2\n</code></pre>"},{"location":"reference/baclib/engines/motif/","title":"motif","text":""},{"location":"reference/baclib/engines/motif/#baclib.engines.motif","title":"<code>baclib.engines.motif</code>","text":"<p>Engines for motif scanning and de novo motif discovery using k-mer enrichment analysis.</p>"},{"location":"reference/baclib/engines/motif/#baclib.engines.motif.MotifFinder","title":"<code>MotifFinder</code>","text":"<p>Engine for de novo motif discovery. Implements a STREME-like pipeline: 1. Efficient K-mer counting (using Dense Index). 2. Statistical Enrichment Testing (Binomial/Fisher). 3. Iterative Refinement (Seed -&gt; Scan -&gt; Update PSSM).</p> Source code in <code>baclib/engines/motif.py</code> <pre><code>class MotifFinder:\n    \"\"\"\n    Engine for de novo motif discovery.\n    Implements a STREME-like pipeline:\n    1. Efficient K-mer counting (using Dense Index).\n    2. Statistical Enrichment Testing (Binomial/Fisher).\n    3. Iterative Refinement (Seed -&gt; Scan -&gt; Update PSSM).\n    \"\"\"\n    def __init__(self, foreground: SeqBatch, background: Union[SeqBatch, Background] = None, k: int = 8):\n        self.k = k\n        self.fg = foreground\n        self.alphabet = foreground.alphabet\n\n        # 1. Build Foreground Index (The \"Observed\" Data)\n        self._fg_idx = SparseMapIndex(k=k, mode=SparseMapIndexMode.DENSE, alphabet=self.alphabet)\n        self._fg_idx.build(foreground)\n\n        # 2. Count Foreground K-mers\n        # Sorts by hash, allowing O(log N) lookups\n        self._fg_kmers, self._fg_counts = np.unique(self._fg_idx.hashes, return_counts=True)\n        self._n_fg = self._fg_counts.sum()\n\n        # Symbol mask for decoding (e.g. 3 for DNA, 31 for Amino)\n        self._symbol_mask = (1 &lt;&lt; self._fg_idx.bps) - 1\n\n        # 3. Setup Background Model\n        self._bg_probs = None  # Probability of finding each unique k-mer in the background\n        self._bg_model = None  # The Background object (for PSSM creation)\n\n        if isinstance(background, SeqBatch):\n            # Discriminative Mode (vs Control Sequences)\n            bg_idx = SparseMapIndex(k=k, mode=SparseMapIndexMode.DENSE, alphabet=self.alphabet)\n            bg_idx.build(background)\n            bg_kmers_raw, bg_counts_raw = np.unique(bg_idx.hashes, return_counts=True)\n            n_bg = bg_counts_raw.sum()\n\n            # Align counts to Foreground k-mers\n            bg_counts_aligned = _align_counts(self._fg_kmers, bg_kmers_raw, bg_counts_raw)\n\n            # P(kmer) = (Count_BG + 1) / (Total_BG + Unique_Kmers) - Laplace Smoothing\n            denom = n_bg + len(bg_kmers_raw)\n            self._bg_probs = (bg_counts_aligned + 1) / denom\n            self._bg_model = Background.from_seq(background)\n\n        elif isinstance(background, Background):\n            # Provided Background Model\n            self._bg_model = background\n            self._bg_probs = _calc_kmer_probs_kernel(\n                self._fg_kmers, k, background.data, self._fg_idx.bps, self._symbol_mask\n            )\n        else:\n            # Null Model (Learn 0-order from Foreground)\n            self._bg_model = Background.from_seq(foreground)\n            self._bg_probs = _calc_kmer_probs_kernel(\n                self._fg_kmers, k, self._bg_model.data, self._fg_idx.bps, self._symbol_mask\n            )\n\n    def run(self, n_motifs: int = 5, max_pvalue: float = 0.05, refine: bool = True) -&gt; MotifBatch:\n        \"\"\"\n        Executes the discovery pipeline.\n\n        Args:\n            n_motifs: Number of top motifs to return.\n            max_pvalue: Significance threshold for seeds.\n            refine: If True, performs 1 round of EM-refinement (Scan -&gt; Rebuild PSSM).\n        \"\"\"\n        try:\n            from scipy.special import bdtrc\n        except ImportError:\n            raise ImportError(\"Motif discovery requires 'scipy' to be installed.\")\n\n        # 1. Statistical Test (Binomial Survival Function)\n        # H0: k-mer occurrences follow Binomial(n=total_fg, p=bg_probs)\n        pvalues = bdtrc(self._fg_counts - 1, self._n_fg, self._bg_probs)\n\n        # 2. Filter &amp; Sort Seeds\n        sig_mask = pvalues &lt; max_pvalue\n        if not np.any(sig_mask): return MotifBatch([])\n\n        sig_indices = np.where(sig_mask)[0]\n        sorted_indices = sig_indices[np.argsort(pvalues[sig_indices])]\n\n        # 3. Select unique seeds (Naive approach: just take top N)\n        # In a full implementation, you'd mask occurrences of the top motif before finding the next.\n        top_indices = sorted_indices[:n_motifs]\n\n        final_motifs = []\n\n        for i, idx in enumerate(top_indices):\n            kmer_int = self._fg_kmers[idx]\n            pval = pvalues[idx]\n\n            # 3a. Create Seed Motif\n            # We initialize the PSSM with the k-mer sequence itself (count=100 equiv)\n            seed_counts = _int_to_onehot(kmer_int, self.k, self._fg_idx.bps, self._symbol_mask) * 100.0\n            name = f\"rank_{i + 1}_p{pval:.1e}\".encode(Alphabet.ENCODING)\n            m = Motif.from_counts(name, seed_counts, self._bg_model)\n\n            if refine:\n                # 3b. Refine (The EM Step)\n                # Scan with a modest threshold to find variations of the motif\n                hits = MotifScanner(m).scan(self.fg, pvalue_threshold=1e-3)\n\n                if len(hits) &gt; 5:  # Only update if we found decent support\n                    # Accumulate actual sequences from hits into a new Count Matrix\n                    # This is much faster than extracting strings\n                    data, starts, _ = self.fg.arrays\n                    new_counts = _accumulate_hits_kernel(\n                        data, hits.seq_indices, hits.positions, hits.strands,\n                        starts, self.k, self.alphabet.complement\n                    )\n                    # Create new, refined motif\n                    m = Motif.from_counts(name, new_counts, self._bg_model)\n\n            final_motifs.append(m)\n\n        return MotifBatch(final_motifs)\n</code></pre>"},{"location":"reference/baclib/engines/motif/#baclib.engines.motif.MotifFinder.run","title":"<code>run(n_motifs=5, max_pvalue=0.05, refine=True)</code>","text":"<p>Executes the discovery pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>n_motifs</code> <code>int</code> <p>Number of top motifs to return.</p> <code>5</code> <code>max_pvalue</code> <code>float</code> <p>Significance threshold for seeds.</p> <code>0.05</code> <code>refine</code> <code>bool</code> <p>If True, performs 1 round of EM-refinement (Scan -&gt; Rebuild PSSM).</p> <code>True</code> Source code in <code>baclib/engines/motif.py</code> <pre><code>def run(self, n_motifs: int = 5, max_pvalue: float = 0.05, refine: bool = True) -&gt; MotifBatch:\n    \"\"\"\n    Executes the discovery pipeline.\n\n    Args:\n        n_motifs: Number of top motifs to return.\n        max_pvalue: Significance threshold for seeds.\n        refine: If True, performs 1 round of EM-refinement (Scan -&gt; Rebuild PSSM).\n    \"\"\"\n    try:\n        from scipy.special import bdtrc\n    except ImportError:\n        raise ImportError(\"Motif discovery requires 'scipy' to be installed.\")\n\n    # 1. Statistical Test (Binomial Survival Function)\n    # H0: k-mer occurrences follow Binomial(n=total_fg, p=bg_probs)\n    pvalues = bdtrc(self._fg_counts - 1, self._n_fg, self._bg_probs)\n\n    # 2. Filter &amp; Sort Seeds\n    sig_mask = pvalues &lt; max_pvalue\n    if not np.any(sig_mask): return MotifBatch([])\n\n    sig_indices = np.where(sig_mask)[0]\n    sorted_indices = sig_indices[np.argsort(pvalues[sig_indices])]\n\n    # 3. Select unique seeds (Naive approach: just take top N)\n    # In a full implementation, you'd mask occurrences of the top motif before finding the next.\n    top_indices = sorted_indices[:n_motifs]\n\n    final_motifs = []\n\n    for i, idx in enumerate(top_indices):\n        kmer_int = self._fg_kmers[idx]\n        pval = pvalues[idx]\n\n        # 3a. Create Seed Motif\n        # We initialize the PSSM with the k-mer sequence itself (count=100 equiv)\n        seed_counts = _int_to_onehot(kmer_int, self.k, self._fg_idx.bps, self._symbol_mask) * 100.0\n        name = f\"rank_{i + 1}_p{pval:.1e}\".encode(Alphabet.ENCODING)\n        m = Motif.from_counts(name, seed_counts, self._bg_model)\n\n        if refine:\n            # 3b. Refine (The EM Step)\n            # Scan with a modest threshold to find variations of the motif\n            hits = MotifScanner(m).scan(self.fg, pvalue_threshold=1e-3)\n\n            if len(hits) &gt; 5:  # Only update if we found decent support\n                # Accumulate actual sequences from hits into a new Count Matrix\n                # This is much faster than extracting strings\n                data, starts, _ = self.fg.arrays\n                new_counts = _accumulate_hits_kernel(\n                    data, hits.seq_indices, hits.positions, hits.strands,\n                    starts, self.k, self.alphabet.complement\n                )\n                # Create new, refined motif\n                m = Motif.from_counts(name, new_counts, self._bg_model)\n\n        final_motifs.append(m)\n\n    return MotifBatch(final_motifs)\n</code></pre>"},{"location":"reference/baclib/engines/motif/#baclib.engines.motif.MotifScanner","title":"<code>MotifScanner</code>","text":"<p>High-performance scanner for finding motif occurrences in sequence batches.</p> Source code in <code>baclib/engines/motif.py</code> <pre><code>class MotifScanner:\n    \"\"\"\n    High-performance scanner for finding motif occurrences in sequence batches.\n    \"\"\"\n    def __init__(self, motifs: Union[Motif, Iterable[Motif], MotifBatch]):\n        if isinstance(motifs, Motif): motifs = [motifs]\n        self.batch = motifs if isinstance(motifs, MotifBatch) else MotifBatch(motifs)\n\n    def scan(self, seqs: SeqBatch, pvalue_threshold: float = 1e-4) -&gt; 'MotifHitBatch':\n        if len(self.batch) == 0: return MotifHitBatch.new_empty(self.batch)\n        if seqs.alphabet != self.batch._motifs[0].background.alphabet:\n            raise ValueError(f\"Motif alphabet does not match Sequence alphabet\")\n\n        # Calculate thresholds for all motifs\n        thresholds = np.array([m.get_score(pvalue_threshold) for m in self.batch._motifs], dtype=np.float32)\n        data, starts, lengths = seqs.arrays\n\n        seq_idxs_list, pos_list, scores_list, strands_list, motif_idxs_list = [], [], [], [], []\n\n        def _process(pssm_comb, max_suff, strand):\n            counts = _scan_batch_count_kernel(\n                data, starts, lengths, pssm_comb, self.batch._offsets, thresholds, max_suff\n            )\n            total = counts.sum()\n            if total &gt; 0:\n                offsets = np.zeros(len(counts) + 1, dtype=np.int32)\n                np.cumsum(counts, out=offsets[1:])\n                out_pos = np.empty(total, dtype=np.int32)\n                out_scores = np.empty(total, dtype=np.float32)\n                out_seq_idxs = np.empty(total, dtype=np.int32)\n                out_motif_idxs = np.empty(total, dtype=np.int32)\n                _scan_batch_fill_kernel(\n                    data, starts, lengths, pssm_comb, self.batch._offsets, thresholds, max_suff,\n                    offsets, out_pos, out_scores, out_seq_idxs, out_motif_idxs\n                )\n                seq_idxs_list.append(out_seq_idxs)\n                pos_list.append(out_pos)\n                scores_list.append(out_scores)\n                motif_idxs_list.append(out_motif_idxs)\n                strands_list.append(np.full(total, strand, dtype=np.int8))\n\n        _process(self.batch._pssm_combined, self.batch._max_suffixes, 1)\n        if self.batch._pssm_rc_combined is not None:\n            _process(self.batch._pssm_rc_combined, self.batch._max_suffixes_rc, -1)\n\n        if not seq_idxs_list: return MotifHitBatch.new_empty(self.batch)\n\n        return MotifHitBatch(\n            np.concatenate(seq_idxs_list), np.concatenate(pos_list), np.concatenate(scores_list),\n            np.concatenate(strands_list), self.batch, np.concatenate(motif_idxs_list)\n        )\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/","title":"pairwise","text":""},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise","title":"<code>baclib.engines.pairwise</code>","text":"<p>Pairwise sequence alignment engine with seeding, chaining, and dynamic programming.</p>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.Aligner","title":"<code>Aligner</code>","text":"<p>High-performance pairwise aligner. Optimized for Seeding -&gt; Window Extraction -&gt; DP.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; aligner = Aligner()\n&gt;&gt;&gt; aligner.build(target_batch)\n&gt;&gt;&gt; hits = aligner.map(query_batch)\n</code></pre> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>class Aligner:\n    \"\"\"\n    High-performance pairwise aligner.\n    Optimized for Seeding -&gt; Window Extraction -&gt; DP.\n\n    Examples:\n        &gt;&gt;&gt; aligner = Aligner()\n        &gt;&gt;&gt; aligner.build(target_batch)\n        &gt;&gt;&gt; hits = aligner.map(query_batch)\n    \"\"\"\n    _REGISTRY = {}\n\n    @classmethod\n    def register(cls, mode: int):\n        def decorator(func):\n            cls._REGISTRY[mode] = func\n            return func\n        return decorator\n\n    __slots__ = ('_index', '_score_matrix', '_data', 'gap_open', 'gap_extend', '_mode', \n                 '_compute_traceback', '_traceback_kernel', '_pad_val')\n    def __init__(self, targets: Union[list[Seq], SeqBatch], index: SparseMapIndex,\n                 mode: Union[str, AlignmentMode] = AlignmentMode.GLOCAL,\n                 score_matrix: ScoreMatrix = None, match: int = 1,  mismatch: int = -1, gap_open: int = 5,\n                 gap_extend: int = 2, compute_traceback: bool = False):\n\n        if index.built: raise ValueError('Index already built')\n        self._index = index\n        self._data: SeqBatch = targets if isinstance(targets, SeqBatch) else index.alphabet.batch_from(targets)\n        self._index.build(self._data)\n\n        n_sym = len(self._index.alphabet)\n        self._pad_val = n_sym\n\n        if score_matrix is None:\n            score_matrix = ScoreMatrix.build(n_sym, match, mismatch)\n            fill = mismatch\n        else:\n            fill = np.min(score_matrix)\n\n        self._score_matrix = score_matrix.pad(shape=(n_sym + 1, n_sym + 1), fill_value=fill)\n        self.gap_open = gap_open\n        self.gap_extend = gap_extend\n        self._mode = AlignmentMode[mode.upper() if isinstance(mode, str) else mode]\n        self._compute_traceback = compute_traceback\n        # Registry lookup\n        self._traceback_kernel = self._REGISTRY[self._mode]\n\n    def map(self, queries: Union[list[Seq], SeqBatch], min_score: int = 0, min_seeds: int = 2,\n            padding: int = 50, chain_gap: int = 500, chain_bandwidth: int = 100) -&gt; AlignmentBatch:\n        \"\"\"\n        Aligns queries to the built index.\n\n        Args:\n            queries: Query sequences.\n            min_score: Minimum alignment score to report.\n            min_seeds: Minimum seeds to trigger alignment.\n            padding: Padding around seed clusters.\n            chain_gap: Gap around seed clusters.\n            chain_bandwidth: Bandwidth around seed clusters.\n\n        Returns:\n            AlignmentBatch containing the alignments.\n        \"\"\"\n        # 1. Access Index Internals to get Seed Coordinates\n        if not self._index.built: raise RuntimeError(\"Index not built\")\n\n        # Helper to compute hashes for the query batch\n        q_hashes, q_starts, q_ids = self._index.compute_seeds(queries)\n\n        # Pack Query ID and Position into uint64 to use the generic index kernel\n        q_packed = (q_ids.astype(np.uint64) &lt;&lt; 32) | q_starts.astype(np.uint64)\n\n        raw_packed, raw_t_pos, raw_t_ids = _find_hits_kernel(\n            q_hashes, q_packed,\n            self._index._hashes, self._index._positions, self._index._ids\n        )\n        raw_q_ids = (raw_packed &gt;&gt; 32).astype(np.uint32)\n        raw_q_pos = raw_packed.astype(np.uint32)\n\n        if len(raw_q_ids) == 0: return AlignmentBatch.empty()\n\n        # 2. Cluster &amp; Bounding Box (Crucial Optimization)\n        # Groups seeds by (q_id, t_id) and finds min/max coords\n        k = self._index.k\n        candidates = _chain_seeds_kernel(\n            raw_q_ids, raw_t_ids, raw_q_pos, raw_t_pos,\n            k, min_seeds, chain_gap, chain_bandwidth, padding,\n            queries.arrays[2], self._data.arrays[2],\n            is_global=(self._mode == AlignmentMode.GLOBAL)\n        )\n        if len(candidates) == 0: return AlignmentBatch.empty()\n\n        # Unpack candidates: [q_idx, t_idx, qs, qe, ts, te]\n        c_q_idx = candidates[:, 0]\n        c_t_idx = candidates[:, 1]\n        c_q_start = candidates[:, 2]\n        c_q_end = candidates[:, 3]\n        c_t_start = candidates[:, 4]\n        c_t_end = candidates[:, 5]\n\n        # 3. Batch Extension (DP on Windows)\n        n_cand = len(candidates)\n        out_scores = np.empty(n_cand, dtype=np.int32)\n        out_end_coords = np.empty((n_cand, 2), dtype=np.int32)\n\n        _batch_score_driver(\n            queries.arrays, self._data.arrays,\n            c_q_idx, c_t_idx, c_q_start, c_q_end, c_t_start, c_t_end,\n            self._score_matrix, self.gap_open, self.gap_extend,\n            out_scores, out_end_coords,\n            mode=self._mode,\n            pad_val=self._pad_val\n        )\n        # 4. Filter &amp; Refine\n        mask = out_scores &gt;= min_score\n        if not np.any(mask): return AlignmentBatch.empty()\n\n        v_q_idx = c_q_idx[mask]\n        v_t_idx = c_t_idx[mask]\n        v_scores = out_scores[mask]\n\n        # Adjust local coordinates back to global\n        # out_end_coords are relative to the window start (c_q_start, c_t_start)\n        v_ends = out_end_coords[mask]\n        v_ends[:, 0] += c_q_start[mask]\n        v_ends[:, 1] += c_t_start[mask]\n\n        n_valid = len(v_scores)\n        q_coords = np.zeros((n_valid, 2), dtype=np.int32)\n        t_coords = np.zeros((n_valid, 2), dtype=np.int32)\n\n        q_coords[:, 1] = v_ends[:, 0]\n        t_coords[:, 1] = v_ends[:, 1]\n\n        cigars = None\n        if self._compute_traceback:\n            cigars, q_starts, t_starts = _batch_traceback_driver(\n                queries.arrays, self._data.arrays,\n                v_q_idx, v_t_idx,\n                c_q_start[mask], c_q_end[mask], c_t_start[mask], c_t_end[mask],\n                v_ends,  # These are global ends\n                self._score_matrix, self.gap_open, self.gap_extend,\n                self._traceback_kernel,\n                mode=self._mode.value\n            )\n            q_coords[:, 0] = q_starts\n            t_coords[:, 0] = t_starts\n        else:\n            # Approx starts logic relies on window start for Local/Glocal\n            if self._mode == AlignmentMode.GLOBAL:\n                q_coords[:, 0] = 0\n                t_coords[:, 0] = 0\n            else:\n                # Approximate start is the Window Start\n                # (Ideally traceback is used for exact start)\n                q_coords[:, 0] = c_q_start[mask]\n                t_coords[:, 0] = c_t_start[mask]\n\n        return AlignmentBatch.from_data(\n            q_idx=v_q_idx, t_idx=v_t_idx, score=v_scores.astype(np.float32),\n            q_coords=q_coords, t_coords=t_coords,\n            q_lens=queries.arrays[2][v_q_idx], t_lens=self._data.arrays[2][v_t_idx],\n            cigars=cigars\n        )\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.Aligner.map","title":"<code>map(queries, min_score=0, min_seeds=2, padding=50, chain_gap=500, chain_bandwidth=100)</code>","text":"<p>Aligns queries to the built index.</p> <p>Parameters:</p> Name Type Description Default <code>queries</code> <code>Union[list[Seq], SeqBatch]</code> <p>Query sequences.</p> required <code>min_score</code> <code>int</code> <p>Minimum alignment score to report.</p> <code>0</code> <code>min_seeds</code> <code>int</code> <p>Minimum seeds to trigger alignment.</p> <code>2</code> <code>padding</code> <code>int</code> <p>Padding around seed clusters.</p> <code>50</code> <code>chain_gap</code> <code>int</code> <p>Gap around seed clusters.</p> <code>500</code> <code>chain_bandwidth</code> <code>int</code> <p>Bandwidth around seed clusters.</p> <code>100</code> <p>Returns:</p> Type Description <code>AlignmentBatch</code> <p>AlignmentBatch containing the alignments.</p> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>def map(self, queries: Union[list[Seq], SeqBatch], min_score: int = 0, min_seeds: int = 2,\n        padding: int = 50, chain_gap: int = 500, chain_bandwidth: int = 100) -&gt; AlignmentBatch:\n    \"\"\"\n    Aligns queries to the built index.\n\n    Args:\n        queries: Query sequences.\n        min_score: Minimum alignment score to report.\n        min_seeds: Minimum seeds to trigger alignment.\n        padding: Padding around seed clusters.\n        chain_gap: Gap around seed clusters.\n        chain_bandwidth: Bandwidth around seed clusters.\n\n    Returns:\n        AlignmentBatch containing the alignments.\n    \"\"\"\n    # 1. Access Index Internals to get Seed Coordinates\n    if not self._index.built: raise RuntimeError(\"Index not built\")\n\n    # Helper to compute hashes for the query batch\n    q_hashes, q_starts, q_ids = self._index.compute_seeds(queries)\n\n    # Pack Query ID and Position into uint64 to use the generic index kernel\n    q_packed = (q_ids.astype(np.uint64) &lt;&lt; 32) | q_starts.astype(np.uint64)\n\n    raw_packed, raw_t_pos, raw_t_ids = _find_hits_kernel(\n        q_hashes, q_packed,\n        self._index._hashes, self._index._positions, self._index._ids\n    )\n    raw_q_ids = (raw_packed &gt;&gt; 32).astype(np.uint32)\n    raw_q_pos = raw_packed.astype(np.uint32)\n\n    if len(raw_q_ids) == 0: return AlignmentBatch.empty()\n\n    # 2. Cluster &amp; Bounding Box (Crucial Optimization)\n    # Groups seeds by (q_id, t_id) and finds min/max coords\n    k = self._index.k\n    candidates = _chain_seeds_kernel(\n        raw_q_ids, raw_t_ids, raw_q_pos, raw_t_pos,\n        k, min_seeds, chain_gap, chain_bandwidth, padding,\n        queries.arrays[2], self._data.arrays[2],\n        is_global=(self._mode == AlignmentMode.GLOBAL)\n    )\n    if len(candidates) == 0: return AlignmentBatch.empty()\n\n    # Unpack candidates: [q_idx, t_idx, qs, qe, ts, te]\n    c_q_idx = candidates[:, 0]\n    c_t_idx = candidates[:, 1]\n    c_q_start = candidates[:, 2]\n    c_q_end = candidates[:, 3]\n    c_t_start = candidates[:, 4]\n    c_t_end = candidates[:, 5]\n\n    # 3. Batch Extension (DP on Windows)\n    n_cand = len(candidates)\n    out_scores = np.empty(n_cand, dtype=np.int32)\n    out_end_coords = np.empty((n_cand, 2), dtype=np.int32)\n\n    _batch_score_driver(\n        queries.arrays, self._data.arrays,\n        c_q_idx, c_t_idx, c_q_start, c_q_end, c_t_start, c_t_end,\n        self._score_matrix, self.gap_open, self.gap_extend,\n        out_scores, out_end_coords,\n        mode=self._mode,\n        pad_val=self._pad_val\n    )\n    # 4. Filter &amp; Refine\n    mask = out_scores &gt;= min_score\n    if not np.any(mask): return AlignmentBatch.empty()\n\n    v_q_idx = c_q_idx[mask]\n    v_t_idx = c_t_idx[mask]\n    v_scores = out_scores[mask]\n\n    # Adjust local coordinates back to global\n    # out_end_coords are relative to the window start (c_q_start, c_t_start)\n    v_ends = out_end_coords[mask]\n    v_ends[:, 0] += c_q_start[mask]\n    v_ends[:, 1] += c_t_start[mask]\n\n    n_valid = len(v_scores)\n    q_coords = np.zeros((n_valid, 2), dtype=np.int32)\n    t_coords = np.zeros((n_valid, 2), dtype=np.int32)\n\n    q_coords[:, 1] = v_ends[:, 0]\n    t_coords[:, 1] = v_ends[:, 1]\n\n    cigars = None\n    if self._compute_traceback:\n        cigars, q_starts, t_starts = _batch_traceback_driver(\n            queries.arrays, self._data.arrays,\n            v_q_idx, v_t_idx,\n            c_q_start[mask], c_q_end[mask], c_t_start[mask], c_t_end[mask],\n            v_ends,  # These are global ends\n            self._score_matrix, self.gap_open, self.gap_extend,\n            self._traceback_kernel,\n            mode=self._mode.value\n        )\n        q_coords[:, 0] = q_starts\n        t_coords[:, 0] = t_starts\n    else:\n        # Approx starts logic relies on window start for Local/Glocal\n        if self._mode == AlignmentMode.GLOBAL:\n            q_coords[:, 0] = 0\n            t_coords[:, 0] = 0\n        else:\n            # Approximate start is the Window Start\n            # (Ideally traceback is used for exact start)\n            q_coords[:, 0] = c_q_start[mask]\n            t_coords[:, 0] = c_t_start[mask]\n\n    return AlignmentBatch.from_data(\n        q_idx=v_q_idx, t_idx=v_t_idx, score=v_scores.astype(np.float32),\n        q_coords=q_coords, t_coords=t_coords,\n        q_lens=queries.arrays[2][v_q_idx], t_lens=self._data.arrays[2][v_t_idx],\n        cigars=cigars\n    )\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.AlignmentMode","title":"<code>AlignmentMode</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Alignment strategy controlling how terminal gaps are scored.</p> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>class AlignmentMode(IntEnum):\n    \"\"\"Alignment strategy controlling how terminal gaps are scored.\"\"\"\n    LOCAL = 0\n    GLOBAL = 1\n    GLOCAL = 2\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.ScoreMatrix","title":"<code>ScoreMatrix</code>","text":"<p>Represents a substitution matrix for alignment.</p> <p>Attributes:</p> Name Type Description <code>_data</code> <code>ndarray</code> <p>The raw matrix data.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; m = ScoreMatrix.build(4, match=2, mismatch=-2)\n</code></pre> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>class ScoreMatrix:\n    \"\"\"\n    Represents a substitution matrix for alignment.\n\n    Attributes:\n        _data (np.ndarray): The raw matrix data.\n\n    Examples:\n        &gt;&gt;&gt; m = ScoreMatrix.build(4, match=2, mismatch=-2)\n    \"\"\"\n    _DTYPE = np.int8\n    __slots__ = ('_data',)\n\n    def __init__(self, data: Union[np.ndarray, Iterable]):\n        self._data = np.ascontiguousarray(data, dtype=self._DTYPE)\n        self._data.flags.writeable = False\n\n    def __getitem__(self, item): return self._data[item]\n    def __array__(self, dtype=None): return self._data.astype(dtype, copy=False) if dtype else self._data\n    def __repr__(self): return f\"ScoreMatrix{self._data.shape}\"\n    @property\n    def shape(self): return self._data.shape\n\n    @classmethod\n    def blosum62(cls):\n        \"\"\"Returns the BLOSUM62 matrix.\"\"\"\n        data = [\n            4, 0, -2, -1, -2, 0, -2, -1, -1, -1, -1, -2, -1, -1, -1, 1, 0, 0, -3, -2,\n            0, 9, -3, -4, -2, -3, -3, -1, -3, -1, -1, -3, -3, -3, -3, -1, -1, -1, -2, -2,\n            -2, -3, 6, 2, -3, -1, -1, -3, -1, -4, -3, 1, -1, 0, -2, 0, -1, -3, -4, -3,\n            -1, -4, 2, 5, -3, -2, 0, -3, 1, -3, -2, 0, -1, 2, 0, 0, -1, -2, -3, -2,\n            -2, -2, -3, -3, 6, -3, -1, 0, -3, 0, 0, -3, -4, -3, -3, -2, -2, -1, 1, 3,\n            0, -3, -1, -2, -3, 6, -2, -4, -2, -4, -3, 0, -2, -2, -2, 0, -2, -3, -2, -3,\n            -2, -3, -1, 0, -1, -2, 8, -3, -1, -3, -2, 1, -2, 0, 0, -1, -2, -3, -2, 2,\n            -1, -1, -3, -3, 0, -4, -3, 4, -3, 2, 1, -3, -3, -3, -3, -2, -1, 3, -3, -1,\n            -1, -3, -1, 1, -3, -2, -1, -3, 5, -2, -3, 2, 0, -3, -3, 1, 0, -3, -1, 2,\n            -1, -1, -4, -3, 0, -4, -3, 2, -2, 4, 2, -3, -3, -2, -2, -2, -1, 1, -2, -1,\n            -1, -1, -3, -2, 0, -3, -2, 1, -3, 2, 5, -2, -2, 0, -1, -1, -1, 1, -1, -1,\n            -2, -3, 1, 0, -3, 0, 1, -3, 2, -3, -2, 6, -2, -4, -4, -1, 0, -3, -1, -3,\n            -1, -3, -1, -1, -4, -2, -2, -3, 0, -3, -2, -2, 7, -1, -2, -1, -1, -2, -4, -3,\n            -1, -3, 0, 2, -3, -2, 0, -3, -3, -2, 0, -4, -1, 5, 1, 0, -1, -2, -2, -1,\n            -1, -3, -2, 0, -3, -2, 0, -3, -3, -2, -1, -4, -2, 1, 5, -1, -1, -3, -3, -2,\n            1, -1, 0, 0, -2, 0, -1, -2, 1, -2, -1, -1, -1, 0, -1, 4, 1, -2, -3, -2,\n            0, -1, -1, -1, -2, -2, -2, -1, 0, -1, -1, 0, -1, -1, -1, 1, 5, 0, -2, -2,\n            0, -1, -3, -2, -1, -3, -3, 3, -3, 1, 1, -3, -2, -2, -3, -2, 0, 4, -3, -1,\n            -3, -2, -4, -3, 1, -2, -2, -3, -1, -2, -1, -1, -4, -2, -3, -3, -2, -3, 11, 2,\n            -2, -2, -3, -2, 3, -3, 2, -1, 2, -1, -1, -3, -3, -1, -2, -2, -2, -1, 2, 7\n        ]\n        return cls(np.array(data, dtype=cls._DTYPE).reshape(20, 20))\n\n    @classmethod\n    def build(cls, n, match=1, mismatch=-1):\n        \"\"\"Builds a simple match/mismatch matrix.\"\"\"\n        M = np.full((n, n), mismatch, dtype=cls._DTYPE)\n        np.fill_diagonal(M, match)\n        return cls(M)\n\n    def pad(self, shape=(256, 256), fill_value=None) -&gt; np.ndarray:\n        \"\"\"Pads the matrix to handle full uint8 range (0-255) without bounds checks.\"\"\"\n        if self.shape == shape: return self._data\n        if fill_value is None: fill_value = np.min(self)\n        new_data = np.full(shape, fill_value, dtype=self._data.dtype)\n        r, c = self.shape\n        new_data[:r, :c] = self._data\n        return new_data\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.ScoreMatrix.blosum62","title":"<code>blosum62()</code>  <code>classmethod</code>","text":"<p>Returns the BLOSUM62 matrix.</p> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>@classmethod\ndef blosum62(cls):\n    \"\"\"Returns the BLOSUM62 matrix.\"\"\"\n    data = [\n        4, 0, -2, -1, -2, 0, -2, -1, -1, -1, -1, -2, -1, -1, -1, 1, 0, 0, -3, -2,\n        0, 9, -3, -4, -2, -3, -3, -1, -3, -1, -1, -3, -3, -3, -3, -1, -1, -1, -2, -2,\n        -2, -3, 6, 2, -3, -1, -1, -3, -1, -4, -3, 1, -1, 0, -2, 0, -1, -3, -4, -3,\n        -1, -4, 2, 5, -3, -2, 0, -3, 1, -3, -2, 0, -1, 2, 0, 0, -1, -2, -3, -2,\n        -2, -2, -3, -3, 6, -3, -1, 0, -3, 0, 0, -3, -4, -3, -3, -2, -2, -1, 1, 3,\n        0, -3, -1, -2, -3, 6, -2, -4, -2, -4, -3, 0, -2, -2, -2, 0, -2, -3, -2, -3,\n        -2, -3, -1, 0, -1, -2, 8, -3, -1, -3, -2, 1, -2, 0, 0, -1, -2, -3, -2, 2,\n        -1, -1, -3, -3, 0, -4, -3, 4, -3, 2, 1, -3, -3, -3, -3, -2, -1, 3, -3, -1,\n        -1, -3, -1, 1, -3, -2, -1, -3, 5, -2, -3, 2, 0, -3, -3, 1, 0, -3, -1, 2,\n        -1, -1, -4, -3, 0, -4, -3, 2, -2, 4, 2, -3, -3, -2, -2, -2, -1, 1, -2, -1,\n        -1, -1, -3, -2, 0, -3, -2, 1, -3, 2, 5, -2, -2, 0, -1, -1, -1, 1, -1, -1,\n        -2, -3, 1, 0, -3, 0, 1, -3, 2, -3, -2, 6, -2, -4, -4, -1, 0, -3, -1, -3,\n        -1, -3, -1, -1, -4, -2, -2, -3, 0, -3, -2, -2, 7, -1, -2, -1, -1, -2, -4, -3,\n        -1, -3, 0, 2, -3, -2, 0, -3, -3, -2, 0, -4, -1, 5, 1, 0, -1, -2, -2, -1,\n        -1, -3, -2, 0, -3, -2, 0, -3, -3, -2, -1, -4, -2, 1, 5, -1, -1, -3, -3, -2,\n        1, -1, 0, 0, -2, 0, -1, -2, 1, -2, -1, -1, -1, 0, -1, 4, 1, -2, -3, -2,\n        0, -1, -1, -1, -2, -2, -2, -1, 0, -1, -1, 0, -1, -1, -1, 1, 5, 0, -2, -2,\n        0, -1, -3, -2, -1, -3, -3, 3, -3, 1, 1, -3, -2, -2, -3, -2, 0, 4, -3, -1,\n        -3, -2, -4, -3, 1, -2, -2, -3, -1, -2, -1, -1, -4, -2, -3, -3, -2, -3, 11, 2,\n        -2, -2, -3, -2, 3, -3, 2, -1, 2, -1, -1, -3, -3, -1, -2, -2, -2, -1, 2, 7\n    ]\n    return cls(np.array(data, dtype=cls._DTYPE).reshape(20, 20))\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.ScoreMatrix.build","title":"<code>build(n, match=1, mismatch=-1)</code>  <code>classmethod</code>","text":"<p>Builds a simple match/mismatch matrix.</p> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>@classmethod\ndef build(cls, n, match=1, mismatch=-1):\n    \"\"\"Builds a simple match/mismatch matrix.\"\"\"\n    M = np.full((n, n), mismatch, dtype=cls._DTYPE)\n    np.fill_diagonal(M, match)\n    return cls(M)\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.ScoreMatrix.pad","title":"<code>pad(shape=(256, 256), fill_value=None)</code>","text":"<p>Pads the matrix to handle full uint8 range (0-255) without bounds checks.</p> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>def pad(self, shape=(256, 256), fill_value=None) -&gt; np.ndarray:\n    \"\"\"Pads the matrix to handle full uint8 range (0-255) without bounds checks.\"\"\"\n    if self.shape == shape: return self._data\n    if fill_value is None: fill_value = np.min(self)\n    new_data = np.full(shape, fill_value, dtype=self._data.dtype)\n    r, c = self.shape\n    new_data[:r, :c] = self._data\n    return new_data\n</code></pre>"},{"location":"reference/baclib/engines/pairwise/#baclib.engines.pairwise.Trace","title":"<code>Trace</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Traceback pointer flags stored in the DP matrix during alignment.</p> Source code in <code>baclib/engines/pairwise.py</code> <pre><code>class Trace(IntEnum):\n    \"\"\"Traceback pointer flags stored in the DP matrix during alignment.\"\"\"\n    MATCH = 0\n    E = 1\n    F = 2\n    STOP = 3\n    E_EXT = 4\n    F_EXT = 8\n</code></pre>"},{"location":"reference/baclib/engines/sparse/","title":"sparse","text":""},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse","title":"<code>baclib.engines.sparse</code>","text":"<p>Engines that rely on scipy.sparse data and algorithms.</p>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.Aggregator","title":"<code>Aggregator</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Enumeration of aggregation modes for node attributes.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>class Aggregator(IntEnum):\n    \"\"\"Enumeration of aggregation modes for node attributes.\"\"\"\n    TO = 0\n    FROM = 1\n    SUM = 2\n    MEAN = 3\n    MIN = 4\n    MAX = 5\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.AttributeSource","title":"<code>AttributeSource</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Whether a graph weight is sourced from an edge or a node attribute.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>class AttributeSource(IntEnum):\n    \"\"\"Whether a graph weight is sourced from an edge or a node attribute.\"\"\"\n    EDGE = 0\n    NODE = 1\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.ClusterFinder","title":"<code>ClusterFinder</code>","text":"<p>               Bases: <code>CsGraphBuilder</code></p> <p>Algorithms for clustering and covering a Graph.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>class ClusterFinder(CsGraphBuilder):\n    \"\"\"\n    Algorithms for clustering and covering a Graph.\n    \"\"\"\n    @staticmethod\n    def otsu(similarity_matrix) -&gt; float:\n        \"\"\"\n        Calculates Otsu's threshold for a similarity matrix.\n\n        Args:\n            similarity_matrix (np.ndarray): Array of similarity scores.\n\n        Returns:\n            float: The calculated threshold.\n        \"\"\"\n        if len(similarity_matrix) == 0: return 0.5\n        hist, bin_edges = np.histogram(similarity_matrix, bins=256, range=(0.0, 1.0))\n        bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n        weight = hist.cumsum()\n        mean = (hist * bin_centers).cumsum()\n        total_mean = mean[-1]\n        total_weight = weight[-1]\n        with np.errstate(divide='ignore', invalid='ignore'):\n            mean_bg = mean / weight\n            mean_fg = (total_mean - mean) / (total_weight - weight)\n        mean_bg[np.isnan(mean_bg)] = 0.0\n        mean_fg[np.isnan(mean_fg)] = 0.0\n        w0 = weight\n        w1 = total_weight - weight\n        between_class_variance = w0 * w1 * (mean_bg - mean_fg) ** 2\n        idx = np.argmax(between_class_variance)\n        return bin_centers[idx]\n\n    @classmethod\n    def connected_components(cls, graph: Graph, policy: CsGraphPolicy) -&gt; ClusterBatch:\n        \"\"\"\n        Returns a ClusterBatch of connected components.\n        \"\"\"\n        # Use a default PathFinder to get connectivity (weights don't matter)\n        csr = cls.get_matrix(graph, policy)\n\n        n, labels = csgraph.connected_components(csr, connection='weak', directed=graph.directed)\n\n        # Sort nodes by component label to group them\n        order = np.argsort(labels)\n        sorted_labels = labels[order]\n\n        # Find boundaries where labels change\n        _, start_indices = np.unique(sorted_labels, return_index=True)\n\n        offsets = np.zeros(len(start_indices) + 1, dtype=np.int32)\n        offsets[:-1] = start_indices\n        offsets[-1] = len(labels)\n\n        # Map indices to node IDs\n        nodes_arr = np.array(graph.nodes, dtype=object)\n        flat_nodes = nodes_arr[order]\n\n        ids = np.array([b'cc_%d' % i for i in range(len(start_indices))], dtype=object)\n\n        return ClusterBatch(flat_nodes, offsets, ids=ids)\n\n    @classmethod\n    def greedy_set_cover(cls, graph: Graph, policy: CsGraphPolicy) -&gt; ClusterBatch:\n        \"\"\"\n        Solves Set Cover and returns a ClusterBatch where each cluster is a selected set.\n        Rows are treated as Sets, Columns as Elements.\n        \"\"\"\n        csr = cls.get_matrix(graph, policy)\n\n        # Use Scipy to build the inverted index (CSC) efficiently\n        csc = csr.tocsc()\n\n        # Calculate max score (max set size) for bucket initialization\n        # We can infer this from the CSR index pointers\n        max_score = 0\n        if csr.shape[0] &gt; 0:\n            max_score = np.max(np.diff(csr.indptr))\n\n        indices = _greedy_set_cover_kernel(\n            csr.shape[0], int(max_score), \n            csr.indptr, csr.indices, \n            csc.indptr, csc.indices\n        )\n\n        # Gather members\n        sizes = np.diff(csr.indptr)[indices]\n        offsets = np.zeros(len(indices) + 1, dtype=np.int32)\n        np.cumsum(sizes, out=offsets[1:])\n\n        total_size = offsets[-1]\n        flat_indices = np.empty(total_size, dtype=np.int32)\n\n        _fill_cluster_members(indices, csr.indptr, csr.indices, offsets, flat_indices)\n\n        nodes_arr = np.array(graph.nodes, dtype=object)\n        flat_nodes = nodes_arr[flat_indices]\n        representatives = nodes_arr[indices]\n\n        return ClusterBatch(flat_nodes, offsets, representatives=representatives)\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.ClusterFinder.connected_components","title":"<code>connected_components(graph, policy)</code>  <code>classmethod</code>","text":"<p>Returns a ClusterBatch of connected components.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@classmethod\ndef connected_components(cls, graph: Graph, policy: CsGraphPolicy) -&gt; ClusterBatch:\n    \"\"\"\n    Returns a ClusterBatch of connected components.\n    \"\"\"\n    # Use a default PathFinder to get connectivity (weights don't matter)\n    csr = cls.get_matrix(graph, policy)\n\n    n, labels = csgraph.connected_components(csr, connection='weak', directed=graph.directed)\n\n    # Sort nodes by component label to group them\n    order = np.argsort(labels)\n    sorted_labels = labels[order]\n\n    # Find boundaries where labels change\n    _, start_indices = np.unique(sorted_labels, return_index=True)\n\n    offsets = np.zeros(len(start_indices) + 1, dtype=np.int32)\n    offsets[:-1] = start_indices\n    offsets[-1] = len(labels)\n\n    # Map indices to node IDs\n    nodes_arr = np.array(graph.nodes, dtype=object)\n    flat_nodes = nodes_arr[order]\n\n    ids = np.array([b'cc_%d' % i for i in range(len(start_indices))], dtype=object)\n\n    return ClusterBatch(flat_nodes, offsets, ids=ids)\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.ClusterFinder.greedy_set_cover","title":"<code>greedy_set_cover(graph, policy)</code>  <code>classmethod</code>","text":"<p>Solves Set Cover and returns a ClusterBatch where each cluster is a selected set. Rows are treated as Sets, Columns as Elements.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@classmethod\ndef greedy_set_cover(cls, graph: Graph, policy: CsGraphPolicy) -&gt; ClusterBatch:\n    \"\"\"\n    Solves Set Cover and returns a ClusterBatch where each cluster is a selected set.\n    Rows are treated as Sets, Columns as Elements.\n    \"\"\"\n    csr = cls.get_matrix(graph, policy)\n\n    # Use Scipy to build the inverted index (CSC) efficiently\n    csc = csr.tocsc()\n\n    # Calculate max score (max set size) for bucket initialization\n    # We can infer this from the CSR index pointers\n    max_score = 0\n    if csr.shape[0] &gt; 0:\n        max_score = np.max(np.diff(csr.indptr))\n\n    indices = _greedy_set_cover_kernel(\n        csr.shape[0], int(max_score), \n        csr.indptr, csr.indices, \n        csc.indptr, csc.indices\n    )\n\n    # Gather members\n    sizes = np.diff(csr.indptr)[indices]\n    offsets = np.zeros(len(indices) + 1, dtype=np.int32)\n    np.cumsum(sizes, out=offsets[1:])\n\n    total_size = offsets[-1]\n    flat_indices = np.empty(total_size, dtype=np.int32)\n\n    _fill_cluster_members(indices, csr.indptr, csr.indices, offsets, flat_indices)\n\n    nodes_arr = np.array(graph.nodes, dtype=object)\n    flat_nodes = nodes_arr[flat_indices]\n    representatives = nodes_arr[indices]\n\n    return ClusterBatch(flat_nodes, offsets, representatives=representatives)\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.ClusterFinder.otsu","title":"<code>otsu(similarity_matrix)</code>  <code>staticmethod</code>","text":"<p>Calculates Otsu's threshold for a similarity matrix.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_matrix</code> <code>ndarray</code> <p>Array of similarity scores.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The calculated threshold.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@staticmethod\ndef otsu(similarity_matrix) -&gt; float:\n    \"\"\"\n    Calculates Otsu's threshold for a similarity matrix.\n\n    Args:\n        similarity_matrix (np.ndarray): Array of similarity scores.\n\n    Returns:\n        float: The calculated threshold.\n    \"\"\"\n    if len(similarity_matrix) == 0: return 0.5\n    hist, bin_edges = np.histogram(similarity_matrix, bins=256, range=(0.0, 1.0))\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n    weight = hist.cumsum()\n    mean = (hist * bin_centers).cumsum()\n    total_mean = mean[-1]\n    total_weight = weight[-1]\n    with np.errstate(divide='ignore', invalid='ignore'):\n        mean_bg = mean / weight\n        mean_fg = (total_mean - mean) / (total_weight - weight)\n    mean_bg[np.isnan(mean_bg)] = 0.0\n    mean_fg[np.isnan(mean_fg)] = 0.0\n    w0 = weight\n    w1 = total_weight - weight\n    between_class_variance = w0 * w1 * (mean_bg - mean_fg) ** 2\n    idx = np.argmax(between_class_variance)\n    return bin_centers[idx]\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.CsGraphBuilder","title":"<code>CsGraphBuilder</code>","text":"<p>Constructs scipy CSR matrices from a <code>Graph</code> according to a <code>CsGraphPolicy</code>.</p> <p>Results are cached on the <code>Graph</code> for repeated queries.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>class CsGraphBuilder:\n    \"\"\"Constructs scipy CSR matrices from a ``Graph`` according to a ``CsGraphPolicy``.\n\n    Results are cached on the ``Graph`` for repeated queries.\n    \"\"\"\n    _MAX_PENALTY = 1e12\n\n    @classmethod\n    def _build_matrix(cls, graph: Graph, policy: CsGraphPolicy) -&gt; 'csr_matrix':\n        \"\"\"\n        Optimized matrix builder using Coordinate Format (COO) -&gt; CSR.\n        Uses pre-allocation and vectorized operations where possible.\n        \"\"\"\n        # 1. Check Cache\n        if (cached := graph.matrix_cache.get(policy)) is not None: return cached\n\n        n = len(graph.nodes)\n        if n == 0: return csr_matrix((0, 0))\n\n        graph.ensure_topology()\n        u_idx, v_idx, edge_list = graph.topology_cache\n        if len(u_idx) == 0: return csr_matrix((n, n))\n\n        # Determine reduction mode for multigraphs\n        # Map TO(0)/FROM(1) to MIN(4) as 'direction' doesn't imply summation for parallel edges\n        reduce_mode = policy.aggregator.value\n        if reduce_mode &lt;= 1: reduce_mode = 4\n\n        if policy.source == AttributeSource.NODE:\n            node_vals = np.full(n, policy.default, dtype=np.float64)\n            for n_id, attrs in graph.node_attributes.items():\n                if (idx := graph.node_to_idx.get(n_id)) is not None:\n                    val = attrs.get(policy.attr)\n                    if val is not None: node_vals[idx] = float(val)\n\n            agg_mode = policy.aggregator.value\n\n            rows, cols, data = _build_node_graph_kernel(\n                u_idx, v_idx, node_vals, agg_mode, graph.directed, policy.invert, cls._MAX_PENALTY\n            )\n        else:\n            # Optimization: Pre-allocate array to avoid list overhead\n            n_edges = len(edge_list)\n            vals_arr = np.full(n_edges, policy.default, dtype=np.float64)\n            attr = policy.attr\n            for i, e in enumerate(edge_list):\n                if (val := e.attributes.get(attr)) is not None: vals_arr[i] = float(val)\n\n            rows, cols, data = _build_edge_graph_kernel(\n                u_idx, v_idx, vals_arr, graph.directed, policy.invert, cls._MAX_PENALTY\n            )\n\n        # Multigraph Reduction: Sort and Reduce\n        if len(rows) &gt; 0:\n            order = np.lexsort((cols, rows))\n            rows, cols, data = rows[order], cols[order], data[order]\n            rows, cols, data = _reduce_duplicates_kernel(rows, cols, data, reduce_mode)\n\n        csr = csr_matrix((data, (rows, cols)), shape=(n, n))\n        graph.matrix_cache[policy] = csr\n        return csr\n\n    @classmethod\n    def get_matrix(cls, graph: Graph, policy: CsGraphPolicy) -&gt; csr_matrix:\n        \"\"\"\n        Fetches the matrix corresponding to the policy from the cache,\n        or builds and caches it if it does not exist.\n\n        Args:\n            policy: The WeightingPolicy defining how to build the matrix.\n\n        Returns:\n            A scipy.sparse.csr_matrix representing the graph weights.\n        \"\"\"\n        if (cached := graph.matrix_cache.get(policy)) is None:\n            graph.matrix_cache[policy] = (cached := cls._build_matrix(graph, policy))\n        return cached\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.CsGraphBuilder.get_matrix","title":"<code>get_matrix(graph, policy)</code>  <code>classmethod</code>","text":"<p>Fetches the matrix corresponding to the policy from the cache, or builds and caches it if it does not exist.</p> <p>Parameters:</p> Name Type Description Default <code>policy</code> <code>CsGraphPolicy</code> <p>The WeightingPolicy defining how to build the matrix.</p> required <p>Returns:</p> Type Description <code>csr_matrix</code> <p>A scipy.sparse.csr_matrix representing the graph weights.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@classmethod\ndef get_matrix(cls, graph: Graph, policy: CsGraphPolicy) -&gt; csr_matrix:\n    \"\"\"\n    Fetches the matrix corresponding to the policy from the cache,\n    or builds and caches it if it does not exist.\n\n    Args:\n        policy: The WeightingPolicy defining how to build the matrix.\n\n    Returns:\n        A scipy.sparse.csr_matrix representing the graph weights.\n    \"\"\"\n    if (cached := graph.matrix_cache.get(policy)) is None:\n        graph.matrix_cache[policy] = (cached := cls._build_matrix(graph, policy))\n    return cached\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.CsGraphPolicy","title":"<code>CsGraphPolicy</code>  <code>dataclass</code>","text":"<p>Defines HOW a matrix should be constructed. Frozen = Immutable and Hashable (can be used as a cache key).</p> <p>Attributes:</p> Name Type Description <code>attr</code> <code>bytes</code> <p>The attribute name to use for weighting.</p> <code>source</code> <code>Union[AttributeSource, str]</code> <p>Whether to pull the attribute from the 'edge' or the 'node'.</p> <code>aggregator</code> <code>Union[Aggregator, str, int]</code> <p>How to combine node attributes if source='node'.</p> <code>invert</code> <code>bool</code> <p>If True, uses 1/value as the weight (useful for converting similarity to distance).</p> <code>default</code> <code>float</code> <p>Default value if the attribute is missing.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@dataclass(frozen=True, slots=True)\nclass CsGraphPolicy:\n    \"\"\"\n    Defines HOW a matrix should be constructed.\n    Frozen = Immutable and Hashable (can be used as a cache key).\n\n    Attributes:\n        attr: The attribute name to use for weighting.\n        source: Whether to pull the attribute from the 'edge' or the 'node'.\n        aggregator: How to combine node attributes if source='node'.\n        invert: If True, uses 1/value as the weight (useful for converting similarity to distance).\n        default: Default value if the attribute is missing.\n    \"\"\"\n    attr: bytes\n    source: Union[AttributeSource, str] = AttributeSource.EDGE\n    aggregator: Union[Aggregator, str, int] = Aggregator.TO\n    invert: bool = False\n    default: float = 1.0\n\n    def __post_init__(self):  # Robust Aggregator Normalization\n        # Source Normalization\n        if not isinstance(self.source, AttributeSource):\n            try:\n                val = AttributeSource[self.source.upper()]\n                object.__setattr__(self, 'source', val)\n            except KeyError:\n                raise ValueError(f\"Invalid source: {self.source}\")\n\n        if isinstance(self.aggregator, Aggregator):\n            return\n\n        val = None\n        if isinstance(self.aggregator, str):\n            try:\n                val = Aggregator[self.aggregator.upper()]\n            except KeyError:\n                raise ValueError(f\"Invalid aggregator name: {self.aggregator}\")\n        else:\n            try:\n                val = Aggregator(self.aggregator)\n            except ValueError:\n                raise ValueError(f\"Invalid aggregator value: {self.aggregator}\")\n\n        object.__setattr__(self, 'aggregator', val)\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.PathAlgorithm","title":"<code>PathAlgorithm</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Shortest-path algorithm selector wrapping scipy.sparse.csgraph routines.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>class PathAlgorithm(str, Enum):\n    \"\"\"Shortest-path algorithm selector wrapping scipy.sparse.csgraph routines.\"\"\"\n    DIJKSTRA = 'D'\n    BELLMAN_FORD = 'BF'\n    JOHNSON = 'J'\n    FLOYD_WARSHALL = 'FW'\n\n    @classmethod\n    def normalize(cls, arg: Union[str, 'PathAlgorithm']) -&gt; 'PathAlgorithm':\n        \"\"\"Normalizes a string or enum member to a PathAlgorithm.\"\"\"\n        if isinstance(arg, cls): return arg\n        s = str(arg).upper()\n        try: return cls[s]  # Try name (e.g. DIJKSTRA)\n        except KeyError: pass\n        try: return cls(s)  # Try value (e.g. D)\n        except ValueError: raise ValueError(f\"Unknown algorithm: {arg}\")\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.PathAlgorithm.normalize","title":"<code>normalize(arg)</code>  <code>classmethod</code>","text":"<p>Normalizes a string or enum member to a PathAlgorithm.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@classmethod\ndef normalize(cls, arg: Union[str, 'PathAlgorithm']) -&gt; 'PathAlgorithm':\n    \"\"\"Normalizes a string or enum member to a PathAlgorithm.\"\"\"\n    if isinstance(arg, cls): return arg\n    s = str(arg).upper()\n    try: return cls[s]  # Try name (e.g. DIJKSTRA)\n    except KeyError: pass\n    try: return cls(s)  # Try value (e.g. D)\n    except ValueError: raise ValueError(f\"Unknown algorithm: {arg}\")\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.PathFinder","title":"<code>PathFinder</code>","text":"<p>               Bases: <code>CsGraphBuilder</code></p> <p>Shortest-path and traversal queries over a weighted <code>Graph</code>.</p> <p>Inherits <code>CsGraphBuilder</code> to transparently build and cache CSR matrices.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>class PathFinder(CsGraphBuilder):\n    \"\"\"Shortest-path and traversal queries over a weighted ``Graph``.\n\n    Inherits ``CsGraphBuilder`` to transparently build and cache CSR matrices.\n    \"\"\"\n    _REGISTRY: dict[PathAlgorithm, Callable] = {\n        PathAlgorithm.DIJKSTRA: csgraph.dijkstra,\n        PathAlgorithm.FLOYD_WARSHALL: csgraph.floyd_warshall,\n        PathAlgorithm.JOHNSON: csgraph.johnson,\n        PathAlgorithm.BELLMAN_FORD: csgraph.bellman_ford\n    }\n\n    @classmethod\n    def _get_pathfinder(cls, algo: PathAlgorithm) -&gt; Callable:\n        if finder := cls._REGISTRY.get(algo): return finder\n        raise ValueError(f\"Algorithm {algo} is not available.\")\n\n    @staticmethod\n    def _reconstruct_path(graph: Graph, start_idx: int, end_idx: int, predecessors: np.ndarray, \n                          cost: float) -&gt; Optional[Path]:\n        \"\"\"Helper to reconstruct path from predecessor array.\"\"\"\n        # Use Numba kernel for fast array traversal\n        path_indices = _reconstruct_path_kernel_single(predecessors, start_idx, end_idx)\n        if len(path_indices) == 0: return None\n        path_nodes = [graph.nodes[i] for i in path_indices]  # Kernel returns Start-&gt;End\n        return Path(path_nodes, cost)\n\n    @classmethod\n    def shortest_path(cls, graph: Graph, policy: CsGraphPolicy, start: bytes, end: bytes,\n                      algorithm: Union[str, PathAlgorithm] = PathAlgorithm.DIJKSTRA,\n                      max_cost: float = np.inf) -&gt; Optional[Path]:\n        \"\"\"\n        Finds the shortest path between two nodes using the specified algorithm.\n        \"\"\"\n        if (start_idx := graph.node_to_idx.get(start)) is None or (end_idx := graph.node_to_idx.get(end)) is None:\n            return None\n        csr =cls.get_matrix(graph, policy)\n        finder = cls._get_pathfinder(algo := PathAlgorithm.normalize(algorithm))\n        try:\n            if algo == PathAlgorithm.DIJKSTRA:\n                dist, preds = finder(csr, directed=graph.directed, indices=start_idx, return_predecessors=True,\n                                     limit=max_cost)\n            elif algo == PathAlgorithm.FLOYD_WARSHALL:\n                dist_mat, pred_mat = finder(csr, directed=graph.directed, return_predecessors=True)\n                dist = dist_mat[start_idx, end_idx]\n                preds = pred_mat[start_idx]\n            else:  # BF, J\n                dist, preds = finder(csr, directed=graph.directed, indices=start_idx, return_predecessors=True)\n        except ValueError:\n            return None  # Negative cycle\n        # Handle result extraction\n        d = dist if np.isscalar(dist) else dist[end_idx]\n        if np.isinf(d) or d &gt;= (cls._MAX_PENALTY * 0.1): return None\n        return cls._reconstruct_path(graph, start_idx, end_idx, preds, float(d))\n\n    @classmethod\n    def traverse(cls, graph: Graph, policy: CsGraphPolicy, start: bytes, mode: Literal['DFS', 'BFS'] = 'DFS',\n                 return_predecessors: bool = True) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n        \"\"\"\n        Returns a breadth-first or depth-first traversal of the graph.\n        Wraps scipy.sparse.csgraph.breadth_first_order or scipy.sparse.csgraph.depth_first_order.\n\n        Args:\n            graph: The graph.\n            policy: Matrix construction policy.\n            start: Start node ID.\n            mode: Traverse mode.\n            return_predecessors: If True, returns (node_indices, predecessors).\n\n        Returns:\n            node_indices: Array of visited node indices in order.\n            predecessors: Array of predecessor indices (if requested).\n        \"\"\"\n        if (start_idx := graph.node_to_idx.get(start)) is None:\n            raise ValueError(f\"Start node {start} not found\")\n        csr = cls.get_matrix(graph, policy)\n        func = csgraph.depth_first_order if mode == 'DFS' else csgraph.breadth_first_order\n        return func(csr, start_idx, directed=graph.directed, return_predecessors=return_predecessors)\n\n\n    @classmethod\n    def find_paths(cls, graph: Graph, policy: CsGraphPolicy, start: bytes, end: bytes, max_hops: int = 10) -&gt; PathBatch:\n        \"\"\"\n        Finds all simple paths between start and end nodes within a hop limit.\n        Useful for exploring local graph topology (e.g., bubbles) between anchors.\n        \"\"\"\n        s_idx = graph.node_to_idx.get(start)\n        e_idx = graph.node_to_idx.get(end)\n        if s_idx is None or e_idx is None: return PathBatch.from_paths([])\n\n        # Access CSR internals directly for speed\n        csr = cls.get_matrix(graph, policy)\n        indices = csr.indices\n        indptr = csr.indptr\n        data = csr.data\n\n        results = []\n\n        # Optimization: Recursive Backtracking to avoid list copying overhead\n        # Using a single mutable list 'path' is much faster than 'path + [neighbor]'\n        def _dfs(curr, path, cost):\n            if curr == e_idx:\n                nodes = [graph.nodes[i] for i in path]\n                results.append(Path(nodes, cost))\n                return\n\n            if len(path) &gt; max_hops: return\n\n            for i in range(indptr[curr], indptr[curr + 1]):\n                neighbor = indices[i]\n                if neighbor not in path:  # O(Depth) check, acceptable for small max_hops\n                    weight = data[i]\n                    path.append(neighbor)\n                    _dfs(neighbor, path, cost + weight)\n                    path.pop()\n\n        _dfs(s_idx, [s_idx], 0.0)\n        return PathBatch.from_paths(results)\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.PathFinder.find_paths","title":"<code>find_paths(graph, policy, start, end, max_hops=10)</code>  <code>classmethod</code>","text":"<p>Finds all simple paths between start and end nodes within a hop limit. Useful for exploring local graph topology (e.g., bubbles) between anchors.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@classmethod\ndef find_paths(cls, graph: Graph, policy: CsGraphPolicy, start: bytes, end: bytes, max_hops: int = 10) -&gt; PathBatch:\n    \"\"\"\n    Finds all simple paths between start and end nodes within a hop limit.\n    Useful for exploring local graph topology (e.g., bubbles) between anchors.\n    \"\"\"\n    s_idx = graph.node_to_idx.get(start)\n    e_idx = graph.node_to_idx.get(end)\n    if s_idx is None or e_idx is None: return PathBatch.from_paths([])\n\n    # Access CSR internals directly for speed\n    csr = cls.get_matrix(graph, policy)\n    indices = csr.indices\n    indptr = csr.indptr\n    data = csr.data\n\n    results = []\n\n    # Optimization: Recursive Backtracking to avoid list copying overhead\n    # Using a single mutable list 'path' is much faster than 'path + [neighbor]'\n    def _dfs(curr, path, cost):\n        if curr == e_idx:\n            nodes = [graph.nodes[i] for i in path]\n            results.append(Path(nodes, cost))\n            return\n\n        if len(path) &gt; max_hops: return\n\n        for i in range(indptr[curr], indptr[curr + 1]):\n            neighbor = indices[i]\n            if neighbor not in path:  # O(Depth) check, acceptable for small max_hops\n                weight = data[i]\n                path.append(neighbor)\n                _dfs(neighbor, path, cost + weight)\n                path.pop()\n\n    _dfs(s_idx, [s_idx], 0.0)\n    return PathBatch.from_paths(results)\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.PathFinder.shortest_path","title":"<code>shortest_path(graph, policy, start, end, algorithm=PathAlgorithm.DIJKSTRA, max_cost=np.inf)</code>  <code>classmethod</code>","text":"<p>Finds the shortest path between two nodes using the specified algorithm.</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@classmethod\ndef shortest_path(cls, graph: Graph, policy: CsGraphPolicy, start: bytes, end: bytes,\n                  algorithm: Union[str, PathAlgorithm] = PathAlgorithm.DIJKSTRA,\n                  max_cost: float = np.inf) -&gt; Optional[Path]:\n    \"\"\"\n    Finds the shortest path between two nodes using the specified algorithm.\n    \"\"\"\n    if (start_idx := graph.node_to_idx.get(start)) is None or (end_idx := graph.node_to_idx.get(end)) is None:\n        return None\n    csr =cls.get_matrix(graph, policy)\n    finder = cls._get_pathfinder(algo := PathAlgorithm.normalize(algorithm))\n    try:\n        if algo == PathAlgorithm.DIJKSTRA:\n            dist, preds = finder(csr, directed=graph.directed, indices=start_idx, return_predecessors=True,\n                                 limit=max_cost)\n        elif algo == PathAlgorithm.FLOYD_WARSHALL:\n            dist_mat, pred_mat = finder(csr, directed=graph.directed, return_predecessors=True)\n            dist = dist_mat[start_idx, end_idx]\n            preds = pred_mat[start_idx]\n        else:  # BF, J\n            dist, preds = finder(csr, directed=graph.directed, indices=start_idx, return_predecessors=True)\n    except ValueError:\n        return None  # Negative cycle\n    # Handle result extraction\n    d = dist if np.isscalar(dist) else dist[end_idx]\n    if np.isinf(d) or d &gt;= (cls._MAX_PENALTY * 0.1): return None\n    return cls._reconstruct_path(graph, start_idx, end_idx, preds, float(d))\n</code></pre>"},{"location":"reference/baclib/engines/sparse/#baclib.engines.sparse.PathFinder.traverse","title":"<code>traverse(graph, policy, start, mode='DFS', return_predecessors=True)</code>  <code>classmethod</code>","text":"<p>Returns a breadth-first or depth-first traversal of the graph. Wraps scipy.sparse.csgraph.breadth_first_order or scipy.sparse.csgraph.depth_first_order.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>Graph</code> <p>The graph.</p> required <code>policy</code> <code>CsGraphPolicy</code> <p>Matrix construction policy.</p> required <code>start</code> <code>bytes</code> <p>Start node ID.</p> required <code>mode</code> <code>Literal['DFS', 'BFS']</code> <p>Traverse mode.</p> <code>'DFS'</code> <code>return_predecessors</code> <code>bool</code> <p>If True, returns (node_indices, predecessors).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>node_indices</code> <code>Union[ndarray, tuple[ndarray, ndarray]]</code> <p>Array of visited node indices in order.</p> <code>predecessors</code> <code>Union[ndarray, tuple[ndarray, ndarray]]</code> <p>Array of predecessor indices (if requested).</p> Source code in <code>baclib/engines/sparse.py</code> <pre><code>@classmethod\ndef traverse(cls, graph: Graph, policy: CsGraphPolicy, start: bytes, mode: Literal['DFS', 'BFS'] = 'DFS',\n             return_predecessors: bool = True) -&gt; Union[np.ndarray, tuple[np.ndarray, np.ndarray]]:\n    \"\"\"\n    Returns a breadth-first or depth-first traversal of the graph.\n    Wraps scipy.sparse.csgraph.breadth_first_order or scipy.sparse.csgraph.depth_first_order.\n\n    Args:\n        graph: The graph.\n        policy: Matrix construction policy.\n        start: Start node ID.\n        mode: Traverse mode.\n        return_predecessors: If True, returns (node_indices, predecessors).\n\n    Returns:\n        node_indices: Array of visited node indices in order.\n        predecessors: Array of predecessor indices (if requested).\n    \"\"\"\n    if (start_idx := graph.node_to_idx.get(start)) is None:\n        raise ValueError(f\"Start node {start} not found\")\n    csr = cls.get_matrix(graph, policy)\n    func = csgraph.depth_first_order if mode == 'DFS' else csgraph.breadth_first_order\n    return func(csr, start_idx, directed=graph.directed, return_predecessors=return_predecessors)\n</code></pre>"},{"location":"reference/baclib/engines/variant/","title":"variant","text":""},{"location":"reference/baclib/engines/variant/#baclib.engines.variant","title":"<code>baclib.engines.variant</code>","text":"<p>Variant calling engine for identifying SNPs, indels, and pileup-based consensus variants from alignments.</p>"},{"location":"reference/baclib/engines/variant/#baclib.engines.variant.VariantCaller","title":"<code>VariantCaller</code>","text":"<p>Engine for calling variants from alignments.</p> Source code in <code>baclib/engines/variant.py</code> <pre><code>class VariantCaller:\n    \"\"\"\n    Engine for calling variants from alignments.\n    \"\"\"\n    @staticmethod\n    def call_variants(alignments: AlignmentBatch, queries: SeqBatch, targets: SeqBatch) -&gt; MutationBatch:\n        \"\"\"\n        Identifies mutations (SNPs, Indels) from an AlignmentBatch.\n\n        Args:\n            alignments: The alignment batch.\n            queries: The query sequences (SeqBatch).\n            targets: The target sequences (SeqBatch).\n\n        Returns:\n            A MutationBatch containing all identified variants.\n        \"\"\"\n        # We need to iterate alignments and parse CIGARs\n        # This is hard to fully vectorize because CIGARs are variable length strings\n        # and produce variable numbers of mutations.\n\n        mut_starts = []\n        mut_ends = []\n        mut_strands = []\n        mut_refs = []\n        mut_alts = []\n\n        # Access raw arrays for speed\n        q_data, q_starts, _ = queries.arrays\n        t_data, t_starts, _ = targets.arrays\n\n        # Iterate alignments\n        # Note: We assume alignments are valid and indices match the batches\n        for i in range(len(alignments)):\n            # Get Alignment Metadata\n            q_idx = alignments.q_indices[i]\n            t_idx = alignments.t_indices[i]\n\n            # Global offsets in SeqBatch\n            q_offset = q_starts[q_idx]\n            t_offset = t_starts[t_idx]\n\n            # Alignment start coords (0-based on sequence)\n            q_aln_start = alignments.q_starts[i]\n            t_aln_start = alignments.t_starts[i]\n            t_strand = alignments.t_strands[i]\n\n            cigar = alignments.cigars[i]\n            if not cigar: continue\n\n            # Parse CIGAR\n            # We track current position in Query and Target\n            curr_q = q_aln_start\n            curr_t = t_aln_start\n\n            ops, counts = Cigar.parse_into_arrays(cigar)\n\n            for k in range(len(ops)):\n                op = ops[k]\n                count = counts[k]\n\n                if op == CigarOp.EQ: # Match (=)\n                    curr_q += count\n                    curr_t += count\n                elif op == CigarOp.X: # Mismatch (X)\n                    # We have 'count' mismatches. \n                    # Ideally we check base-by-base to see if they are contiguous SNPs or MNP\n                    # For simplicity, we treat contiguous X as one MNP block\n\n                    # Extract Ref (Target) and Alt (Query)\n                    # Note: We need to handle strand if query is RC. \n                    # But usually AlignmentBatch stores coords relative to forward strand of SeqBatch entry.\n\n                    ref_slice = t_data[t_offset + curr_t : t_offset + curr_t + count]\n                    alt_slice = q_data[q_offset + curr_q : q_offset + curr_q + count]\n\n                    # Create Mutation\n                    # Interval is on Target (Ref)\n                    mut_starts.append(curr_t)\n                    mut_ends.append(curr_t + count)\n                    mut_strands.append(t_strand)\n                    mut_refs.append(targets.alphabet.seq_from(ref_slice))\n                    mut_alts.append(queries.alphabet.seq_from(alt_slice))\n\n                    curr_q += count\n                    curr_t += count\n\n                elif op == CigarOp.I: # Insertion (in Query, gap in Target)\n                    alt_slice = q_data[q_offset + curr_q : q_offset + curr_q + count]\n                    mut_starts.append(curr_t)\n                    mut_ends.append(curr_t)\n                    mut_strands.append(t_strand)\n                    mut_refs.append(targets.alphabet.empty_seq())\n                    mut_alts.append(queries.alphabet.seq_from(alt_slice))\n                    curr_q += count\n\n                elif op == CigarOp.D: # Deletion (gap in Query, present in Target)\n                    ref_slice = t_data[t_offset + curr_t : t_offset + curr_t + count]\n                    mut_starts.append(curr_t)\n                    mut_ends.append(curr_t + count)\n                    mut_strands.append(t_strand)\n                    mut_refs.append(targets.alphabet.seq_from(ref_slice))\n                    mut_alts.append(queries.alphabet.empty_seq())\n                    curr_t += count\n\n                elif op == CigarOp.M: # Match or Mismatch (Ambiguous)\n                    # We need to compare sequences to find SNPs\n                    # This is expensive in Python loop. \n                    # Ideally we use extended CIGAR (=/X) from the start.\n                    # If forced to use M, we would need a kernel to scan q/t arrays.\n                    curr_q += count\n                    curr_t += count\n\n                elif op == CigarOp.S: # Soft Clip\n                    curr_q += count\n                    # Soft clips consume query but not target, and are not mutations\n\n        # Construct Batch\n        if not mut_starts:\n            return MutationBatch.empty()\n\n        return MutationBatch(\n            IntervalBatch(\n                np.array(mut_starts, dtype=np.int32),\n                np.array(mut_ends, dtype=np.int32),\n                np.array(mut_strands, dtype=np.int32)\n            ),\n            targets.alphabet.batch_from(mut_refs),\n            queries.alphabet.batch_from(mut_alts)\n        )\n\n    @staticmethod\n    def call_pileup_variants(alignments: AlignmentBatch,\n                             queries: SeqBatch, targets: SeqBatch,\n                             min_depth: int = 10, min_freq: float = 0.2) -&gt; MutationBatch:\n        \"\"\"\n        Calls consensus variants (SNPs/Deletions) based on alignment pileup statistics.\n\n        Args:\n            alignments: The alignment batch.\n            queries: The query sequences (SeqBatch).\n            targets: The target sequences (SeqBatch).\n            min_depth: Minimum coverage depth to call a variant.\n            min_freq: Minimum allele frequency (count / depth) to call a variant.\n\n        Returns:\n            A MutationBatch of consensus variants.\n        \"\"\"\n        mut_starts = []\n        mut_ends = []\n        mut_strands = []\n        mut_refs = []\n        mut_alts = []\n\n        q_data, q_starts, _ = queries.arrays\n        t_data, t_starts, t_lengths = targets.arrays\n\n        # RC Table for handling reverse strand alignments\n        rc_table = queries.alphabet.complement\n        if rc_table is None:\n            # Fallback identity if no complement (e.g. Protein), though pileup usually implies DNA\n            rc_table = np.arange(len(queries.alphabet), dtype=np.uint8)\n\n        # Process per target to keep memory usage low\n        for t_idx, group_indices in alignments.group_by(by_target=True):\n            t_len = t_lengths[t_idx]\n            t_offset = t_starts[t_idx]\n\n            # Counts: [Pos, Base]. Base 0-3 (ACGT), 4 (Deletion)\n            # Assuming DNA alphabet size 4. If larger, we need to adjust.\n            n_sym = len(targets.alphabet)\n            counts = np.zeros((t_len, n_sym + 1), dtype=np.int32)\n\n            # 1. Accumulate Counts\n            for aln_i in group_indices:\n                q_idx = alignments.q_indices[aln_i]\n                q_offset = q_starts[q_idx]\n\n                q_s = alignments.q_starts[aln_i]\n                t_s = alignments.t_starts[aln_i]\n                strand = alignments.t_strands[aln_i] # Relative strand\n\n                cigar = alignments.cigars[aln_i]\n                if not cigar: continue\n\n                _pileup_add_kernel(\n                    counts, q_data, q_offset, q_s, t_s, \n                    cigar, Cigar._BYTE_TO_OP,\n                    strand, rc_table\n                )\n\n            # 2. Call Variants\n            # We scan the counts matrix for non-ref alleles\n            ref_seq_data = t_data[t_offset : t_offset + t_len]\n\n            for pos in range(t_len):\n                total = np.sum(counts[pos])\n                if total &lt; min_depth: continue\n\n                ref_base = ref_seq_data[pos]\n\n                # Check for variants\n                for allele in range(n_sym + 1):\n                    if allele == ref_base: continue\n\n                    count = counts[pos, allele]\n                    if count == 0: continue\n\n                    freq = count / total\n                    if freq &gt;= min_freq:\n                        # Found a variant!\n                        mut_starts.append(pos)\n                        mut_ends.append(pos + 1)\n                        mut_strands.append(1)\n\n                        # Ref Seq\n                        mut_refs.append(targets.alphabet.seq_from(ref_seq_data[pos:pos+1]))\n\n                        # Alt Seq\n                        if allele == n_sym: # Deletion\n                            mut_alts.append(queries.alphabet.empty_seq())\n                        else:\n                            # We need to construct a 1-byte array for the alt base\n                            alt_arr = np.array([allele], dtype=np.uint8)\n                            mut_alts.append(queries.alphabet.seq_from(alt_arr))\n\n        if not mut_starts:\n            return MutationBatch.empty()\n\n        return MutationBatch(\n            IntervalBatch(\n                np.array(mut_starts, dtype=np.int32),\n                np.array(mut_ends, dtype=np.int32),\n                np.array(mut_strands, dtype=np.int32)\n            ),\n            targets.alphabet.batch_from(mut_refs),\n            queries.alphabet.batch_from(mut_alts)\n        )\n</code></pre>"},{"location":"reference/baclib/engines/variant/#baclib.engines.variant.VariantCaller.call_pileup_variants","title":"<code>call_pileup_variants(alignments, queries, targets, min_depth=10, min_freq=0.2)</code>  <code>staticmethod</code>","text":"<p>Calls consensus variants (SNPs/Deletions) based on alignment pileup statistics.</p> <p>Parameters:</p> Name Type Description Default <code>alignments</code> <code>AlignmentBatch</code> <p>The alignment batch.</p> required <code>queries</code> <code>SeqBatch</code> <p>The query sequences (SeqBatch).</p> required <code>targets</code> <code>SeqBatch</code> <p>The target sequences (SeqBatch).</p> required <code>min_depth</code> <code>int</code> <p>Minimum coverage depth to call a variant.</p> <code>10</code> <code>min_freq</code> <code>float</code> <p>Minimum allele frequency (count / depth) to call a variant.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>MutationBatch</code> <p>A MutationBatch of consensus variants.</p> Source code in <code>baclib/engines/variant.py</code> <pre><code>@staticmethod\ndef call_pileup_variants(alignments: AlignmentBatch,\n                         queries: SeqBatch, targets: SeqBatch,\n                         min_depth: int = 10, min_freq: float = 0.2) -&gt; MutationBatch:\n    \"\"\"\n    Calls consensus variants (SNPs/Deletions) based on alignment pileup statistics.\n\n    Args:\n        alignments: The alignment batch.\n        queries: The query sequences (SeqBatch).\n        targets: The target sequences (SeqBatch).\n        min_depth: Minimum coverage depth to call a variant.\n        min_freq: Minimum allele frequency (count / depth) to call a variant.\n\n    Returns:\n        A MutationBatch of consensus variants.\n    \"\"\"\n    mut_starts = []\n    mut_ends = []\n    mut_strands = []\n    mut_refs = []\n    mut_alts = []\n\n    q_data, q_starts, _ = queries.arrays\n    t_data, t_starts, t_lengths = targets.arrays\n\n    # RC Table for handling reverse strand alignments\n    rc_table = queries.alphabet.complement\n    if rc_table is None:\n        # Fallback identity if no complement (e.g. Protein), though pileup usually implies DNA\n        rc_table = np.arange(len(queries.alphabet), dtype=np.uint8)\n\n    # Process per target to keep memory usage low\n    for t_idx, group_indices in alignments.group_by(by_target=True):\n        t_len = t_lengths[t_idx]\n        t_offset = t_starts[t_idx]\n\n        # Counts: [Pos, Base]. Base 0-3 (ACGT), 4 (Deletion)\n        # Assuming DNA alphabet size 4. If larger, we need to adjust.\n        n_sym = len(targets.alphabet)\n        counts = np.zeros((t_len, n_sym + 1), dtype=np.int32)\n\n        # 1. Accumulate Counts\n        for aln_i in group_indices:\n            q_idx = alignments.q_indices[aln_i]\n            q_offset = q_starts[q_idx]\n\n            q_s = alignments.q_starts[aln_i]\n            t_s = alignments.t_starts[aln_i]\n            strand = alignments.t_strands[aln_i] # Relative strand\n\n            cigar = alignments.cigars[aln_i]\n            if not cigar: continue\n\n            _pileup_add_kernel(\n                counts, q_data, q_offset, q_s, t_s, \n                cigar, Cigar._BYTE_TO_OP,\n                strand, rc_table\n            )\n\n        # 2. Call Variants\n        # We scan the counts matrix for non-ref alleles\n        ref_seq_data = t_data[t_offset : t_offset + t_len]\n\n        for pos in range(t_len):\n            total = np.sum(counts[pos])\n            if total &lt; min_depth: continue\n\n            ref_base = ref_seq_data[pos]\n\n            # Check for variants\n            for allele in range(n_sym + 1):\n                if allele == ref_base: continue\n\n                count = counts[pos, allele]\n                if count == 0: continue\n\n                freq = count / total\n                if freq &gt;= min_freq:\n                    # Found a variant!\n                    mut_starts.append(pos)\n                    mut_ends.append(pos + 1)\n                    mut_strands.append(1)\n\n                    # Ref Seq\n                    mut_refs.append(targets.alphabet.seq_from(ref_seq_data[pos:pos+1]))\n\n                    # Alt Seq\n                    if allele == n_sym: # Deletion\n                        mut_alts.append(queries.alphabet.empty_seq())\n                    else:\n                        # We need to construct a 1-byte array for the alt base\n                        alt_arr = np.array([allele], dtype=np.uint8)\n                        mut_alts.append(queries.alphabet.seq_from(alt_arr))\n\n    if not mut_starts:\n        return MutationBatch.empty()\n\n    return MutationBatch(\n        IntervalBatch(\n            np.array(mut_starts, dtype=np.int32),\n            np.array(mut_ends, dtype=np.int32),\n            np.array(mut_strands, dtype=np.int32)\n        ),\n        targets.alphabet.batch_from(mut_refs),\n        queries.alphabet.batch_from(mut_alts)\n    )\n</code></pre>"},{"location":"reference/baclib/engines/variant/#baclib.engines.variant.VariantCaller.call_variants","title":"<code>call_variants(alignments, queries, targets)</code>  <code>staticmethod</code>","text":"<p>Identifies mutations (SNPs, Indels) from an AlignmentBatch.</p> <p>Parameters:</p> Name Type Description Default <code>alignments</code> <code>AlignmentBatch</code> <p>The alignment batch.</p> required <code>queries</code> <code>SeqBatch</code> <p>The query sequences (SeqBatch).</p> required <code>targets</code> <code>SeqBatch</code> <p>The target sequences (SeqBatch).</p> required <p>Returns:</p> Type Description <code>MutationBatch</code> <p>A MutationBatch containing all identified variants.</p> Source code in <code>baclib/engines/variant.py</code> <pre><code>@staticmethod\ndef call_variants(alignments: AlignmentBatch, queries: SeqBatch, targets: SeqBatch) -&gt; MutationBatch:\n    \"\"\"\n    Identifies mutations (SNPs, Indels) from an AlignmentBatch.\n\n    Args:\n        alignments: The alignment batch.\n        queries: The query sequences (SeqBatch).\n        targets: The target sequences (SeqBatch).\n\n    Returns:\n        A MutationBatch containing all identified variants.\n    \"\"\"\n    # We need to iterate alignments and parse CIGARs\n    # This is hard to fully vectorize because CIGARs are variable length strings\n    # and produce variable numbers of mutations.\n\n    mut_starts = []\n    mut_ends = []\n    mut_strands = []\n    mut_refs = []\n    mut_alts = []\n\n    # Access raw arrays for speed\n    q_data, q_starts, _ = queries.arrays\n    t_data, t_starts, _ = targets.arrays\n\n    # Iterate alignments\n    # Note: We assume alignments are valid and indices match the batches\n    for i in range(len(alignments)):\n        # Get Alignment Metadata\n        q_idx = alignments.q_indices[i]\n        t_idx = alignments.t_indices[i]\n\n        # Global offsets in SeqBatch\n        q_offset = q_starts[q_idx]\n        t_offset = t_starts[t_idx]\n\n        # Alignment start coords (0-based on sequence)\n        q_aln_start = alignments.q_starts[i]\n        t_aln_start = alignments.t_starts[i]\n        t_strand = alignments.t_strands[i]\n\n        cigar = alignments.cigars[i]\n        if not cigar: continue\n\n        # Parse CIGAR\n        # We track current position in Query and Target\n        curr_q = q_aln_start\n        curr_t = t_aln_start\n\n        ops, counts = Cigar.parse_into_arrays(cigar)\n\n        for k in range(len(ops)):\n            op = ops[k]\n            count = counts[k]\n\n            if op == CigarOp.EQ: # Match (=)\n                curr_q += count\n                curr_t += count\n            elif op == CigarOp.X: # Mismatch (X)\n                # We have 'count' mismatches. \n                # Ideally we check base-by-base to see if they are contiguous SNPs or MNP\n                # For simplicity, we treat contiguous X as one MNP block\n\n                # Extract Ref (Target) and Alt (Query)\n                # Note: We need to handle strand if query is RC. \n                # But usually AlignmentBatch stores coords relative to forward strand of SeqBatch entry.\n\n                ref_slice = t_data[t_offset + curr_t : t_offset + curr_t + count]\n                alt_slice = q_data[q_offset + curr_q : q_offset + curr_q + count]\n\n                # Create Mutation\n                # Interval is on Target (Ref)\n                mut_starts.append(curr_t)\n                mut_ends.append(curr_t + count)\n                mut_strands.append(t_strand)\n                mut_refs.append(targets.alphabet.seq_from(ref_slice))\n                mut_alts.append(queries.alphabet.seq_from(alt_slice))\n\n                curr_q += count\n                curr_t += count\n\n            elif op == CigarOp.I: # Insertion (in Query, gap in Target)\n                alt_slice = q_data[q_offset + curr_q : q_offset + curr_q + count]\n                mut_starts.append(curr_t)\n                mut_ends.append(curr_t)\n                mut_strands.append(t_strand)\n                mut_refs.append(targets.alphabet.empty_seq())\n                mut_alts.append(queries.alphabet.seq_from(alt_slice))\n                curr_q += count\n\n            elif op == CigarOp.D: # Deletion (gap in Query, present in Target)\n                ref_slice = t_data[t_offset + curr_t : t_offset + curr_t + count]\n                mut_starts.append(curr_t)\n                mut_ends.append(curr_t + count)\n                mut_strands.append(t_strand)\n                mut_refs.append(targets.alphabet.seq_from(ref_slice))\n                mut_alts.append(queries.alphabet.empty_seq())\n                curr_t += count\n\n            elif op == CigarOp.M: # Match or Mismatch (Ambiguous)\n                # We need to compare sequences to find SNPs\n                # This is expensive in Python loop. \n                # Ideally we use extended CIGAR (=/X) from the start.\n                # If forced to use M, we would need a kernel to scan q/t arrays.\n                curr_q += count\n                curr_t += count\n\n            elif op == CigarOp.S: # Soft Clip\n                curr_q += count\n                # Soft clips consume query but not target, and are not mutations\n\n    # Construct Batch\n    if not mut_starts:\n        return MutationBatch.empty()\n\n    return MutationBatch(\n        IntervalBatch(\n            np.array(mut_starts, dtype=np.int32),\n            np.array(mut_ends, dtype=np.int32),\n            np.array(mut_strands, dtype=np.int32)\n        ),\n        targets.alphabet.batch_from(mut_refs),\n        queries.alphabet.batch_from(mut_alts)\n    )\n</code></pre>"},{"location":"reference/baclib/io/","title":"io","text":""},{"location":"reference/baclib/io/#baclib.io","title":"<code>baclib.io</code>","text":"<p>Module for parsing and managing bacterial sequence files and data.</p>"},{"location":"reference/baclib/io/#baclib.io.BaseReader","title":"<code>BaseReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sequence file readers.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class BaseReader(ABC):\n    \"\"\"Abstract base class for sequence file readers.\"\"\"\n    _CHUNK_SIZE = 65536\n    __slots__ = ('_handle', '_iterator')\n    def __init__(self, handle: BinaryIO, **kwargs):\n        \"\"\"\n        Initializes the reader.\n\n        Args:\n            handle: The open file handle to read from.\n            **kwargs: Additional arguments.\n        \"\"\"\n        self._handle = handle\n        self._iterator = None\n\n    @classmethod\n    @classmethod\n    @abstractmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if input bytes look like the format.\"\"\"\n        ...\n\n    @abstractmethod\n    def __iter__(self) -&gt; Generator:\n        \"\"\"Iterates over records.\"\"\"\n        ...\n\n    def __enter__(self): return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb): self.close()\n\n    def __next__(self):\n        if self._iterator is None:\n            self._iterator = self.__iter__()\n        return next(self._iterator)\n\n    def close(self):\n        \"\"\"Closes the reader.\"\"\"\n        pass\n\n    def read_chunks(self, chunk_size: int = None) -&gt; Generator[bytes, None, None]:\n        \"\"\"\n        Yields chunks of data from the file handle.\n        Uses a background thread for prefetching.\n        \"\"\"\n        if chunk_size is None: chunk_size = self._CHUNK_SIZE\n        return ThreadedChunkReader(self._handle, chunk_size)\n\n    def batches(self, size: int = 1024) -&gt; Generator[Batch, None, None]:\n        \"\"\"Yields records in batches for efficiency.\n\n        Args:\n            size: Number of records per batch.\n\n        Yields:\n            ``Batch`` objects.\n        \"\"\"\n        batch_records = []\n        for record in self:\n            batch_records.append(record)\n            if len(batch_records) &gt;= size:\n                yield self._make_batch(batch_records)\n                batch_records = []\n        if batch_records:\n            yield self._make_batch(batch_records)\n\n    def _make_batch(self, records: list) -&gt; Batch:\n        return RecordBatch(records)\n\n    def _build_seq_batch(self, seq_bytes_list: list[bytes], alphabet: Alphabet) -&gt; SeqBatch:\n        \"\"\"Helper to efficiently build a SeqBatch from a list of sequence bytes.\n\n        Args:\n            seq_bytes_list: List of bytes sequences.\n            alphabet: The alphabet to use.\n\n        Returns:\n            A new ``SeqBatch``.\n        \"\"\"\n        if not seq_bytes_list:\n             return alphabet.new_batch(np.empty(0, dtype=np.uint8), np.empty(0, dtype=np.int32), np.empty(0, dtype=np.int32))\n\n        # Optimization: Bulk encode if no deletions are required\n        # This avoids overhead of N translate calls and N numpy array allocations\n        if len(alphabet._delete_bytes) == 0:\n            lengths = np.array([len(s) for s in seq_bytes_list], dtype=np.int32)\n            n = len(lengths)\n            starts = np.zeros(n, dtype=np.int32)\n            if n &gt; 1: np.cumsum(lengths[:-1], out=starts[1:])\n\n            encoded_data = alphabet.encode(b\"\".join(seq_bytes_list))\n            return alphabet.new_batch(encoded_data, starts, lengths)\n\n        # 1. Encode individually to ensure lengths match the encoded data\n        # (Alphabet.encode drops invalid chars like newlines, so raw length != encoded length)\n        encoded_list = [alphabet.encode(s) for s in seq_bytes_list]\n\n        # 2. Build Metadata\n        n = len(encoded_list)\n        lengths = np.array([len(s) for s in encoded_list], dtype=np.int32)\n        starts = np.zeros(n, dtype=np.int32)\n        if n &gt; 1:\n            np.cumsum(lengths[:-1], out=starts[1:])\n\n        encoded_data = np.concatenate(encoded_list) if n &gt; 0 else np.empty(0, dtype=np.uint8)\n        return alphabet.new_batch(encoded_data, starts, lengths)\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseReader.__init__","title":"<code>__init__(handle, **kwargs)</code>","text":"<p>Initializes the reader.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>BinaryIO</code> <p>The open file handle to read from.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __init__(self, handle: BinaryIO, **kwargs):\n    \"\"\"\n    Initializes the reader.\n\n    Args:\n        handle: The open file handle to read from.\n        **kwargs: Additional arguments.\n    \"\"\"\n    self._handle = handle\n    self._iterator = None\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseReader.__iter__","title":"<code>__iter__()</code>  <code>abstractmethod</code>","text":"<p>Iterates over records.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>@abstractmethod\ndef __iter__(self) -&gt; Generator:\n    \"\"\"Iterates over records.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseReader.batches","title":"<code>batches(size=1024)</code>","text":"<p>Yields records in batches for efficiency.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Number of records per batch.</p> <code>1024</code> <p>Yields:</p> Type Description <code>Batch</code> <p><code>Batch</code> objects.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def batches(self, size: int = 1024) -&gt; Generator[Batch, None, None]:\n    \"\"\"Yields records in batches for efficiency.\n\n    Args:\n        size: Number of records per batch.\n\n    Yields:\n        ``Batch`` objects.\n    \"\"\"\n    batch_records = []\n    for record in self:\n        batch_records.append(record)\n        if len(batch_records) &gt;= size:\n            yield self._make_batch(batch_records)\n            batch_records = []\n    if batch_records:\n        yield self._make_batch(batch_records)\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseReader.close","title":"<code>close()</code>","text":"<p>Closes the reader.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def close(self):\n    \"\"\"Closes the reader.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseReader.read_chunks","title":"<code>read_chunks(chunk_size=None)</code>","text":"<p>Yields chunks of data from the file handle. Uses a background thread for prefetching.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def read_chunks(self, chunk_size: int = None) -&gt; Generator[bytes, None, None]:\n    \"\"\"\n    Yields chunks of data from the file handle.\n    Uses a background thread for prefetching.\n    \"\"\"\n    if chunk_size is None: chunk_size = self._CHUNK_SIZE\n    return ThreadedChunkReader(self._handle, chunk_size)\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseReader.sniff","title":"<code>sniff(s)</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Checks if input bytes look like the format.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>@classmethod\n@classmethod\n@abstractmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if input bytes look like the format.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter","title":"<code>BaseWriter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for sequence file writers.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with FastaWriter(\"output.fasta\") as w:\n...     w.write(record1, record2)\n</code></pre> Source code in <code>baclib/io/__init__.py</code> <pre><code>class BaseWriter(ABC):\n    \"\"\"\n    Abstract base class for sequence file writers.\n\n    Examples:\n        &gt;&gt;&gt; with FastaWriter(\"output.fasta\") as w:\n        ...     w.write(record1, record2)\n    \"\"\"\n    __slots__ = ('_opener', '_handle', '_threaded', '_writer_wrapper', '_real_handle')\n    def __init__(self, file: Union[str, Path, BinaryIO], mode: Union[str, OpenMode] = OpenMode.WRITE,\n                 compression: Union[str, CompressionFormat] = CompressionFormat.INFER,\n                 threaded: bool = True, **kwargs):\n        \"\"\"\n        Initializes the writer.\n        \"\"\"\n        self._opener = Xopen(file, mode=mode, **kwargs)\n        self._handle = None\n        self._threaded = threaded\n        self._writer_wrapper = None\n        self._real_handle = None\n\n    def __enter__(self):\n        \"\"\"Opens the file and writes the header.\"\"\"\n        self._handle = self._opener.__enter__()\n        if self._threaded:\n            self._writer_wrapper = ThreadedChunkWriter(self._handle)\n            self._real_handle = self._handle\n            self._handle = self._writer_wrapper\n        self.write_header()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Closes the file.\"\"\"\n        if self._writer_wrapper:\n            self._writer_wrapper.close()\n            self._handle = self._real_handle\n        self._opener.__exit__(exc_type, exc_val, exc_tb)\n\n    def write(self, *items: Union[Record, Feature, Batch, list, tuple]):\n        \"\"\"\n        Writes multiple records or features.\n        Automatically unpacks Batches and lists.\n\n        Args:\n            *items: Variable number of Record, Feature, Batch, or list objects.\n        \"\"\"\n        for item in items:\n            if isinstance(item, Batch):\n                self.write_batch(item)\n            elif isinstance(item, (list, tuple)):\n                for sub_item in item: self.write_one(sub_item)\n            else:\n                self.write_one(item)\n\n    def write_batch(self, batch: Batch):\n        \"\"\"Writes a batch of items. Subclasses can override for optimization.\"\"\"\n        for item in batch:\n            self.write_one(item)\n\n    @abstractmethod\n    def write_one(self, item: Union[Record, Feature]):\n        \"\"\"\n        Writes a single item.\n\n        Args:\n            item: Record or Feature to write.\n        \"\"\"\n        pass\n\n    def write_header(self):\n        \"\"\"Writes the file header if applicable.\"\"\"\n        pass\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter.__enter__","title":"<code>__enter__()</code>","text":"<p>Opens the file and writes the header.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __enter__(self):\n    \"\"\"Opens the file and writes the header.\"\"\"\n    self._handle = self._opener.__enter__()\n    if self._threaded:\n        self._writer_wrapper = ThreadedChunkWriter(self._handle)\n        self._real_handle = self._handle\n        self._handle = self._writer_wrapper\n    self.write_header()\n    return self\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Closes the file.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Closes the file.\"\"\"\n    if self._writer_wrapper:\n        self._writer_wrapper.close()\n        self._handle = self._real_handle\n    self._opener.__exit__(exc_type, exc_val, exc_tb)\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter.__init__","title":"<code>__init__(file, mode=OpenMode.WRITE, compression=CompressionFormat.INFER, threaded=True, **kwargs)</code>","text":"<p>Initializes the writer.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __init__(self, file: Union[str, Path, BinaryIO], mode: Union[str, OpenMode] = OpenMode.WRITE,\n             compression: Union[str, CompressionFormat] = CompressionFormat.INFER,\n             threaded: bool = True, **kwargs):\n    \"\"\"\n    Initializes the writer.\n    \"\"\"\n    self._opener = Xopen(file, mode=mode, **kwargs)\n    self._handle = None\n    self._threaded = threaded\n    self._writer_wrapper = None\n    self._real_handle = None\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter.write","title":"<code>write(*items)</code>","text":"<p>Writes multiple records or features. Automatically unpacks Batches and lists.</p> <p>Parameters:</p> Name Type Description Default <code>*items</code> <code>Union[Record, Feature, Batch, list, tuple]</code> <p>Variable number of Record, Feature, Batch, or list objects.</p> <code>()</code> Source code in <code>baclib/io/__init__.py</code> <pre><code>def write(self, *items: Union[Record, Feature, Batch, list, tuple]):\n    \"\"\"\n    Writes multiple records or features.\n    Automatically unpacks Batches and lists.\n\n    Args:\n        *items: Variable number of Record, Feature, Batch, or list objects.\n    \"\"\"\n    for item in items:\n        if isinstance(item, Batch):\n            self.write_batch(item)\n        elif isinstance(item, (list, tuple)):\n            for sub_item in item: self.write_one(sub_item)\n        else:\n            self.write_one(item)\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter.write_batch","title":"<code>write_batch(batch)</code>","text":"<p>Writes a batch of items. Subclasses can override for optimization.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def write_batch(self, batch: Batch):\n    \"\"\"Writes a batch of items. Subclasses can override for optimization.\"\"\"\n    for item in batch:\n        self.write_one(item)\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter.write_header","title":"<code>write_header()</code>","text":"<p>Writes the file header if applicable.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def write_header(self):\n    \"\"\"Writes the file header if applicable.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.BaseWriter.write_one","title":"<code>write_one(item)</code>  <code>abstractmethod</code>","text":"<p>Writes a single item.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[Record, Feature]</code> <p>Record or Feature to write.</p> required Source code in <code>baclib/io/__init__.py</code> <pre><code>@abstractmethod\ndef write_one(self, item: Union[Record, Feature]):\n    \"\"\"\n    Writes a single item.\n\n    Args:\n        item: Record or Feature to write.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.FormatSpec","title":"<code>FormatSpec</code>","text":"<p>Metadata for a supported file format.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class FormatSpec:\n    \"\"\"Metadata for a supported file format.\"\"\"\n    __slots__ = ('extensions', 'alphabets', 'reader', 'writer')\n    def __init__(self, extensions: list[str] = None, alphabets: dict[str, 'Alphabet'] = None):\n        self.extensions = extensions or []\n        self.alphabets = alphabets or {}\n        self.reader: Type[BaseReader] = None\n        self.writer: Type[BaseWriter] = None\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.ParserError","title":"<code>ParserError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Raised when a file parser encounters malformed or unexpected input.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class ParserError(Exception):\n    \"\"\"Raised when a file parser encounters malformed or unexpected input.\"\"\"\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.Qualifier","title":"<code>Qualifier</code>","text":"<p>Parses qualifier strings from various file formats.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class Qualifier:\n    \"\"\"\n    Parses qualifier strings from various file formats.\n    \"\"\"\n    _TYPE_MAP = {b'f': float, b'i': int}\n    _ABBREVIATIONS = {b'dp': b'depth', b'ln': b'length', b'kc': b'kmer count'}\n\n    @staticmethod\n    def parse_gff_attributes(items: bytes) -&gt; list[tuple]:\n        \"\"\"\n        Parses GFF3 attribute strings.\n\n        Args:\n            items: The attribute string (e.g., \"ID=gene1;Name=foo\").\n\n        Returns:\n            A list of Qualifier objects.\n        \"\"\"\n        if not items or not b'=' in items: return []\n        quals = []\n        for item in items.split(b';'):\n            if not item: continue\n            key, _, val = item.partition(b'=')\n            if not key: continue\n            val = unquote_to_bytes(val)\n            quals.append((key, val))\n        return quals\n\n    @classmethod\n    def parse_tags(cls, items: list[bytes]) -&gt; list[tuple]:\n        \"\"\"\n        Parses SAM/PAF style tags.\n\n        Args:\n            items: List of tag bytes (e.g., [\"NM:i:0\", \"AS:f:100\"]).\n\n        Returns:\n            A list of Qualifier objects.\n        \"\"\"\n        quals = []\n        for item in items:\n            if len(item) &lt; 5: continue\n            parts = item.split(b':', 2)\n            if len(parts) != 3: continue\n            tag, typ, val = parts\n            tag = cls._ABBREVIATIONS.get(tag.lower(), tag)\n            converter = cls._TYPE_MAP.get(typ, bytes)\n            try: quals.append((tag, converter(val)))\n            except ValueError: continue\n        return quals\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.Qualifier.parse_gff_attributes","title":"<code>parse_gff_attributes(items)</code>  <code>staticmethod</code>","text":"<p>Parses GFF3 attribute strings.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>bytes</code> <p>The attribute string (e.g., \"ID=gene1;Name=foo\").</p> required <p>Returns:</p> Type Description <code>list[tuple]</code> <p>A list of Qualifier objects.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>@staticmethod\ndef parse_gff_attributes(items: bytes) -&gt; list[tuple]:\n    \"\"\"\n    Parses GFF3 attribute strings.\n\n    Args:\n        items: The attribute string (e.g., \"ID=gene1;Name=foo\").\n\n    Returns:\n        A list of Qualifier objects.\n    \"\"\"\n    if not items or not b'=' in items: return []\n    quals = []\n    for item in items.split(b';'):\n        if not item: continue\n        key, _, val = item.partition(b'=')\n        if not key: continue\n        val = unquote_to_bytes(val)\n        quals.append((key, val))\n    return quals\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.Qualifier.parse_tags","title":"<code>parse_tags(items)</code>  <code>classmethod</code>","text":"<p>Parses SAM/PAF style tags.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[bytes]</code> <p>List of tag bytes (e.g., [\"NM:i:0\", \"AS:f:100\"]).</p> required <p>Returns:</p> Type Description <code>list[tuple]</code> <p>A list of Qualifier objects.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>@classmethod\ndef parse_tags(cls, items: list[bytes]) -&gt; list[tuple]:\n    \"\"\"\n    Parses SAM/PAF style tags.\n\n    Args:\n        items: List of tag bytes (e.g., [\"NM:i:0\", \"AS:f:100\"]).\n\n    Returns:\n        A list of Qualifier objects.\n    \"\"\"\n    quals = []\n    for item in items:\n        if len(item) &lt; 5: continue\n        parts = item.split(b':', 2)\n        if len(parts) != 3: continue\n        tag, typ, val = parts\n        tag = cls._ABBREVIATIONS.get(tag.lower(), tag)\n        converter = cls._TYPE_MAP.get(typ, bytes)\n        try: quals.append((tag, converter(val)))\n        except ValueError: continue\n    return quals\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile","title":"<code>SeqFile</code>","text":"<p>Main interface for reading sequence files.</p> <p>Automatically detects file format (e.g., FASTA, GenBank, GFF) and compression (e.g., gzip, bzip2), providing a single, unified way to iterate over sequence data.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class SeqFile:\n    \"\"\"\n    Main interface for reading sequence files.\n\n    Automatically detects file format (e.g., FASTA, GenBank, GFF) and\n    compression (e.g., gzip, bzip2), providing a single, unified way to\n    iterate over sequence data.\n    \"\"\"\n    __slots__ = ('_name', '_opener', '_handle', '_format', '_reader', '_reader_kwargs', '_iterator')\n    _REGISTRY = {}\n    def __init__(self, file: Union[str, Path, BinaryIO], fmt: Union[str, SeqFileFormat] = SeqFileFormat.INFER, **reader_kwargs):\n        \"\"\"\n        Initializes the SeqFile reader.\n        \"\"\"\n        self._opener = Xopen(file, mode='rb')\n        self._name = self._opener.name or None\n        self._handle = None\n        self._format = SeqFileFormat(fmt)\n        self._reader = None\n        self._reader_kwargs = reader_kwargs\n        self._iterator = None\n\n        if self._format == SeqFileFormat.INFER and self._name is not None:\n            filename, _, ext = self._name.rpartition('.')\n            ext = f'.{ext.lower()}'\n            for registered_format, spec in self._REGISTRY.items():\n                if ext in spec.extensions:\n                    self._name = filename\n                    self._format = registered_format\n                    if (alpha := spec.alphabets.get(ext)) and 'alphabet' not in self._reader_kwargs:\n                        self._reader_kwargs['alphabet'] = alpha\n                    break\n\n    @property\n    def format(self) -&gt; SeqFileFormat: return self._format\n    @property\n    def name(self) -&gt; str: return self._opener.name\n\n    @classmethod\n    def open(cls, file: Union[str, Path, BinaryIO], mode: Union[str, OpenMode] = OpenMode.READ,\n             fmt: Union[str, SeqFileFormat] = None, alphabet: Alphabet = None, **kwargs) -&gt; Union['SeqFile', BaseWriter]:\n        \"\"\"Opens a sequence file for reading or writing.\n\n        Args:\n            file: File path or object.\n            mode: Mode to open file ('r', 'w', 'a', etc.).\n            fmt: Format specifier (e.g. 'fasta'). If None, infers from extension.\n            alphabet: Alphabet to use for reading.\n            **kwargs: Additional arguments for the reader/writer.\n\n        Returns:\n            A ``SeqFile`` (reader) or ``BaseWriter``.\n        \"\"\"\n        if fmt is None:\n            fmt = kwargs.pop('format', None)\n\n        mode_str = str(mode)\n        if 'w' in mode_str or 'a' in mode_str or 'x' in mode_str:\n            # Writing\n            if fmt is None:\n                 # Try to infer from filename\n                 path = Path(str(file)) # naive\n                 if isinstance(file, (str, Path)):\n                      path = Path(file)\n                      name = path.name.lower()\n                      # Remove compression ext\n                      if name.endswith(('.gz', '.bz2', '.xz', '.zst')):\n                           name = path.stem.lower()\n\n                      for f, spec in cls._REGISTRY.items():\n                           if any(name.endswith(ext) for ext in spec.extensions):\n                               fmt = f\n                               break\n\n            if fmt is None:\n                raise ValueError(\"Format must be specified for writing if it cannot be inferred from extension.\")\n\n            spec = cls._REGISTRY.get(cls.Format(fmt))\n            if not spec or not spec.writer:\n                raise ValueError(f\"No writer available for format {fmt}\")\n\n            return spec.writer(file, mode=mode, **kwargs)\n\n        return cls(file, fmt=fmt, alphabet=alphabet, **kwargs)\n\n    def __iter__(self) -&gt; Generator[Union[Record, Feature, Batch], None, None]:\n        \"\"\"\n        Iterates over the content of the file.\n\n        Yields:\n            Parsed objects depending on the format.\n        \"\"\"\n        # Auto-open if not used as a context manager\n        close_on_exit = False\n        if self._handle is None:\n            self._handle = self._opener.__enter__()\n            close_on_exit = True\n\n        handle_to_use = self._handle\n\n        try:\n            if self._format is None:\n                # Optimization: Use native peek if available to avoid PeekableHandle overhead\n                if hasattr(self._handle, 'peek'):\n                    self._format = self._sniff_format(self._handle)\n                else:\n                    peekable_handle = PeekableHandle(self._handle)\n                    self._format = self._sniff_format(peekable_handle)\n                    handle_to_use = peekable_handle\n\n            if self._format not in self._REGISTRY:\n                raise SeqFileError(f\"Cannot parse file: format '{self._format}' is unknown or unsupported.\")\n\n            self._reader = self._REGISTRY[self._format].reader(handle_to_use, **self._reader_kwargs)\n            yield from self._reader\n        finally:\n            if close_on_exit:\n                self.close()\n\n    def __next__(self):\n        if self._iterator is None:\n            self._iterator = self.__iter__()\n        return next(self._iterator)\n\n    def batches(self, size: int = 1024) -&gt; Generator[Batch, None, None]:\n        \"\"\"\n        Iterates over the content of the file in batches.\n\n        Args:\n            size: Number of records per batch.\n\n        Yields:\n            Batch objects depending on the format.\n        \"\"\"\n        # Auto-open if not used as a context manager\n        close_on_exit = False\n        if self._handle is None:\n            self._handle = self._opener.__enter__()\n            close_on_exit = True\n\n        handle_to_use = self._handle\n\n        try:\n            if self._format is None:\n                # Optimization: Use native peek if available\n                if hasattr(self._handle, 'peek'):\n                    self._format = self._sniff_format(self._handle)\n                else:\n                    peekable_handle = PeekableHandle(self._handle)\n                    self._format = self._sniff_format(peekable_handle)\n                    handle_to_use = peekable_handle\n\n            if self._format not in self._REGISTRY:\n                raise SeqFileError(f\"Cannot parse file: format '{self._format}' is unknown or unsupported.\")\n\n            self._reader = self._REGISTRY[self._format].reader(handle_to_use, **self._reader_kwargs)\n            yield from self._reader.batches(size)\n        finally:\n            if close_on_exit:\n                self.close()\n\n    def _sniff_format(self, handle: Union[PeekableHandle, BinaryIO]) -&gt; BaseReader:\n        \"\"\"\n        Detects the file format by peeking at the content.\n\n        Args:\n            handle: The peekable file handle.\n\n        Returns:\n            The detected format string.\n\n        Raises:\n            ValueError: If format cannot be determined.\n        \"\"\"\n        peek_window = handle.peek(1024)[:1024]\n        for fmt, spec in self._REGISTRY.items():\n            if spec.reader and spec.reader.sniff(peek_window): return fmt\n        raise ValueError(\"Could not determine format from stream content.\")\n\n    def close(self):\n        \"\"\"Closes the file.\"\"\"\n        self._opener.__exit__(None, None, None)\n        self._handle = None\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        self._handle = self._opener.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Context manager exit.\"\"\"\n        self.close()\n\n    @classmethod\n    def register(cls, fmt: SeqFileFormat, extensions: list[str] = None, alphabets: dict[str, 'Alphabet'] = None):\n        \"\"\"\n        Decorator to register a Reader or Writer for a specific format.\n        \"\"\"\n\n        def decorator(func: Callable) -&gt; Callable:\n            if (spec := cls._REGISTRY.get(fmt)) is None:\n                spec = FormatSpec()\n                cls._REGISTRY[fmt] = spec\n\n            if extensions:\n                [spec.extensions.append(ext) for ext in extensions if ext not in spec.extensions]\n\n            if alphabets: spec.alphabets.update(alphabets)\n\n            if issubclass(func, BaseReader):\n                spec.reader = func\n            elif issubclass(func, BaseWriter):\n                spec.writer = func\n            return func\n\n        return decorator\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    self._handle = self._opener.__enter__()\n    return self\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Context manager exit.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    self.close()\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.__init__","title":"<code>__init__(file, fmt=SeqFileFormat.INFER, **reader_kwargs)</code>","text":"<p>Initializes the SeqFile reader.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __init__(self, file: Union[str, Path, BinaryIO], fmt: Union[str, SeqFileFormat] = SeqFileFormat.INFER, **reader_kwargs):\n    \"\"\"\n    Initializes the SeqFile reader.\n    \"\"\"\n    self._opener = Xopen(file, mode='rb')\n    self._name = self._opener.name or None\n    self._handle = None\n    self._format = SeqFileFormat(fmt)\n    self._reader = None\n    self._reader_kwargs = reader_kwargs\n    self._iterator = None\n\n    if self._format == SeqFileFormat.INFER and self._name is not None:\n        filename, _, ext = self._name.rpartition('.')\n        ext = f'.{ext.lower()}'\n        for registered_format, spec in self._REGISTRY.items():\n            if ext in spec.extensions:\n                self._name = filename\n                self._format = registered_format\n                if (alpha := spec.alphabets.get(ext)) and 'alphabet' not in self._reader_kwargs:\n                    self._reader_kwargs['alphabet'] = alpha\n                break\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over the content of the file.</p> <p>Yields:</p> Type Description <code>Union[Record, Feature, Batch]</code> <p>Parsed objects depending on the format.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def __iter__(self) -&gt; Generator[Union[Record, Feature, Batch], None, None]:\n    \"\"\"\n    Iterates over the content of the file.\n\n    Yields:\n        Parsed objects depending on the format.\n    \"\"\"\n    # Auto-open if not used as a context manager\n    close_on_exit = False\n    if self._handle is None:\n        self._handle = self._opener.__enter__()\n        close_on_exit = True\n\n    handle_to_use = self._handle\n\n    try:\n        if self._format is None:\n            # Optimization: Use native peek if available to avoid PeekableHandle overhead\n            if hasattr(self._handle, 'peek'):\n                self._format = self._sniff_format(self._handle)\n            else:\n                peekable_handle = PeekableHandle(self._handle)\n                self._format = self._sniff_format(peekable_handle)\n                handle_to_use = peekable_handle\n\n        if self._format not in self._REGISTRY:\n            raise SeqFileError(f\"Cannot parse file: format '{self._format}' is unknown or unsupported.\")\n\n        self._reader = self._REGISTRY[self._format].reader(handle_to_use, **self._reader_kwargs)\n        yield from self._reader\n    finally:\n        if close_on_exit:\n            self.close()\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.batches","title":"<code>batches(size=1024)</code>","text":"<p>Iterates over the content of the file in batches.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Number of records per batch.</p> <code>1024</code> <p>Yields:</p> Type Description <code>Batch</code> <p>Batch objects depending on the format.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def batches(self, size: int = 1024) -&gt; Generator[Batch, None, None]:\n    \"\"\"\n    Iterates over the content of the file in batches.\n\n    Args:\n        size: Number of records per batch.\n\n    Yields:\n        Batch objects depending on the format.\n    \"\"\"\n    # Auto-open if not used as a context manager\n    close_on_exit = False\n    if self._handle is None:\n        self._handle = self._opener.__enter__()\n        close_on_exit = True\n\n    handle_to_use = self._handle\n\n    try:\n        if self._format is None:\n            # Optimization: Use native peek if available\n            if hasattr(self._handle, 'peek'):\n                self._format = self._sniff_format(self._handle)\n            else:\n                peekable_handle = PeekableHandle(self._handle)\n                self._format = self._sniff_format(peekable_handle)\n                handle_to_use = peekable_handle\n\n        if self._format not in self._REGISTRY:\n            raise SeqFileError(f\"Cannot parse file: format '{self._format}' is unknown or unsupported.\")\n\n        self._reader = self._REGISTRY[self._format].reader(handle_to_use, **self._reader_kwargs)\n        yield from self._reader.batches(size)\n    finally:\n        if close_on_exit:\n            self.close()\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.close","title":"<code>close()</code>","text":"<p>Closes the file.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>def close(self):\n    \"\"\"Closes the file.\"\"\"\n    self._opener.__exit__(None, None, None)\n    self._handle = None\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.open","title":"<code>open(file, mode=OpenMode.READ, fmt=None, alphabet=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Opens a sequence file for reading or writing.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path, BinaryIO]</code> <p>File path or object.</p> required <code>mode</code> <code>Union[str, OpenMode]</code> <p>Mode to open file ('r', 'w', 'a', etc.).</p> <code>READ</code> <code>fmt</code> <code>Union[str, SeqFileFormat]</code> <p>Format specifier (e.g. 'fasta'). If None, infers from extension.</p> <code>None</code> <code>alphabet</code> <code>Alphabet</code> <p>Alphabet to use for reading.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments for the reader/writer.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[SeqFile, BaseWriter]</code> <p>A <code>SeqFile</code> (reader) or <code>BaseWriter</code>.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>@classmethod\ndef open(cls, file: Union[str, Path, BinaryIO], mode: Union[str, OpenMode] = OpenMode.READ,\n         fmt: Union[str, SeqFileFormat] = None, alphabet: Alphabet = None, **kwargs) -&gt; Union['SeqFile', BaseWriter]:\n    \"\"\"Opens a sequence file for reading or writing.\n\n    Args:\n        file: File path or object.\n        mode: Mode to open file ('r', 'w', 'a', etc.).\n        fmt: Format specifier (e.g. 'fasta'). If None, infers from extension.\n        alphabet: Alphabet to use for reading.\n        **kwargs: Additional arguments for the reader/writer.\n\n    Returns:\n        A ``SeqFile`` (reader) or ``BaseWriter``.\n    \"\"\"\n    if fmt is None:\n        fmt = kwargs.pop('format', None)\n\n    mode_str = str(mode)\n    if 'w' in mode_str or 'a' in mode_str or 'x' in mode_str:\n        # Writing\n        if fmt is None:\n             # Try to infer from filename\n             path = Path(str(file)) # naive\n             if isinstance(file, (str, Path)):\n                  path = Path(file)\n                  name = path.name.lower()\n                  # Remove compression ext\n                  if name.endswith(('.gz', '.bz2', '.xz', '.zst')):\n                       name = path.stem.lower()\n\n                  for f, spec in cls._REGISTRY.items():\n                       if any(name.endswith(ext) for ext in spec.extensions):\n                           fmt = f\n                           break\n\n        if fmt is None:\n            raise ValueError(\"Format must be specified for writing if it cannot be inferred from extension.\")\n\n        spec = cls._REGISTRY.get(cls.Format(fmt))\n        if not spec or not spec.writer:\n            raise ValueError(f\"No writer available for format {fmt}\")\n\n        return spec.writer(file, mode=mode, **kwargs)\n\n    return cls(file, fmt=fmt, alphabet=alphabet, **kwargs)\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFile.register","title":"<code>register(fmt, extensions=None, alphabets=None)</code>  <code>classmethod</code>","text":"<p>Decorator to register a Reader or Writer for a specific format.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>@classmethod\ndef register(cls, fmt: SeqFileFormat, extensions: list[str] = None, alphabets: dict[str, 'Alphabet'] = None):\n    \"\"\"\n    Decorator to register a Reader or Writer for a specific format.\n    \"\"\"\n\n    def decorator(func: Callable) -&gt; Callable:\n        if (spec := cls._REGISTRY.get(fmt)) is None:\n            spec = FormatSpec()\n            cls._REGISTRY[fmt] = spec\n\n        if extensions:\n            [spec.extensions.append(ext) for ext in extensions if ext not in spec.extensions]\n\n        if alphabets: spec.alphabets.update(alphabets)\n\n        if issubclass(func, BaseReader):\n            spec.reader = func\n        elif issubclass(func, BaseWriter):\n            spec.writer = func\n        return func\n\n    return decorator\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFileError","title":"<code>SeqFileError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for errors in sequence file processing.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class SeqFileError(Exception):\n    \"\"\"Exception raised for errors in sequence file processing.\"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqFileFormat","title":"<code>SeqFileFormat</code>","text":"<p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Supported sequence file formats.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class SeqFileFormat(str, Enum):\n    \"\"\"Supported sequence file formats.\"\"\"\n    INFER = 'infer'\n    FASTA = 'fasta'\n    FASTQ = 'fastq'\n    GENBANK = 'genbank'\n    GFA = 'gfa'\n    GFF = 'gff'\n    BED = 'bed'\n    PAF = 'paf'\n    MEME = 'meme'\n    TRANSFAC = 'transfac'\n    VCF = 'vcf'\n    EMBL = 'embl'\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.SeqIOError","title":"<code>SeqIOError</code>","text":"<p>               Bases: <code>IOError</code></p> <p>Base class for sequence I/O errors.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class SeqIOError(IOError):\n    \"\"\"Base class for sequence I/O errors.\"\"\"\n</code></pre>"},{"location":"reference/baclib/io/#baclib.io.TruncatedFileError","title":"<code>TruncatedFileError</code>","text":"<p>               Bases: <code>SeqIOError</code></p> <p>Raised when a file appears to be truncated.</p> Source code in <code>baclib/io/__init__.py</code> <pre><code>class TruncatedFileError(SeqIOError):\n    \"\"\"Raised when a file appears to be truncated.\"\"\"\n</code></pre>"},{"location":"reference/baclib/io/genbank/","title":"genbank","text":""},{"location":"reference/baclib/io/genbank/#baclib.io.genbank","title":"<code>baclib.io.genbank</code>","text":"<p>Readers and writers for GenBank and EMBL flat-file formats (INSDC feature table).</p>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.EmblReader","title":"<code>EmblReader</code>","text":"<p>               Bases: <code>InsdcReader</code></p> <p>EMBL Format Reader.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>@SeqFile.register(SeqFileFormat.EMBL, extensions=['.embl'])\nclass EmblReader(InsdcReader):\n    \"\"\"\n    EMBL Format Reader.\n    \"\"\"\n    _HEADER_START = b'ID'\n    _FEATURE_START = b'FH'\n    _FEATURE_TABLE_PREFIX = b'FT' # EMBL lines start with 'FT   '\n\n    def _split_meta_seq(self, data: bytes) -&gt; Tuple[bytes, bytes]:\n        # EMBL Sequence starts after 'SQ' line\n        # SQ   Sequence 1859 BP; 609 A; 314 C; 355 G; 581 T; 0 other;\n        sq_idx = data.find(b'\\nSQ')\n        if sq_idx != -1:\n             line_end = data.find(b'\\n', sq_idx + 1)\n             if line_end != -1:\n                 return data[:sq_idx], data[line_end+1:]\n        return data, None\n\n    def _split_features(self, meta_data: bytes) -&gt; Tuple[List[bytes], List[bytes]]:\n        # EMBL Features start with FH key (Feature Header) and contain FT lines\n        # Stop at SQ or XX?\n        # We can just iterate lines and filter.\n\n        # But to fit _split_features signature efficiently:\n        # Find start of FT lines?\n        # Usually internal to metadata.\n        # Let's split all lines.\n        lines = meta_data.splitlines()\n        header = []\n        features = []\n\n        in_features = False\n        for line in lines:\n            if line.startswith(b'FH'):\n                in_features = True\n                continue\n            if line.startswith(b'FT'):\n                features.append(line)\n            else:\n                 header.append(line)\n\n        return header, features\n\n    def _parse_header(self, lines: List[bytes]) -&gt; Tuple[bytes, bytes, List[Tuple[bytes, bytes]]]:\n        \"\"\"Parses ID, DE, and OS lines from EMBL header.\"\"\"\n        name = b'unknown'\n        description = b''\n        qualifiers = []\n        for line in lines:\n             if line.startswith(b'ID'):\n                 parts = line.split()\n                 if len(parts) &gt; 1: name = parts[1].strip(b';')\n             elif line.startswith(b'DE'):\n                 if not description:\n                     description = line[5:].strip()\n                 else:\n                     description += b\" \" + line[5:].strip()\n             elif line.startswith(b'OS'):\n                 val = line[5:].strip()\n                 qualifiers.append((b'organism', val))\n        return name, description, qualifiers\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool: \n        \"\"\"Checks if the input bytes look like an EMBL file.\"\"\"\n        return b'ID' in s[:100] and b'; SV' in s[:200]\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.EmblReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if the input bytes look like an EMBL file.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool: \n    \"\"\"Checks if the input bytes look like an EMBL file.\"\"\"\n    return b'ID' in s[:100] and b'; SV' in s[:200]\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.EmblWriter","title":"<code>EmblWriter</code>","text":"<p>               Bases: <code>InsdcWriter</code></p> <p>Writer for EMBL flat-file format.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>@SeqFile.register(SeqFileFormat.EMBL, extensions=['.embl'])\nclass EmblWriter(InsdcWriter):\n    \"\"\"Writer for EMBL flat-file format.\"\"\"\n    _FT_PREFIX = b'FT'\n\n    def write_one(self, record: Record):\n        # ID   Name; SV 1; linear; DNA; STD; UNK; Length BP.\n        name = record.id or b'Unknown'\n        length = len(record.seq)\n\n        id_line = f\"ID   {name.decode('ascii')}; SV 1; linear; DNA; STD; UNK; {length} BP.\".encode('ascii')\n        self.write_line(id_line)\n\n        # XX\n        self.write_line(b\"XX\")\n\n        # DE\n        desc = record.description or b'.'\n        self.write_line(b\"DE   \" + desc)\n        self.write_line(b\"XX\")\n\n        # Features\n        self._write_feature_table(record)\n\n        # Sequence\n        # SQ   Sequence 1859 BP; 609 A; 314 C; 355 G; 581 T; 0 other;\n        # Stats?\n        self.write_line(f\"SQ   Sequence {length} BP;\".encode('ascii'))\n\n        # Sequence data\n        #      gatc gatc       10\n        seq = record.seq\n        seq_bytes = bytes(seq) if seq is not None and len(seq) &gt; 0 else b\"\"\n        seq_len = len(seq_bytes)\n\n        for i in range(0, seq_len, 60):\n             chunk = seq_bytes[i:i+60]\n             # Blocks of 10\n             blocks = [chunk[j:j+10] for j in range(0, len(chunk), 10)]\n             seq_content = b' '.join(blocks)\n\n             # Padding to 72 chars? Then count.\n             # \"     \" (5 spaces) + content + padding + count\n\n             # EMBL format: 5 spaces, seq data, char count at col 72&gt;?\n             #      cctttatcgg aatgaaaaaa ttatttattt attagaggaa agaacataca atggacaatg         60\n\n             line_prefix = b\"     \" + seq_content\n             # Pad to somewhere?\n             # Count at end.\n             count_str = f\"{min(i+60, seq_len)}\".encode('ascii')\n\n             # Pad line_prefix to 72 chars?\n             padding = b\" \" * max(1, 72 - len(line_prefix))\n\n             self.write_line(line_prefix + padding + count_str)\n\n        self.write_line(b\"//\")\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.GenbankReader","title":"<code>GenbankReader</code>","text":"<p>               Bases: <code>InsdcReader</code></p> <p>GenBank Format Reader.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>@SeqFile.register(SeqFileFormat.GENBANK, extensions=['.gb', '.gbk', '.genbank'])\nclass GenbankReader(InsdcReader):\n    \"\"\"\n    GenBank Format Reader.\n    \"\"\"\n    _HEADER_START = b'LOCUS'\n    _FEATURE_START = b'FEATURES'\n    _FEATURE_TABLE_PREFIX = b''\n\n    def _split_meta_seq(self, data: bytes) -&gt; Tuple[bytes, bytes]:\n        origin_idx = data.find(b'\\nORIGIN')\n        if origin_idx != -1:\n            line_end = data.find(b'\\n', origin_idx + 1)\n            if line_end != -1:\n                return data[:origin_idx], data[line_end+1:]\n\n        # If no ORIGIN, maybe it's all meta or purely meta (contig?)\n        return data, None\n\n    def _split_features(self, meta_data: bytes) -&gt; Tuple[List[bytes], List[bytes]]:\n        features_idx = meta_data.find(b'\\nFEATURES')\n\n        if features_idx != -1:\n            header_bytes = meta_data[:features_idx]\n            feature_bytes = meta_data[features_idx+1:]\n            return header_bytes.splitlines(), feature_bytes.splitlines()\n        elif meta_data.strip().startswith(b'FEATURES'):\n             return [], meta_data.splitlines()\n\n        return meta_data.splitlines(), []\n\n    def _parse_header(self, lines: List[bytes]) -&gt; Tuple[bytes, bytes, List[Tuple[bytes, bytes]]]:\n        \"\"\"Parses the LOCUS, DEFINITION, and SOURCE lines.\"\"\"\n        name = b'unknown'\n        description = b''\n        qualifiers = []\n        for line in lines:\n            if line.startswith(b'LOCUS'):\n                parts = line.split()\n                if len(parts) &gt; 1: name = parts[1]\n            elif line.startswith(b'DEFINITION'):\n                description = line[12:].strip()\n            elif line.startswith(b'SOURCE'):\n                qualifiers.append((b'organism', line[12:].strip()))\n        return name, description, qualifiers\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if the input bytes look like a GenBank file.\"\"\"\n        return b'LOCUS' in s[:100]\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.GenbankReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if the input bytes look like a GenBank file.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if the input bytes look like a GenBank file.\"\"\"\n    return b'LOCUS' in s[:100]\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.GenbankWriter","title":"<code>GenbankWriter</code>","text":"<p>               Bases: <code>InsdcWriter</code></p> <p>Writer for GenBank flat-file format.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>@SeqFile.register(SeqFileFormat.GENBANK, extensions=['.gb', '.gbk', '.genbank'])\nclass GenbankWriter(InsdcWriter):\n    \"\"\"Writer for GenBank flat-file format.\"\"\"\n    def write_one(self, record: Record):\n        # Header\n        # LOCUS       Action_17                671 bp    DNA     linear   PLN 18-FEB-2026\n        # Name: 12 chars max?\n        name = record.id or b'Unknown'\n        length = len(record.seq)\n\n        # Fixed padding is tricky without f-string for bytes or strict formatting\n        # LOCUS (col 0-4)\n        # Name (col 12-?)\n        # Length\n        # MolType\n        # Div\n        # Date\n\n        # Simplified LOCUS line\n        locus_line = f\"LOCUS       {name.decode('ascii'):&lt;20} {length:&gt;10} bp    DNA     linear   UNK {self._get_date()}\".encode('ascii')\n        self.write_line(locus_line)\n\n        # DEFINITION\n        desc = record.description or b'.'\n        self._write_wrapped_line(b\"DEFINITION  \", desc)\n\n        # ACCESSION / VERSION\n        self.write_line(b\"ACCESSION   \" + name)\n        self.write_line(b\"VERSION     \" + name) # Placeholder\n\n        # KEYWORDS\n        self.write_line(b\"KEYWORDS    .\")\n\n        # SOURCE\n        # Check source feature?\n        source_feat = next((f for f in record.features if f.key == FeatureKey.SOURCE), None)\n        organism = b\"Unknown\"\n        if source_feat and b'organism' in source_feat.qualifiers:\n             organism = source_feat.qualifiers[b'organism']\n\n        self._write_wrapped_line(b\"SOURCE      \", organism)\n        self.write_line(b\"  ORGANISM  \" + organism)\n        self.write_line(b\"            .\")\n\n        # Features\n        self._write_feature_table(record)\n\n        # Sequence\n        self.write_line(b\"ORIGIN\")\n        self._write_sequence(record.seq)\n\n        self.write_line(b\"//\")\n\n    def _get_date(self) -&gt; str:\n        return \"01-JAN-1900\" # Placeholder, strict date logic needs imports\n\n    def _write_wrapped_line(self, prefix: bytes, content: bytes):\n        # Wrap content with prefix\n        # Indent subsequent lines equal to prefix len\n        indent_len = len(prefix)\n        indent = b' ' * 12 # Standard indent for DEFINITION/SOURCE bodies is 12\n\n        # Actually logic:\n        # Header (12 chars) + Content\n        # definition is 12 chars \"DEFINITION  \"\n\n        # Reuse wrap logic?\n        full = prefix + content\n        if len(full) &lt;= 80:\n             self.write_line(full)\n        else:\n             # Split\n             # TODO: Proper wrapping\n             self.write_line(full[:80])\n             self.write_line(indent + full[80:]) # Simple split\n\n    def _write_sequence(self, seq: Seq):\n        # 60 bp per line, blocks of 10\n        #        1 ctgctggcgc catcttgctc tggctgtcgg cgatccggcg gccaatgtgc aggcgctggt\n\n        # Convert to bytes\n        seq_bytes = bytes(seq)\n        length = len(seq_bytes)\n\n        for i in range(0, length, 60):\n            chunk = seq_bytes[i:i+60]\n\n            # Format: 9 chars numbering (right align) + space + seq chunks\n            # Numbering is 1-based start of line\n            line_start = f\"{i+1:&gt;9} \".encode('ascii')\n\n            # Split into blocks of 10\n            blocks = [chunk[j:j+10] for j in range(0, len(chunk), 10)]\n            seq_str = b' '.join(blocks)\n\n            self.write_line(line_start + seq_str)\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.InsdcReader","title":"<code>InsdcReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Base reader for INSDC formats (GenBank, EMBL, DDBJ). Implements strict column-based feature table parsing.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>class InsdcReader(BaseReader):\n    \"\"\"\n    Base reader for INSDC formats (GenBank, EMBL, DDBJ).\n    Implements strict column-based feature table parsing.\n    \"\"\"\n    __slots__ = ('_alphabet', '_base')\n    _INTERVAL_REGEX = regex(rb'(?P&lt;partial_start&gt;&lt;)?(?P&lt;start&gt;[0-9]+)\\.\\.(?P&lt;partial_end&gt;&gt;)?(?P&lt;end&gt;[0-9]+)')\n\n    # Subclasses must define these\n    _HEADER_START = b''  # e.g., b'LOCUS' or b'ID'\n    _FEATURE_START = b'' # e.g., b'FEATURES' or b'FH'\n    _FEATURE_TABLE_PREFIX = b'' # e.g. b'FT ' for EMBL, empty for GenBank\n\n    def __init__(self, handle: BinaryIO, alphabet: Alphabet = None):\n        \"\"\"Initializes the INSDC reader.\n\n        Args:\n            handle: Binary file handle.\n            alphabet: Alphabet to use detection if None).\n        \"\"\"\n        super().__init__(handle)\n        self._alphabet = alphabet\n        self._base = 1\n\n    def __iter__(self) -&gt; Generator[Record, None, None]:\n        \"\"\"Iterates over records.\n\n        Yields:\n            ``Record`` objects.\n        \"\"\"\n        for chunk in self._read_chunks():\n            try:\n                yield self._parse_record_chunk(chunk)\n            except Exception:\n                # Fallback for truncated records at EOF or malformed chunks\n                pass\n\n    def batches(self, size: int = 1024) -&gt; Generator[RecordBatch, None, None]:\n        \"\"\"Reads records in batches.\n\n        Args:\n            size: Maximum records per batch.\n\n        Yields:\n            ``RecordBatch`` objects.\n        \"\"\"\n        chunks = []\n        for chunk in self._read_chunks():\n            chunks.append(chunk)\n            if len(chunks) &gt;= size:\n                yield self._make_batch(chunks)\n                chunks = []\n        if chunks:\n            yield self._make_batch(chunks)\n\n    def _read_chunks(self):\n        # Optimization: Read large binary chunks (Seqtk style)\n\n        buf = bytearray()\n        search_pos = 0\n        for chunk in self.read_chunks(self._CHUNK_SIZE):\n            if not chunk:\n                # EOF\n                if buf and (self._HEADER_START in buf[:100] or b'//' in buf):\n                    if len(buf.strip()) &gt; 2: yield bytes(buf)\n                break\n\n            buf.extend(chunk)\n            pos = 0\n\n            # Find records separated by //\n            while True:\n                # Find end of record marker //\n                # We need to find \\n// which is the standard end\n                # but handle if it's at start of buffer separately\n\n                scan_start = max(pos, search_pos)\n                end_pos = -1\n\n                if scan_start == 0 and buf.startswith(b'//'): end_pos = 0\n                else:\n                    found = buf.find(b'\\n//', scan_start)\n                    if found != -1: end_pos = found + 1\n\n                if end_pos == -1:\n                    # No complete record yet\n                    search_pos = max(pos, len(buf) - 4) \n                    if pos &gt; 0: \n                        del buf[:pos]\n                        search_pos -= pos\n                        search_pos = max(0, search_pos)\n                    break \n\n                # Found delimiter at end_pos\n                next_line = buf.find(b'\\n', end_pos)\n                if next_line == -1:\n                    # Incomplete delimiter line, wait for more data\n                    if pos &gt; 0: \n                        del buf[:pos]\n                        search_pos = max(0, search_pos - pos)\n                    break\n\n                record_bytes = bytes(buf[pos:end_pos])\n                if record_bytes.strip(): yield record_bytes\n\n                pos = next_line + 1\n                search_pos = pos\n\n    @staticmethod\n    def _extract_seq_data(data: bytes) -&gt; Tuple[bytes, bytes]:\n        \"\"\"Returns (metadata, sequence_data)\"\"\"\n        # Common logic: Sequence usually starts after ORIGIN (GenBank) or SQ (EMBL)\n        # But implementations might differ slightly.\n        # We'll use a heuristic: look for ORIGIN line or Sequence Header\n        pass\n\n    def _parse_record_chunk(self, data: bytes) -&gt; Record:\n        meta_data, seq_data = self._split_meta_seq(data)\n\n        # Encode Sequence\n        if self._alphabet is None and seq_data:\n            self._alphabet = Alphabet.detect(seq_data)\n\n        if self._alphabet:\n            seq = self._alphabet.seq_from(seq_data) if seq_data else self._alphabet.empty_seq()\n        else:\n             if not seq_data:\n                 self._alphabet = Alphabet.DNA\n                 seq = self._alphabet.empty_seq()\n             else:\n                 seq = self._alphabet.seq_from(seq_data)\n\n        # Parse Metadata (Features)\n        header_lines, feature_lines = self._split_features(meta_data)\n\n        return self._finalize_record(header_lines, feature_lines, seq)\n\n    def _split_meta_seq(self, data: bytes) -&gt; Tuple[bytes, bytes]:\n        raise NotImplementedError\n\n    def _split_features(self, meta_data: bytes) -&gt; Tuple[List[bytes], List[bytes]]:\n        raise NotImplementedError\n\n    def _make_batch(self, chunks: List[bytes]) -&gt; RecordBatch:\n         seq_bytes_list = []\n         meta_list = []\n\n         for data in chunks:\n             meta_data, seq_data = self._split_meta_seq(data)\n             meta_list.append(meta_data)\n             seq_bytes_list.append(seq_data or b\"\")\n\n         if self._alphabet is None:\n             # Detect from first non-empty sequence\n             for s in seq_bytes_list:\n                 if s:\n                     self._alphabet = Alphabet.detect(s)\n                     break\n             if self._alphabet is None and seq_bytes_list:\n                  self._alphabet = Alphabet.DNA\n\n         batch = self._build_seq_batch(seq_bytes_list, self._alphabet)\n         bulk_data, starts, lengths = batch.arrays\n\n         # Build Records\n         records = []\n         for i, meta_data in enumerate(meta_list):\n             # Create View\n             s_start = starts[i]\n             s_len = lengths[i]\n             s_view = self._alphabet.seq_from(bulk_data[s_start: s_start + s_len])\n\n             # Parse Meta\n             header_lines, feature_lines = self._split_features(meta_data)\n\n             records.append(self._finalize_record(header_lines, feature_lines, s_view))\n\n         return RecordBatch.from_aligned_batch(batch, records)\n\n    def _finalize_record(self, header_lines: List[bytes], feature_lines: List[bytes], seq: Seq) -&gt; Record:\n        # 1. Parse Header\n        name = b'unknown'\n        description = b''\n\n        name, description, qualifiers = self._parse_header(header_lines)\n\n        # 2. Create Record\n        record = Record(seq, name, description, qualifiers=qualifiers)\n\n        # 3. Parse Features (Strict Column Parsing)\n        if feature_lines:\n            self._parse_feature_table(record, feature_lines)\n\n        return record\n\n    def _parse_header(self, lines: List[bytes]) -&gt; Tuple[bytes, bytes]:\n        raise NotImplementedError\n\n    def _parse_feature_table(self, record: Record, lines: List[bytes]):\n        # State machine for strict column parsing\n        # Key: 5-20 (0-indexed)\n        # Data: 21+\n\n        current_key = None\n        current_loc_parts = []\n        current_qualifiers = []      # List of (key, value identifiers)\n        current_qual_key = None      # Current qualifier key being parsed\n        current_qual_val_parts = []  # Accumulator for qualifier value\n\n        def commit_feature():\n            nonlocal current_key, current_loc_parts, current_qualifiers\n            if not current_key: return\n\n            # Parse Location\n            loc_str = b\"\".join(current_loc_parts)\n            strand = -1 if b'complement' in loc_str else 1\n            intervals = []\n            for m in self._INTERVAL_REGEX.finditer(loc_str):\n                intervals.append(Interval(int(m.group('start')) - self._base, int(m.group('end')), strand))\n\n            if not intervals:\n                if current_key == FeatureKey.SOURCE.bytes:\n                    intervals = [Interval(0, 0, 1)]\n                else:\n                    return # Skip if no location? Or default?\n\n            # Span\n            if len(intervals) &gt; 1:\n                span_start = min(i.start for i in intervals)\n                span_end = max(i.end for i in intervals)\n                feat = Feature(Interval(span_start, span_end, strand), current_key)\n            else:\n                feat = Feature(intervals[0], current_key)\n\n            # Add Qualifiers\n            for qk, qv in current_qualifiers:\n                feat.add_qualifier(qk, qv)\n\n            if feat.key == FeatureKey.SOURCE:\n                record.qualifiers.extend(feat.qualifiers)\n            else:\n                record.features.append(feat)\n\n        def commit_qualifier():\n            nonlocal current_qual_key, current_qual_val_parts\n            if current_qual_key:\n                val = b\"\".join(current_qual_val_parts)\n                # Strip quotes if present\n                if val.startswith(b'\"') and val.endswith(b'\"'):\n                    val = val[1:-1]\n\n                # If bool (no value originally), val might be empty or we flag it\n                # Logic: if no '=', val is True. But here we accumulated bytes.\n                # We need to know if '=' was present. \n                # Our parsing logic below splits on '='.\n\n                current_qualifiers.append((current_qual_key, val))\n\n            current_qual_key = None\n            current_qual_val_parts = []\n\n\n        ft_prefix_len = len(self._FEATURE_TABLE_PREFIX)\n\n        for line in lines:\n            if not line.strip(): continue\n\n            # Handle EMBL FT prefix if needed\n            if ft_prefix_len &gt; 0 and not line.startswith(self._FEATURE_TABLE_PREFIX):\n                continue\n\n            if len(line) &lt; 21: continue\n\n            key_col = line[5:20].strip()\n            data_col = line[21:].strip()\n\n            if key_col:\n                # NEW FEATURE\n                commit_qualifier() # Commit previous feature's last qualifier\n                commit_feature()   # Commit previous feature\n\n                current_key = key_col\n                current_loc_parts = [data_col]\n                current_qualifiers = []\n                current_qual_key = None\n                current_qual_val_parts = []\n\n            else:\n                # CONTINUATION LINE (Feature)\n                if not current_key: continue\n\n                if data_col.startswith(b'/'):\n                    # NEW QUALIFIER\n                    commit_qualifier()\n\n                    # Parse key=val\n                    # /strain=\"ATCC 123\"\n                    # /pseudo\n                    content = data_col[1:] # Strip '/'\n                    if b'=' in content:\n                        q_key, q_val_start = content.split(b'=', 1)\n                        current_qual_key = q_key\n                        current_qual_val_parts = [q_val_start]\n                    else:\n                        # Boolean / flag\n                        current_qualifiers.append((content, True))\n                        current_qual_key = None # No value accumulation needed\n                else:\n                    # CONTINUATION OF DATA\n                    if current_qual_key:\n                        # Continue qualifier value\n                        # Add space if needed? GenBank/EMBL spec regarding spaces in line wraps:\n                        # Usually we just join. But for text, often spaces are implied.\n                        # For keys like 'translation', no spaces.\n                        # For 'note', spaces?\n                        # Implementing standard: if it's a quote-wrapped text, spaces are likely needed \n                        # if the previous line didn't end with hyphen?\n                        # Heuristic: just append for now, maybe add space if previous char was not space?\n                        # Safest: join with space if text, join without if sequence?\n                        # Let's simple join for now to match strict byte preservation.\n                        # Actually, strictly, we trim strings. \n                        # Let's append with a space if parsing text, but maybe not for translation?\n                        # Ref: \"Qualifiers ... enclosed in double quotation marks ... string\"\n                        # We will append raw stripped parts.\n\n                        if current_qual_key == b'translation':\n                             current_qual_val_parts.append(data_col)\n                        else:\n                             # Generally add space for text fields\n                             if current_qual_val_parts: \n                                 # Check if we are inside a quote?\n                                 # This is complex without full parser.\n                                 # Simple: Join with space.\n                                 current_qual_val_parts.append(b\" \" + data_col)\n                             else:\n                                 current_qual_val_parts.append(data_col)\n                    else:\n                        # Continue location\n                        current_loc_parts.append(data_col)\n\n        # End of loop\n        commit_qualifier()\n        commit_feature()\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.InsdcReader.__init__","title":"<code>__init__(handle, alphabet=None)</code>","text":"<p>Initializes the INSDC reader.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>BinaryIO</code> <p>Binary file handle.</p> required <code>alphabet</code> <code>Alphabet</code> <p>Alphabet to use detection if None).</p> <code>None</code> Source code in <code>baclib/io/genbank.py</code> <pre><code>def __init__(self, handle: BinaryIO, alphabet: Alphabet = None):\n    \"\"\"Initializes the INSDC reader.\n\n    Args:\n        handle: Binary file handle.\n        alphabet: Alphabet to use detection if None).\n    \"\"\"\n    super().__init__(handle)\n    self._alphabet = alphabet\n    self._base = 1\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.InsdcReader.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over records.</p> <p>Yields:</p> Type Description <code>Record</code> <p><code>Record</code> objects.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>def __iter__(self) -&gt; Generator[Record, None, None]:\n    \"\"\"Iterates over records.\n\n    Yields:\n        ``Record`` objects.\n    \"\"\"\n    for chunk in self._read_chunks():\n        try:\n            yield self._parse_record_chunk(chunk)\n        except Exception:\n            # Fallback for truncated records at EOF or malformed chunks\n            pass\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.InsdcReader.batches","title":"<code>batches(size=1024)</code>","text":"<p>Reads records in batches.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Maximum records per batch.</p> <code>1024</code> <p>Yields:</p> Type Description <code>RecordBatch</code> <p><code>RecordBatch</code> objects.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>def batches(self, size: int = 1024) -&gt; Generator[RecordBatch, None, None]:\n    \"\"\"Reads records in batches.\n\n    Args:\n        size: Maximum records per batch.\n\n    Yields:\n        ``RecordBatch`` objects.\n    \"\"\"\n    chunks = []\n    for chunk in self._read_chunks():\n        chunks.append(chunk)\n        if len(chunks) &gt;= size:\n            yield self._make_batch(chunks)\n            chunks = []\n    if chunks:\n        yield self._make_batch(chunks)\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.InsdcWriter","title":"<code>InsdcWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Base writer for INSDC formats.</p> Source code in <code>baclib/io/genbank.py</code> <pre><code>class InsdcWriter(BaseWriter):\n    \"\"\"\n    Base writer for INSDC formats.\n    \"\"\"\n    _HEADER_WIDTH = 12\n    _FT_PREFIX = b'' \n\n    def _write_feature_table(self, record: Record):\n        if not record.features: return\n\n        # Write Header\n        if self._FT_PREFIX:\n             # EMBL: \"FH   Key             Location/Qualifiers\"\n             self.write_line(b\"FH   Key             Location/Qualifiers\")\n        else:\n             # GenBank: \"FEATURES             Location/Qualifiers\"\n             self.write_line(b\"FEATURES             Location/Qualifiers\")\n\n        for feature in record.features:\n            self._write_feature(feature)\n\n    def _write_feature(self, feature: Feature):\n        # Format Location\n        loc_str = self._format_location(feature.interval)\n\n        # Write Feature Line\n        # Key: col 5-20 (16 chars?)\n        # Data: col 21+\n        # GenBank: \"     source          1..100\"\n        # EMBL:    \"FT   source          1..100\"\n\n        key_bytes = feature.key.bytes\n\n        if self._FT_PREFIX:\n            # EMBL: \"FT   key...\"\n            # Prefix len 5 (\"FT   \")\n            # Key starts at index 5. matches GenBank col 6.\n            # \"FT   \" is 5 chars.\n            # \"FT   source          \"\n            # We need 21 chars total prefix before location.\n            # \"FT   \" + key + padding = 21 chars?\n            # 5 + len(key) + padding = 21 -&gt; padding = 16 - len(key)\n            padding = b\" \" * (16 - len(key_bytes))\n            line = self._FT_PREFIX + b\"   \" + key_bytes + padding + loc_str.encode('ascii')\n        else:\n            # GenBank: \"     source          \"\n            # 5 spaces + key + padding = 21 chars\n            # padding = 21 - 5 - len(key) = 16 - len(key)\n            padding = b\" \" * (16 - len(key_bytes))\n            line = b\"     \" + key_bytes + padding + loc_str.encode('ascii')\n\n        self.write_line(line)\n\n        # Write Qualifiers\n        if feature.qualifiers:\n            for key, val in feature.qualifiers.items():\n                self._write_qualifier(key, val)\n\n    def _format_location(self, interval: Interval) -&gt; str:\n        \"\"\"Formats an Interval into an INSDC location string (strings are 1-based inclusive).\"\"\"\n        # 1-based inclusive\n        start = interval.start + 1\n        end = interval.end\n\n        if start == end:\n             base = f\"{start}\"\n        else:\n             base = f\"{start}..{end}\"\n\n        if interval.strand == -1:\n            return f\"complement({base})\"\n        return base\n\n    def _write_qualifier(self, key: bytes, value: object):\n        # Format: /key=\"value\"\n        # Indent: 21 spaces.\n        # Wrap at 80.\n\n        prefix = b' ' * 21\n        if self._FT_PREFIX: prefix = self._FT_PREFIX + b' ' * 16 # EMBL FT lines start with FT + spaces to col 21?\n        # EMBL \"FT                   /organism=...\"\n        # FT is 2 chars. + 19 spaces = 21.\n\n        if self._FT_PREFIX:\n             prefix = self._FT_PREFIX + b' ' * (21 - len(self._FT_PREFIX))\n\n        q_key = b'/' + key\n\n        # Determine value string\n        if value is True:\n             # Boolean flag\n             self.write_line(prefix + q_key)\n             return\n\n        val_bytes = str(value).encode('ascii') if not isinstance(value, bytes) else value\n\n        # Check if we should quote\n        # Generally yes for text.\n        # Integer values (e.g. taxid) might not need quotes but usually safe to quote or required?\n        # Specification says \"most qualifiers take values... enclosed in double quotes\".\n        # Some like /codon_start are integers (no quotes? or quotes allowed?)\n        # We will quote everything for simplicity unless it's strictly forbidden (which is rare).\n\n        # Escape quotes in value?\n        # Replace \" with \"\"? simple escape.\n        val_bytes = val_bytes.replace(b'\"', b'\"\"')\n\n        full_val = q_key + b'=\"' + val_bytes + b'\"'\n\n        # Wrap logic\n        # Available width = 80 - 21 = 59 chars.\n\n        if len(full_val) + len(prefix) &lt;= 80:\n             self.write_line(prefix + full_val)\n             return\n\n        # Multi-line wrap\n        # First line\n        # We can split full_val\n\n        # Greedy wrap\n        current = full_val\n        first = True\n\n        while current:\n             limit = 80 - len(prefix)\n             if len(current) &lt;= limit:\n                 self.write_line(prefix + current)\n                 break\n\n             # Need to split\n             # Try to split at space if possible?\n             # User said \"avoid textwrap\", implication is purely binary speed or specific logic.\n             # INSDC prefers breaking at space.\n\n             chunk = current[:limit]\n             # Check for space in last 10 chars to break nicely?\n             # Or just hard break?\n             # Let's do hard break to ensure strict column width compliance safely.\n             # \"break at arbitrary position\" is allowed?\n             # Spec: \"lines... max 80 chars\".\n\n             self.write_line(prefix + chunk)\n             current = current[limit:]\n\n    def write_line(self, line: bytes):\n        self._handle.write(line + b'\\n')\n\n    def write_one(self, record: Record):\n        \"\"\"Writes a single ``Record`` to the handle.\n\n        Args:\n            record: The record to write.\n        \"\"\"\n        # Implemented by subclasses (Header, Sequence, Terminator)\n        raise NotImplementedError\n</code></pre>"},{"location":"reference/baclib/io/genbank/#baclib.io.genbank.InsdcWriter.write_one","title":"<code>write_one(record)</code>","text":"<p>Writes a single <code>Record</code> to the handle.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>The record to write.</p> required Source code in <code>baclib/io/genbank.py</code> <pre><code>def write_one(self, record: Record):\n    \"\"\"Writes a single ``Record`` to the handle.\n\n    Args:\n        record: The record to write.\n    \"\"\"\n    # Implemented by subclasses (Header, Sequence, Terminator)\n    raise NotImplementedError\n</code></pre>"},{"location":"reference/baclib/io/motif/","title":"motif","text":""},{"location":"reference/baclib/io/motif/#baclib.io.motif","title":"<code>baclib.io.motif</code>","text":"<p>Readers for motif matrix formats including MEME and TRANSFAC.</p>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.MemeReader","title":"<code>MemeReader</code>","text":"<p>               Bases: <code>MotifReader</code></p> <p>Reader for MEME format files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with open(\"motifs.meme\", \"rb\") as f:\n...     reader = MemeReader(f)\n...     for motif in reader:\n...         print(motif.name)\n</code></pre> Source code in <code>baclib/io/motif.py</code> <pre><code>@SeqFile.register(SeqFileFormat.MEME, extensions=['.meme', '.txt'])\nclass MemeReader(MotifReader):\n    \"\"\"\n    Reader for MEME format files.\n\n    Examples:\n        &gt;&gt;&gt; with open(\"motifs.meme\", \"rb\") as f:\n        ...     reader = MemeReader(f)\n        ...     for motif in reader:\n        ...         print(motif.name)\n    \"\"\"\n\n    def __iter__(self) -&gt; Generator[Motif, None, None]:\n        \"\"\"Iterates over motifs in the MEME file.\n\n        Yields:\n            ``Motif`` objects.\n        \"\"\"\n        lines = (line.strip() for line in self._handle)\n\n        current_name = b\"Unknown\"\n        matrix_lines = []\n        in_matrix = False\n        alphabet_map = [2, 1, 3, 0]  # Default MEME (ACGT) -&gt; BacLib DNA (TCAG)\n\n        for line in lines:\n            if not line: continue\n\n            if line.startswith(b'ALPHABET='):\n                parts = line.split(b'=')\n                if len(parts) &gt; 1:\n                    meme_alpha = parts[1].strip()\n                    new_map = []\n                    for char_code in meme_alpha:\n                        enc = self.background.alphabet.encode(bytes([char_code]))\n                        if len(enc) &gt; 0 and enc[0] != self.background.alphabet.INVALID:\n                            new_map.append(enc[0])\n                        else:\n                            new_map.append(-1)\n                    alphabet_map = new_map\n\n            elif line.startswith(b'MOTIF'):\n                if matrix_lines:\n                    yield self._make_motif(current_name, matrix_lines, alphabet_map)\n                    matrix_lines = []\n                    in_matrix = False\n\n                parts = line.split()\n                if len(parts) &gt;= 2:\n                    current_name = parts[1]\n                    if len(parts) &gt; 2:\n                        current_name = parts[1] + b\" \" + parts[2]\n                else:\n                    current_name = b\"Unknown\"\n\n            elif line.startswith(b'letter-probability matrix:'):\n                in_matrix = True\n\n            elif in_matrix:\n                if line.startswith(b'URL') or line.startswith(b'MOTIF') or line.startswith(b'--------'):\n                    in_matrix = False\n                    if matrix_lines:\n                        yield self._make_motif(current_name, matrix_lines, alphabet_map)\n                        matrix_lines = []\n\n                    if line.startswith(b'MOTIF'):\n                        parts = line.split()\n                        if len(parts) &gt;= 2:\n                            current_name = parts[1]\n                            if len(parts) &gt; 2:\n                                current_name = parts[1] + b\" \" + parts[2]\n                        else:\n                            current_name = b\"Unknown\"\n                else:\n                    try:\n                        float(line.split()[0])\n                        matrix_lines.append(line)\n                    except ValueError:\n                        in_matrix = False\n\n        if matrix_lines:\n            yield self._make_motif(current_name, matrix_lines, alphabet_map)\n\n    def _make_motif(self, name, lines, mapping):\n        data = []\n        for l in lines:\n            data.append([float(x) for x in l.split()])\n\n        raw_freqs = np.array(data, dtype=np.float32)\n        n_rows, n_cols = raw_freqs.shape\n        target_len = len(self.background.alphabet)\n\n        mapped_freqs = np.zeros((n_rows, target_len), dtype=np.float32)\n\n        for i in range(min(n_cols, len(mapping))):\n            target_idx = mapping[i]\n            if target_idx != -1 and target_idx &lt; target_len:\n                mapped_freqs[:, target_idx] = raw_freqs[:, i]\n\n        return Motif.from_frequencies(name, mapped_freqs.T, self.background)\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if input bytes look like MEME format.\"\"\"\n        return s.startswith(b'MEME version')\n</code></pre>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.MemeReader.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over motifs in the MEME file.</p> <p>Yields:</p> Type Description <code>Motif</code> <p><code>Motif</code> objects.</p> Source code in <code>baclib/io/motif.py</code> <pre><code>def __iter__(self) -&gt; Generator[Motif, None, None]:\n    \"\"\"Iterates over motifs in the MEME file.\n\n    Yields:\n        ``Motif`` objects.\n    \"\"\"\n    lines = (line.strip() for line in self._handle)\n\n    current_name = b\"Unknown\"\n    matrix_lines = []\n    in_matrix = False\n    alphabet_map = [2, 1, 3, 0]  # Default MEME (ACGT) -&gt; BacLib DNA (TCAG)\n\n    for line in lines:\n        if not line: continue\n\n        if line.startswith(b'ALPHABET='):\n            parts = line.split(b'=')\n            if len(parts) &gt; 1:\n                meme_alpha = parts[1].strip()\n                new_map = []\n                for char_code in meme_alpha:\n                    enc = self.background.alphabet.encode(bytes([char_code]))\n                    if len(enc) &gt; 0 and enc[0] != self.background.alphabet.INVALID:\n                        new_map.append(enc[0])\n                    else:\n                        new_map.append(-1)\n                alphabet_map = new_map\n\n        elif line.startswith(b'MOTIF'):\n            if matrix_lines:\n                yield self._make_motif(current_name, matrix_lines, alphabet_map)\n                matrix_lines = []\n                in_matrix = False\n\n            parts = line.split()\n            if len(parts) &gt;= 2:\n                current_name = parts[1]\n                if len(parts) &gt; 2:\n                    current_name = parts[1] + b\" \" + parts[2]\n            else:\n                current_name = b\"Unknown\"\n\n        elif line.startswith(b'letter-probability matrix:'):\n            in_matrix = True\n\n        elif in_matrix:\n            if line.startswith(b'URL') or line.startswith(b'MOTIF') or line.startswith(b'--------'):\n                in_matrix = False\n                if matrix_lines:\n                    yield self._make_motif(current_name, matrix_lines, alphabet_map)\n                    matrix_lines = []\n\n                if line.startswith(b'MOTIF'):\n                    parts = line.split()\n                    if len(parts) &gt;= 2:\n                        current_name = parts[1]\n                        if len(parts) &gt; 2:\n                            current_name = parts[1] + b\" \" + parts[2]\n                    else:\n                        current_name = b\"Unknown\"\n            else:\n                try:\n                    float(line.split()[0])\n                    matrix_lines.append(line)\n                except ValueError:\n                    in_matrix = False\n\n    if matrix_lines:\n        yield self._make_motif(current_name, matrix_lines, alphabet_map)\n</code></pre>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.MemeReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if input bytes look like MEME format.</p> Source code in <code>baclib/io/motif.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if input bytes look like MEME format.\"\"\"\n    return s.startswith(b'MEME version')\n</code></pre>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.MotifReader","title":"<code>MotifReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Base class for motif readers.</p> Source code in <code>baclib/io/motif.py</code> <pre><code>class MotifReader(BaseReader):\n    \"\"\"Base class for motif readers.\"\"\"\n    _DEFAULT_BACKGROUND = Background.uniform(Alphabet.DNA)\n    def __init__(self, handle: BinaryIO, background: Background = None, **kwargs):\n        \"\"\"Initializes the MotifReader.\n\n        Args:\n            handle: Binary file handle.\n            background: Background distribution (defaults to uniform DNA).\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__(handle, **kwargs)\n        self.background: Background = background or self._DEFAULT_BACKGROUND\n\n    def _make_batch(self, items: list) -&gt; MotifBatch:\n        return MotifBatch(items)\n</code></pre>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.MotifReader.__init__","title":"<code>__init__(handle, background=None, **kwargs)</code>","text":"<p>Initializes the MotifReader.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>BinaryIO</code> <p>Binary file handle.</p> required <code>background</code> <code>Background</code> <p>Background distribution (defaults to uniform DNA).</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>baclib/io/motif.py</code> <pre><code>def __init__(self, handle: BinaryIO, background: Background = None, **kwargs):\n    \"\"\"Initializes the MotifReader.\n\n    Args:\n        handle: Binary file handle.\n        background: Background distribution (defaults to uniform DNA).\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__(handle, **kwargs)\n    self.background: Background = background or self._DEFAULT_BACKGROUND\n</code></pre>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.TransfacReader","title":"<code>TransfacReader</code>","text":"<p>               Bases: <code>MotifReader</code></p> <p>Reader for TRANSFAC format files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with open(\"motifs.transfac\", \"rb\") as f:\n...     reader = TransfacReader(f)\n...     for motif in reader:\n...         print(motif.name)\n</code></pre> Source code in <code>baclib/io/motif.py</code> <pre><code>@SeqFile.register(SeqFileFormat.TRANSFAC, extensions=['.transfac', '.tf'])\nclass TransfacReader(MotifReader):\n    \"\"\"\n    Reader for TRANSFAC format files.\n\n    Examples:\n        &gt;&gt;&gt; with open(\"motifs.transfac\", \"rb\") as f:\n        ...     reader = TransfacReader(f)\n        ...     for motif in reader:\n        ...         print(motif.name)\n    \"\"\"\n\n    def __iter__(self) -&gt; Generator[Motif, None, None]:\n        \"\"\"Iterates over motifs in the TRANSFAC file.\n\n        Yields:\n            ``Motif`` objects.\n        \"\"\"\n        lines = (line.strip() for line in self._handle)\n\n        current_id = b\"Unknown\"\n        counts = []\n        base_order = None\n\n        for line in lines:\n            if not line: continue\n\n            if line.startswith(b'ID'):\n                parts = line.split(maxsplit=1)\n                current_id = parts[1] if len(parts) &gt; 1 else b\"Unknown\"\n                counts = []\n                base_order = None\n\n            elif line.startswith(b'P0'):\n                parts = line.split()\n                base_order = [self.background.alphabet.encode(b)[0] if self.background.alphabet.encode(b).size &gt; 0 else -1 for b in parts[1:]]\n\n            elif line[0] in b'0123456789' and base_order is not None:\n                parts = line.split()\n                if len(parts) &lt; 2 or not parts[0].isdigit(): continue\n                try:\n                    row_counts = [float(x) for x in parts[1:1 + len(base_order)]]\n                    mapped_row = np.zeros(len(self.background.alphabet), dtype=np.float32)\n                    for i, count in enumerate(row_counts):\n                        if i &lt; len(base_order) and base_order[i] != -1:\n                            mapped_row[base_order[i]] = count\n                    counts.append(mapped_row)\n                except ValueError: continue\n\n            elif line.startswith(b'//'):\n                if counts:\n                    yield Motif.from_counts(current_id, np.array(counts, dtype=np.float32).T, self.background)\n                current_id = b\"Unknown\"\n                counts = []\n                base_order = None\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if input bytes look like TRANSFAC format.\"\"\"\n        return s.startswith(b'ID') or s.startswith(b'AC') or s.startswith(b'VV')\n</code></pre>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.TransfacReader.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over motifs in the TRANSFAC file.</p> <p>Yields:</p> Type Description <code>Motif</code> <p><code>Motif</code> objects.</p> Source code in <code>baclib/io/motif.py</code> <pre><code>def __iter__(self) -&gt; Generator[Motif, None, None]:\n    \"\"\"Iterates over motifs in the TRANSFAC file.\n\n    Yields:\n        ``Motif`` objects.\n    \"\"\"\n    lines = (line.strip() for line in self._handle)\n\n    current_id = b\"Unknown\"\n    counts = []\n    base_order = None\n\n    for line in lines:\n        if not line: continue\n\n        if line.startswith(b'ID'):\n            parts = line.split(maxsplit=1)\n            current_id = parts[1] if len(parts) &gt; 1 else b\"Unknown\"\n            counts = []\n            base_order = None\n\n        elif line.startswith(b'P0'):\n            parts = line.split()\n            base_order = [self.background.alphabet.encode(b)[0] if self.background.alphabet.encode(b).size &gt; 0 else -1 for b in parts[1:]]\n\n        elif line[0] in b'0123456789' and base_order is not None:\n            parts = line.split()\n            if len(parts) &lt; 2 or not parts[0].isdigit(): continue\n            try:\n                row_counts = [float(x) for x in parts[1:1 + len(base_order)]]\n                mapped_row = np.zeros(len(self.background.alphabet), dtype=np.float32)\n                for i, count in enumerate(row_counts):\n                    if i &lt; len(base_order) and base_order[i] != -1:\n                        mapped_row[base_order[i]] = count\n                counts.append(mapped_row)\n            except ValueError: continue\n\n        elif line.startswith(b'//'):\n            if counts:\n                yield Motif.from_counts(current_id, np.array(counts, dtype=np.float32).T, self.background)\n            current_id = b\"Unknown\"\n            counts = []\n            base_order = None\n</code></pre>"},{"location":"reference/baclib/io/motif/#baclib.io.motif.TransfacReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if input bytes look like TRANSFAC format.</p> Source code in <code>baclib/io/motif.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if input bytes look like TRANSFAC format.\"\"\"\n    return s.startswith(b'ID') or s.startswith(b'AC') or s.startswith(b'VV')\n</code></pre>"},{"location":"reference/baclib/io/seq/","title":"seq","text":""},{"location":"reference/baclib/io/seq/#baclib.io.seq","title":"<code>baclib.io.seq</code>","text":"<p>Readers and writers for sequence formats: FASTA, FASTQ, and GFA.</p>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaReader","title":"<code>FastaReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reader for FASTA format files.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>@SeqFile.register(SeqFileFormat.FASTA, extensions=['.fasta', '.fa', '.fna', '.faa', '.ffn'],\n                  alphabets={'.faa': Alphabet.AMINO, '.fna': Alphabet.DNA, '.ffn': Alphabet.DNA})\nclass FastaReader(BaseReader):\n    \"\"\"\n    Reader for FASTA format files.\n    \"\"\"\n    __slots__ = ('_alphabet', '_min_seq_length', '_n_seq_lines')\n    def __init__(self, handle: BinaryIO, alphabet: Alphabet = None, min_seq_length: int = 1, n_seq_lines: int = 0, **kwargs):\n        \"\"\"Initializes the FASTA reader.\n\n        Args:\n            handle: Binary file handle.\n            alphabet: Alphabet to use (auto-detected if None).\n            min_seq_length: Minimum sequence length to yield.\n            n_seq_lines: Number of lines per sequence (optimization for fixed-width FASTA).\n            **kwargs: Additional arguments for base reader.\n        \"\"\"\n        super().__init__(handle, **kwargs)\n        self._alphabet = alphabet\n        self._min_seq_length = min_seq_length\n        self._n_seq_lines = n_seq_lines\n\n    def __iter__(self) -&gt; Generator[Record, None, None]:\n        \"\"\"Iterates over FASTA records.\n\n        Yields:\n            ``Record`` objects.\n        \"\"\"\n        if self._alphabet is None:\n            # First element logic to detect alphabet\n            iterator = self._read_entries()\n            try:\n                first_header, first_seq_parts = next(iterator)\n                # Combine parts to detect\n                first_seq_bytes = b\"\".join(first_seq_parts)\n                self._alphabet = Alphabet.detect(first_seq_bytes)\n                yield self._make_record(first_header, [first_seq_bytes]) # Pass as list to reuse _make_record logic if possible or just make record directly\n            except StopIteration:\n                return\n\n            for header, seq_parts in iterator:\n                yield self._make_record(header, seq_parts)\n        else:\n            for header, seq_parts in self._read_entries():\n                yield self._make_record(header, seq_parts)\n\n    def batches(self, size: int = 1024) -&gt; Generator[RecordBatch, None, None]:\n        \"\"\"Optimized batch reader that performs bulk encoding.\n\n        Args:\n            size: Maximum number of records per batch.\n\n        Yields:\n            ``RecordBatch`` objects.\n        \"\"\"\n        entries = []\n        iterator = self._read_entries()\n\n        # Auto-detect alphabet from first entry if needed\n        if self._alphabet is None:\n            try:\n                first_entry = next(iterator)\n                # first_entry is (header, seq_parts)\n                first_seq = b\"\".join(first_entry[1])\n                self._alphabet = Alphabet.detect(first_seq)\n                entries.append(first_entry)\n            except StopIteration:\n                return\n\n        for entry in iterator:\n            entries.append(entry)\n            if len(entries) &gt;= size:\n                yield self._make_batch(entries)\n                entries = []\n        if entries:\n            yield self._make_batch(entries)\n\n    def _read_entries(self):\n        \"\"\"Internal generator that yields (header, seq_parts_list).\"\"\"\n        if self._n_seq_lines &gt; 0:\n            yield from self._read_entries_fixed()\n            return\n\n        min_len = self._min_seq_length\n\n        buf = bytearray()\n        header = None\n        seq_parts = []\n        current_len = 0\n\n        for chunk in self.read_chunks(self._CHUNK_SIZE):\n            if not chunk:\n                if header is not None:\n                    if buf: \n                        seq_parts.append(bytes(buf))\n                        current_len += len(buf)\n                    if current_len &gt;= min_len:\n                        yield header, seq_parts\n                break\n\n            buf.extend(chunk)\n            pos = 0\n\n            while True:\n                gt_pos = buf.find(b'&gt;', pos)\n\n                if gt_pos == -1:\n                    if header is not None:\n                        part = bytes(buf[pos:])\n                        seq_parts.append(part)\n                        current_len += len(part)\n                    buf.clear()\n                    break\n\n                if header is not None:\n                    part = bytes(buf[pos:gt_pos])\n                    seq_parts.append(part)\n                    current_len += len(part)\n                    if current_len &gt;= min_len:\n                        yield header, seq_parts\n                    seq_parts = []\n                    current_len = 0\n                    header = None\n\n                nl_pos = buf.find(b'\\n', gt_pos)\n                if nl_pos == -1:\n                    del buf[:gt_pos]\n                    break\n\n                header = bytes(buf[gt_pos+1:nl_pos]).rstrip()\n                pos = nl_pos + 1\n\n    def _read_entries_fixed(self):\n        \"\"\"Optimized reader for FASTA with fixed number of sequence lines (e.g. unwrapped).\"\"\"\n        n_lines = self._n_seq_lines\n        min_len = self._min_seq_length\n        buf = bytearray()\n\n        state = 0 # 0=Header, 1=Sequence\n        header = None\n        seq_parts = []\n        seq_lines_read = 0\n\n        for chunk in self.read_chunks(self._CHUNK_SIZE):\n            if not chunk: break\n            buf.extend(chunk)\n            pos = 0\n\n            while True:\n                nl_pos = buf.find(b'\\n', pos)\n                if nl_pos == -1:\n                    del buf[:pos]\n                    break\n\n                # Check for &gt; at start of line\n                is_header_start = (buf[pos] == 62)\n\n                if state == 0:\n                    if is_header_start:\n                        header = bytes(buf[pos+1:nl_pos]).rstrip()\n                        state = 1\n                        seq_parts = []\n                        seq_lines_read = 0\n\n                elif state == 1:\n                    if is_header_start:\n                        raise ParserError(f\"Found '&gt;' inside sequence. Check n_seq_lines={n_lines}.\")\n\n                    seq_parts.append(bytes(buf[pos:nl_pos]))\n                    seq_lines_read += 1\n\n                    if seq_lines_read == n_lines:\n                        if sum(len(p) for p in seq_parts) &gt;= min_len:\n                            yield header, seq_parts\n                        state = 0\n\n                pos = nl_pos + 1\n\n        if state == 1 and seq_parts and sum(len(p) for p in seq_parts) &gt;= min_len:\n            yield header, seq_parts\n\n    def _make_record(self, header, seq_parts: Iterable[bytes]) -&gt; Record:\n        name, _, desc = header.partition(b' ')\n        # Encode the whole sequence at once (releases GIL in Numba)\n        return Record(self._alphabet.seq_from(b\"\".join(seq_parts)), name, desc)\n\n    def _make_batch(self, entries) -&gt; RecordBatch:\n        # Join parts for each record and encode individually to ensure correct lengths\n        raw_seqs = [b\"\".join(parts) for _, parts in entries]\n        batch = self._build_seq_batch(raw_seqs, self._alphabet)\n        encoded_data, starts, lengths = batch.arrays\n\n        # 3. Create Records with Views\n        records = []\n        for i, (header, _) in enumerate(entries):\n            name, _, desc = header.partition(b' ')\n\n            # Create a Seq that views the batch memory (Zero Copy)\n            s_start = starts[i]\n            s_view = self._alphabet.seq_from(encoded_data[s_start: s_start + lengths[i]])\n\n            # Parse metadata\n            records.append(Record(s_view, name, desc))\n\n        return RecordBatch.from_aligned_batch(batch, records)\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if the input bytes look like a FASTA file header.\"\"\"\n        return s.startswith(b\"&gt;\")\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaReader.__init__","title":"<code>__init__(handle, alphabet=None, min_seq_length=1, n_seq_lines=0, **kwargs)</code>","text":"<p>Initializes the FASTA reader.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>BinaryIO</code> <p>Binary file handle.</p> required <code>alphabet</code> <code>Alphabet</code> <p>Alphabet to use (auto-detected if None).</p> <code>None</code> <code>min_seq_length</code> <code>int</code> <p>Minimum sequence length to yield.</p> <code>1</code> <code>n_seq_lines</code> <code>int</code> <p>Number of lines per sequence (optimization for fixed-width FASTA).</p> <code>0</code> <code>**kwargs</code> <p>Additional arguments for base reader.</p> <code>{}</code> Source code in <code>baclib/io/seq.py</code> <pre><code>def __init__(self, handle: BinaryIO, alphabet: Alphabet = None, min_seq_length: int = 1, n_seq_lines: int = 0, **kwargs):\n    \"\"\"Initializes the FASTA reader.\n\n    Args:\n        handle: Binary file handle.\n        alphabet: Alphabet to use (auto-detected if None).\n        min_seq_length: Minimum sequence length to yield.\n        n_seq_lines: Number of lines per sequence (optimization for fixed-width FASTA).\n        **kwargs: Additional arguments for base reader.\n    \"\"\"\n    super().__init__(handle, **kwargs)\n    self._alphabet = alphabet\n    self._min_seq_length = min_seq_length\n    self._n_seq_lines = n_seq_lines\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaReader.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over FASTA records.</p> <p>Yields:</p> Type Description <code>Record</code> <p><code>Record</code> objects.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>def __iter__(self) -&gt; Generator[Record, None, None]:\n    \"\"\"Iterates over FASTA records.\n\n    Yields:\n        ``Record`` objects.\n    \"\"\"\n    if self._alphabet is None:\n        # First element logic to detect alphabet\n        iterator = self._read_entries()\n        try:\n            first_header, first_seq_parts = next(iterator)\n            # Combine parts to detect\n            first_seq_bytes = b\"\".join(first_seq_parts)\n            self._alphabet = Alphabet.detect(first_seq_bytes)\n            yield self._make_record(first_header, [first_seq_bytes]) # Pass as list to reuse _make_record logic if possible or just make record directly\n        except StopIteration:\n            return\n\n        for header, seq_parts in iterator:\n            yield self._make_record(header, seq_parts)\n    else:\n        for header, seq_parts in self._read_entries():\n            yield self._make_record(header, seq_parts)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaReader.batches","title":"<code>batches(size=1024)</code>","text":"<p>Optimized batch reader that performs bulk encoding.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Maximum number of records per batch.</p> <code>1024</code> <p>Yields:</p> Type Description <code>RecordBatch</code> <p><code>RecordBatch</code> objects.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>def batches(self, size: int = 1024) -&gt; Generator[RecordBatch, None, None]:\n    \"\"\"Optimized batch reader that performs bulk encoding.\n\n    Args:\n        size: Maximum number of records per batch.\n\n    Yields:\n        ``RecordBatch`` objects.\n    \"\"\"\n    entries = []\n    iterator = self._read_entries()\n\n    # Auto-detect alphabet from first entry if needed\n    if self._alphabet is None:\n        try:\n            first_entry = next(iterator)\n            # first_entry is (header, seq_parts)\n            first_seq = b\"\".join(first_entry[1])\n            self._alphabet = Alphabet.detect(first_seq)\n            entries.append(first_entry)\n        except StopIteration:\n            return\n\n    for entry in iterator:\n        entries.append(entry)\n        if len(entries) &gt;= size:\n            yield self._make_batch(entries)\n            entries = []\n    if entries:\n        yield self._make_batch(entries)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if the input bytes look like a FASTA file header.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if the input bytes look like a FASTA file header.\"\"\"\n    return s.startswith(b\"&gt;\")\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaWriter","title":"<code>FastaWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for FASTA format files.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>@SeqFile.register(SeqFileFormat.FASTA)\nclass FastaWriter(BaseWriter):\n    \"\"\"\n    Writer for FASTA format files.\n    \"\"\"\n    __slots__ = ('width',)\n    def __init__(self, file: Union[str, Path, BinaryIO], width: int = 0, **kwargs):\n        \"\"\"\n        Initializes the FastaWriter.\n\n        Args:\n            file: File path or object.\n            width: Line width for sequence wrapping (0 for no wrapping).\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__(file, **kwargs)\n        self.width = width\n\n    def write_one(self, record: Record):\n        \"\"\"\n        Writes a single FASTA record.\n\n        Args:\n            record: The Record object to write.\n        \"\"\"\n        if not isinstance(record, Record): raise TypeError(\"FastaWriter expects Record objects\")\n\n        header = b\"&gt;\" + record.id\n        if record.description: header += b\" \" + record.description\n        self._handle.write(header + b\"\\n\")\n\n        # Optimization: Decode once, then slice bytes (faster than decoding slices)\n        seq_bytes = bytes(record.seq)\n        width = self.width\n\n        if width &gt; 0:\n            self._handle.write(b\"\\n\".join(seq_bytes[i:i+width] for i in range(0, len(seq_bytes), width)) + b\"\\n\")\n        else:\n            self._handle.write(seq_bytes + b\"\\n\")\n\n    def write_batch(self, batch: Batch):\n        \"\"\"Writes a batch of sequences efficiently.\n\n        Args:\n            batch: ``RecordBatch`` or other batch type.\n        \"\"\"\n        if isinstance(batch, RecordBatch):\n            # Optimization: Decode entire batch data at once to avoid N translate calls\n            seqs = batch.seqs\n            ids = batch.ids\n            decoded = seqs.alphabet.decode(seqs.encoded)\n            starts = seqs.starts\n            lengths = seqs.lengths\n            width = self.width\n\n            buffer = []\n\n            for i in range(len(batch)):\n                buffer.append(b\"&gt;\")\n                buffer.append(ids[i])\n                buffer.append(b\"\\n\")\n\n                s = starts[i]\n                l = lengths[i]\n                seq_content = decoded[s : s+l]\n\n                if width &gt; 0:\n                    buffer.append(b\"\\n\".join(seq_content[j:j+width] for j in range(0, l, width)))\n                else:\n                    buffer.append(seq_content)\n                buffer.append(b\"\\n\")\n\n                if len(buffer) &gt;= 1000:\n                    self._handle.write(b\"\".join(buffer))\n                    buffer = []\n\n            if buffer:\n                self._handle.write(b\"\".join(buffer))\n        else:\n            super().write_batch(batch)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaWriter.__init__","title":"<code>__init__(file, width=0, **kwargs)</code>","text":"<p>Initializes the FastaWriter.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path, BinaryIO]</code> <p>File path or object.</p> required <code>width</code> <code>int</code> <p>Line width for sequence wrapping (0 for no wrapping).</p> <code>0</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>baclib/io/seq.py</code> <pre><code>def __init__(self, file: Union[str, Path, BinaryIO], width: int = 0, **kwargs):\n    \"\"\"\n    Initializes the FastaWriter.\n\n    Args:\n        file: File path or object.\n        width: Line width for sequence wrapping (0 for no wrapping).\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__(file, **kwargs)\n    self.width = width\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaWriter.write_batch","title":"<code>write_batch(batch)</code>","text":"<p>Writes a batch of sequences efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p><code>RecordBatch</code> or other batch type.</p> required Source code in <code>baclib/io/seq.py</code> <pre><code>def write_batch(self, batch: Batch):\n    \"\"\"Writes a batch of sequences efficiently.\n\n    Args:\n        batch: ``RecordBatch`` or other batch type.\n    \"\"\"\n    if isinstance(batch, RecordBatch):\n        # Optimization: Decode entire batch data at once to avoid N translate calls\n        seqs = batch.seqs\n        ids = batch.ids\n        decoded = seqs.alphabet.decode(seqs.encoded)\n        starts = seqs.starts\n        lengths = seqs.lengths\n        width = self.width\n\n        buffer = []\n\n        for i in range(len(batch)):\n            buffer.append(b\"&gt;\")\n            buffer.append(ids[i])\n            buffer.append(b\"\\n\")\n\n            s = starts[i]\n            l = lengths[i]\n            seq_content = decoded[s : s+l]\n\n            if width &gt; 0:\n                buffer.append(b\"\\n\".join(seq_content[j:j+width] for j in range(0, l, width)))\n            else:\n                buffer.append(seq_content)\n            buffer.append(b\"\\n\")\n\n            if len(buffer) &gt;= 1000:\n                self._handle.write(b\"\".join(buffer))\n                buffer = []\n\n        if buffer:\n            self._handle.write(b\"\".join(buffer))\n    else:\n        super().write_batch(batch)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastaWriter.write_one","title":"<code>write_one(record)</code>","text":"<p>Writes a single FASTA record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>The Record object to write.</p> required Source code in <code>baclib/io/seq.py</code> <pre><code>def write_one(self, record: Record):\n    \"\"\"\n    Writes a single FASTA record.\n\n    Args:\n        record: The Record object to write.\n    \"\"\"\n    if not isinstance(record, Record): raise TypeError(\"FastaWriter expects Record objects\")\n\n    header = b\"&gt;\" + record.id\n    if record.description: header += b\" \" + record.description\n    self._handle.write(header + b\"\\n\")\n\n    # Optimization: Decode once, then slice bytes (faster than decoding slices)\n    seq_bytes = bytes(record.seq)\n    width = self.width\n\n    if width &gt; 0:\n        self._handle.write(b\"\\n\".join(seq_bytes[i:i+width] for i in range(0, len(seq_bytes), width)) + b\"\\n\")\n    else:\n        self._handle.write(seq_bytes + b\"\\n\")\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastqReader","title":"<code>FastqReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reader for FASTQ format files. Optimized for standard 4-line FASTQ records. Wrapped FASTQ sequences are not supported.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with open(\"reads.fastq\", \"rb\") as f:\n...     reader = FastqReader(f)\n...     for record in reader:\n...         print(record.id)\n</code></pre> Source code in <code>baclib/io/seq.py</code> <pre><code>@SeqFile.register(SeqFileFormat.FASTQ, extensions=['.fastq', '.fq'])\nclass FastqReader(BaseReader):\n    \"\"\"\n    Reader for FASTQ format files.\n    Optimized for standard 4-line FASTQ records. Wrapped FASTQ sequences are not supported.\n\n    Examples:\n        &gt;&gt;&gt; with open(\"reads.fastq\", \"rb\") as f:\n        ...     reader = FastqReader(f)\n        ...     for record in reader:\n        ...         print(record.id)\n    \"\"\"\n    __slots__ = ('_alphabet', '_min_seq_length')\n    def __init__(self, handle: BinaryIO, min_seq_length: int = 1, **kwargs):\n        \"\"\"Initializes the FASTQ reader.\n\n        Args:\n            handle: Binary file handle.\n            min_seq_length: Minimum sequence length.\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__(handle, **kwargs)\n        self._alphabet = Alphabet.DNA\n        self._min_seq_length = min_seq_length\n\n    def __iter__(self) -&gt; Generator[Record, None, None]:\n        \"\"\"Iterates over FASTQ records.\n\n        Yields:\n            ``Record`` objects.\n        \"\"\"\n        for header, seq_bytes, qual_bytes in self._read_entries():\n            name, _, desc = header.partition(b' ')\n            yield Record(self._alphabet.seq_from(seq_bytes), name, desc, qualifiers=[(b'quality', qual_bytes)])\n\n    def batches(self, size: int = 1024):\n        \"\"\"Reads FASTQ records in batches.\n\n        Args:\n            size: Maximum records per batch.\n\n        Yields:\n            ``RecordBatch`` objects.\n        \"\"\"\n        entries = []\n        for entry in self._read_entries():\n            entries.append(entry)\n            if len(entries) &gt;= size:\n                yield self._make_batch(entries)\n                entries = []\n        if entries:\n            yield self._make_batch(entries)\n\n    def _read_entries(self):\n        min_len = self._min_seq_length\n        buf = bytearray()\n        for chunk in self.read_chunks(self._CHUNK_SIZE):\n            if not chunk:\n                if buf.strip():\n                    # Ensure last line has a newline to simplify parsing logic\n                    if not buf.endswith(b'\\n'): buf.extend(b'\\n')\n                else:\n                    break\n            else:\n                buf.extend(chunk)\n\n            pos = 0\n            n_len = len(buf)\n\n            while pos &lt; n_len:\n                # Skip whitespace between records\n                while pos &lt; n_len:\n                    b = buf[pos]\n                    if b == 10 or b == 13 or b == 32 or b == 9:\n                        pos += 1\n                    else: break\n\n                if pos &gt;= n_len: break\n\n                # Start of record check\n                if buf[pos] != 64:  # @\n                    raise ParserError(f\"Invalid FASTQ header at byte {pos}: expected '@'\")\n\n                # Find 4 newlines\n                nl1 = buf.find(b'\\n', pos)\n                if nl1 == -1: break\n\n                nl2 = buf.find(b'\\n', nl1 + 1)\n                if nl2 == -1: break\n\n                nl3 = buf.find(b'\\n', nl2 + 1)\n                if nl3 == -1: break\n\n                nl4 = buf.find(b'\\n', nl3 + 1)\n                if nl4 == -1: break\n\n                # Extract fields\n                header = bytes(buf[pos + 1:nl1]).rstrip()\n                seq_bytes = bytes(buf[nl1 + 1:nl2]).rstrip()\n                qual_bytes = bytes(buf[nl3 + 1:nl4]).rstrip()\n\n                if len(seq_bytes) &gt;= min_len:\n                    yield header, seq_bytes, qual_bytes\n\n                pos = nl4 + 1\n\n            if pos &gt; 0: del buf[:pos]\n\n    def _make_batch(self, entries):\n        batch = self._build_seq_batch([e[1] for e in entries], self._alphabet)\n        encoded_data, starts, lengths = batch.arrays\n\n        records = []\n        for i, (header, _, qual) in enumerate(entries):\n            name, _, desc = header.partition(b' ')\n            s_start = starts[i]\n            s_view = self._alphabet.seq_from(encoded_data[s_start: s_start + lengths[i]])\n            records.append(Record(s_view, name, desc, qualifiers=[(b'quality', qual)]))\n\n        return RecordBatch.from_aligned_batch(batch, records)\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if the input bytes look like a FASTQ file.\"\"\"\n        return s.startswith(b\"@\")\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastqReader.__init__","title":"<code>__init__(handle, min_seq_length=1, **kwargs)</code>","text":"<p>Initializes the FASTQ reader.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>BinaryIO</code> <p>Binary file handle.</p> required <code>min_seq_length</code> <code>int</code> <p>Minimum sequence length.</p> <code>1</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>baclib/io/seq.py</code> <pre><code>def __init__(self, handle: BinaryIO, min_seq_length: int = 1, **kwargs):\n    \"\"\"Initializes the FASTQ reader.\n\n    Args:\n        handle: Binary file handle.\n        min_seq_length: Minimum sequence length.\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__(handle, **kwargs)\n    self._alphabet = Alphabet.DNA\n    self._min_seq_length = min_seq_length\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastqReader.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over FASTQ records.</p> <p>Yields:</p> Type Description <code>Record</code> <p><code>Record</code> objects.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>def __iter__(self) -&gt; Generator[Record, None, None]:\n    \"\"\"Iterates over FASTQ records.\n\n    Yields:\n        ``Record`` objects.\n    \"\"\"\n    for header, seq_bytes, qual_bytes in self._read_entries():\n        name, _, desc = header.partition(b' ')\n        yield Record(self._alphabet.seq_from(seq_bytes), name, desc, qualifiers=[(b'quality', qual_bytes)])\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastqReader.batches","title":"<code>batches(size=1024)</code>","text":"<p>Reads FASTQ records in batches.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Maximum records per batch.</p> <code>1024</code> <p>Yields:</p> Type Description <p><code>RecordBatch</code> objects.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>def batches(self, size: int = 1024):\n    \"\"\"Reads FASTQ records in batches.\n\n    Args:\n        size: Maximum records per batch.\n\n    Yields:\n        ``RecordBatch`` objects.\n    \"\"\"\n    entries = []\n    for entry in self._read_entries():\n        entries.append(entry)\n        if len(entries) &gt;= size:\n            yield self._make_batch(entries)\n            entries = []\n    if entries:\n        yield self._make_batch(entries)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.FastqReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if the input bytes look like a FASTQ file.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if the input bytes look like a FASTQ file.\"\"\"\n    return s.startswith(b\"@\")\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaReader","title":"<code>GfaReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reader for GFA (Graphical Fragment Assembly) files.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>@SeqFile.register(SeqFileFormat.GFA, extensions=['.gfa'])\nclass GfaReader(BaseReader):\n    \"\"\"\n    Reader for GFA (Graphical Fragment Assembly) files.\n    \"\"\"\n    __slots__ = ('_alphabet', '_min_seq_length')\n    def __init__(self, handle: BinaryIO, min_seq_length: int = 1, **kwargs):\n        \"\"\"Initializes the GFA reader.\n\n        Args:\n            handle: Binary file handle.\n            min_seq_length: Minimum sequence length for segments.\n            **kwargs: Additional arguments.\n        \"\"\"\n        super().__init__(handle, **kwargs)\n        self._alphabet = Alphabet.DNA\n        self._min_seq_length = min_seq_length\n\n    def __iter__(self) -&gt; Generator[Union[Record, Edge], None, None]:\n        \"\"\"Iterates over GFA lines (Segments as Records, Links as Edges).\n\n        Yields:\n            ``Record`` or ``Edge`` objects.\n        \"\"\"\n        # Optimization: Read large binary chunks\n        buf = bytearray()\n        for chunk in self.read_chunks(self._CHUNK_SIZE):\n            if not chunk:\n                if buf and buf.strip():\n                    yield from self._parse_line(bytes(buf))\n                break\n            buf.extend(chunk)\n            pos = 0\n\n            while True:\n                nl_pos = buf.find(b'\\n', pos)\n                if nl_pos == -1:\n                    del buf[:pos]\n                    break\n\n                line = bytes(buf[pos:nl_pos])\n                yield from self._parse_line(line)\n                pos = nl_pos + 1\n\n    def batches(self, size: int = 1024):\n        \"\"\"Yields batches of Records or Edges.\n\n        Args:\n            size: Maximum items per batch.\n\n        Yields:\n            ``RecordBatch`` or ``EdgeBatch`` objects.\n        \"\"\"\n        rec_entries = []\n        edge_u = []\n        edge_v = []\n        edge_u_strands = []\n        edge_v_strands = []\n        edge_attrs = {b'cigar': []}\n\n        buf = bytearray()\n        for chunk in self.read_chunks(self._CHUNK_SIZE):\n            if not chunk:\n                if buf and buf.strip():\n                    self._process_line_batch(bytes(buf), rec_entries, edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n                break\n\n            buf.extend(chunk)\n            pos = 0\n\n            while True:\n                nl_pos = buf.find(b'\\n', pos)\n                if nl_pos == -1:\n                    del buf[:pos]\n                    break\n\n                line = bytes(buf[pos:nl_pos])\n                self._process_line_batch(line, rec_entries, edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n                pos = nl_pos + 1\n\n                if len(rec_entries) &gt;= size:\n                    yield self._make_record_batch(rec_entries)\n                    rec_entries = []                \n                if len(edge_u) &gt;= size:\n                    yield self._make_edge_batch(edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n                    edge_u = []\n                    edge_v = []\n                    edge_u_strands = []\n                    edge_v_strands = []\n                    edge_attrs = {b'cigar': []}\n\n        if rec_entries: yield self._make_record_batch(rec_entries)\n        if edge_u: yield self._make_edge_batch(edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n\n    def _process_line_batch(self, line, rec_entries, edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs):\n        line = line.rstrip()\n        if not line: return\n        first = line[0]\n\n        if first == 83:  # 'S'\n            parts = line.split(b'\\t')\n            if len(parts) &lt; 3: return\n\n            name, seq_bytes = parts[1], parts[2]\n            if seq_bytes == b'*': seq_bytes = b''\n\n            if len(seq_bytes) &gt;= self._min_seq_length:\n                tags = parts[3:] if len(parts) &gt; 3 else []\n                rec_entries.append((name, seq_bytes, tags))\n                pass\n        elif first == 76:  # 'L'\n            parts = line.split(b'\\t')\n            if len(parts) &lt; 6: return\n            # L name1 ori1 name2 ori2 cigar\n            edge_u.append(parts[1])\n            edge_u_strands.append(Strand.from_symbol(parts[2]))\n            edge_v.append(parts[3])\n            edge_v_strands.append(Strand.from_symbol(parts[4]))\n            edge_attrs[b'cigar'].append(parts[5])\n\n    def _make_record_batch(self, entries):\n        batch = self._build_seq_batch([e[1] for e in entries], self._alphabet)\n        encoded_data, starts, lengths = batch.arrays\n        parse_tags = Qualifier.parse_tags\n        records = []\n        for i, (name, _, tags) in enumerate(entries):\n            s_start = starts[i]\n            s_view = self._alphabet.seq_from(encoded_data[s_start: s_start + lengths[i]])\n            quals = parse_tags(tags) if tags else []\n            records.append(Record(s_view, name, qualifiers=quals))\n\n        return RecordBatch.from_aligned_batch(batch, records)\n\n    @staticmethod\n    def _make_edge_batch(u, v, u_strands, v_strands, attrs):\n        u_arr = np.array(u, dtype=object)\n        v_arr = np.array(v, dtype=object)\n        us_arr = np.array(u_strands, dtype=np.int8)\n        vs_arr = np.array(v_strands, dtype=np.int8)\n        attr_arrays = {k: np.array(val, dtype=object) for k, val in attrs.items()}\n        return EdgeBatch(u_arr, v_arr, us_arr, vs_arr, attr_arrays)\n\n    def _parse_line(self, line: bytes):\n        line = line.rstrip()\n        if not line: return\n        first = line[0]\n\n        if first == 83:  # 'S'\n            parts = line.split(b'\\t')\n            if len(parts) &lt; 3: return\n\n            name, seq_bytes = parts[1], parts[2]\n\n            if len(seq_bytes) &gt;= self._min_seq_length:\n                seq = self._alphabet.seq_from(seq_bytes if seq_bytes != b'*' else b'')\n                quals = Qualifier.parse_tags(parts[3:]) if len(parts) &gt; 3 else []\n                quals = Qualifier.parse_tags(parts[3:]) if len(parts) &gt; 3 else []\n                yield Record(seq, name, qualifiers=quals)\n\n        elif first == 76:  # 'L'\n            parts = line.split(b'\\t')\n            if len(parts) &lt; 6: return\n            yield Edge(parts[1], parts[3], \n                       Strand.from_symbol(parts[2]), \n                       Strand.from_symbol(parts[4]), \n                       {b'cigar': parts[5]})\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if the input bytes look like a GFA file.\"\"\"\n        return s.startswith(b'H\\t') or s.startswith(b'S\\t')\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaReader.__init__","title":"<code>__init__(handle, min_seq_length=1, **kwargs)</code>","text":"<p>Initializes the GFA reader.</p> <p>Parameters:</p> Name Type Description Default <code>handle</code> <code>BinaryIO</code> <p>Binary file handle.</p> required <code>min_seq_length</code> <code>int</code> <p>Minimum sequence length for segments.</p> <code>1</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>baclib/io/seq.py</code> <pre><code>def __init__(self, handle: BinaryIO, min_seq_length: int = 1, **kwargs):\n    \"\"\"Initializes the GFA reader.\n\n    Args:\n        handle: Binary file handle.\n        min_seq_length: Minimum sequence length for segments.\n        **kwargs: Additional arguments.\n    \"\"\"\n    super().__init__(handle, **kwargs)\n    self._alphabet = Alphabet.DNA\n    self._min_seq_length = min_seq_length\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaReader.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over GFA lines (Segments as Records, Links as Edges).</p> <p>Yields:</p> Type Description <code>Union[Record, Edge]</code> <p><code>Record</code> or <code>Edge</code> objects.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>def __iter__(self) -&gt; Generator[Union[Record, Edge], None, None]:\n    \"\"\"Iterates over GFA lines (Segments as Records, Links as Edges).\n\n    Yields:\n        ``Record`` or ``Edge`` objects.\n    \"\"\"\n    # Optimization: Read large binary chunks\n    buf = bytearray()\n    for chunk in self.read_chunks(self._CHUNK_SIZE):\n        if not chunk:\n            if buf and buf.strip():\n                yield from self._parse_line(bytes(buf))\n            break\n        buf.extend(chunk)\n        pos = 0\n\n        while True:\n            nl_pos = buf.find(b'\\n', pos)\n            if nl_pos == -1:\n                del buf[:pos]\n                break\n\n            line = bytes(buf[pos:nl_pos])\n            yield from self._parse_line(line)\n            pos = nl_pos + 1\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaReader.batches","title":"<code>batches(size=1024)</code>","text":"<p>Yields batches of Records or Edges.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Maximum items per batch.</p> <code>1024</code> <p>Yields:</p> Type Description <p><code>RecordBatch</code> or <code>EdgeBatch</code> objects.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>def batches(self, size: int = 1024):\n    \"\"\"Yields batches of Records or Edges.\n\n    Args:\n        size: Maximum items per batch.\n\n    Yields:\n        ``RecordBatch`` or ``EdgeBatch`` objects.\n    \"\"\"\n    rec_entries = []\n    edge_u = []\n    edge_v = []\n    edge_u_strands = []\n    edge_v_strands = []\n    edge_attrs = {b'cigar': []}\n\n    buf = bytearray()\n    for chunk in self.read_chunks(self._CHUNK_SIZE):\n        if not chunk:\n            if buf and buf.strip():\n                self._process_line_batch(bytes(buf), rec_entries, edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n            break\n\n        buf.extend(chunk)\n        pos = 0\n\n        while True:\n            nl_pos = buf.find(b'\\n', pos)\n            if nl_pos == -1:\n                del buf[:pos]\n                break\n\n            line = bytes(buf[pos:nl_pos])\n            self._process_line_batch(line, rec_entries, edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n            pos = nl_pos + 1\n\n            if len(rec_entries) &gt;= size:\n                yield self._make_record_batch(rec_entries)\n                rec_entries = []                \n            if len(edge_u) &gt;= size:\n                yield self._make_edge_batch(edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n                edge_u = []\n                edge_v = []\n                edge_u_strands = []\n                edge_v_strands = []\n                edge_attrs = {b'cigar': []}\n\n    if rec_entries: yield self._make_record_batch(rec_entries)\n    if edge_u: yield self._make_edge_batch(edge_u, edge_v, edge_u_strands, edge_v_strands, edge_attrs)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if the input bytes look like a GFA file.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if the input bytes look like a GFA file.\"\"\"\n    return s.startswith(b'H\\t') or s.startswith(b'S\\t')\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaWriter","title":"<code>GfaWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for GFA format files.</p> Source code in <code>baclib/io/seq.py</code> <pre><code>@SeqFile.register(SeqFileFormat.GFA)\nclass GfaWriter(BaseWriter):\n    \"\"\"\n    Writer for GFA format files.\n    \"\"\"\n    __slots__ = ()\n    def write_one(self, item: Union[Record, Edge]):\n        \"\"\"\n        Writes a Record (Segment) or Edge (Link).\n\n        Args:\n            item: Record or Edge object.\n        \"\"\"\n        if isinstance(item, Record):\n            # S name seq\n            self._handle.write(b\"S\\t\" + item.id + b\"\\t\" + bytes(item.seq) + b\"\\n\")\n        elif isinstance(item, Edge):\n            cigar = item.attributes.get(b'cigar', b'0M')\n            us = item.u_strand.bytes\n            vs = item.v_strand.bytes\n            self._handle.write(b\"L\\t\" + item.u + b\"\\t\" + us + b\"\\t\" +\n                               item.v + b\"\\t\" + vs + b\"\\t\" + cigar + b\"\\n\")\n\n    def write_batch(self, batch: Batch):\n        \"\"\"Writes a batch of Records or Edges efficiently.\n\n        Args:\n            batch: ``RecordBatch`` or ``EdgeBatch``.\n        \"\"\"\n        if isinstance(batch, EdgeBatch):\n            # Vectorized write for Edges\n            u = batch.u\n            v = batch.v\n            u_strands = batch.u_strands\n            v_strands = batch.v_strands\n\n            # Extract columns or use defaults\n            cigars = batch.attributes.get(b'cigar')\n\n            lines = []\n            for i in range(len(batch)):\n                us = Strand(u_strands[i]).bytes\n                vs = Strand(v_strands[i]).bytes\n                cg = cigars[i] if cigars is not None else b'0M'\n\n                lines.append(b\"L\\t\" + u[i] + b\"\\t\" + us + b\"\\t\" + v[i] + b\"\\t\" + vs + b\"\\t\" + cg + b\"\\n\")\n\n                if len(lines) &gt;= 1000:\n                    self._handle.write(b\"\".join(lines))\n                    lines = []\n            if lines:\n                self._handle.write(b\"\".join(lines))\n\n        elif isinstance(batch, RecordBatch):\n            # Vectorized write for Records (Segments)\n            ids = batch.ids\n            seqs = batch.seqs\n            decoded = seqs.alphabet.decode(seqs.encoded)\n            starts = seqs.starts\n            lengths = seqs.lengths\n\n            lines = []\n            for i in range(len(batch)):\n                s = starts[i]\n                l = lengths[i]\n                lines.append(b\"S\\t\" + ids[i] + b\"\\t\" + decoded[s : s+l] + b\"\\n\")\n\n                if len(lines) &gt;= 1000:\n                    self._handle.write(b\"\".join(lines))\n                    lines = []\n            if lines:\n                self._handle.write(b\"\".join(lines))\n        else:\n            super().write_batch(batch)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaWriter.write_batch","title":"<code>write_batch(batch)</code>","text":"<p>Writes a batch of Records or Edges efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>Batch</code> <p><code>RecordBatch</code> or <code>EdgeBatch</code>.</p> required Source code in <code>baclib/io/seq.py</code> <pre><code>def write_batch(self, batch: Batch):\n    \"\"\"Writes a batch of Records or Edges efficiently.\n\n    Args:\n        batch: ``RecordBatch`` or ``EdgeBatch``.\n    \"\"\"\n    if isinstance(batch, EdgeBatch):\n        # Vectorized write for Edges\n        u = batch.u\n        v = batch.v\n        u_strands = batch.u_strands\n        v_strands = batch.v_strands\n\n        # Extract columns or use defaults\n        cigars = batch.attributes.get(b'cigar')\n\n        lines = []\n        for i in range(len(batch)):\n            us = Strand(u_strands[i]).bytes\n            vs = Strand(v_strands[i]).bytes\n            cg = cigars[i] if cigars is not None else b'0M'\n\n            lines.append(b\"L\\t\" + u[i] + b\"\\t\" + us + b\"\\t\" + v[i] + b\"\\t\" + vs + b\"\\t\" + cg + b\"\\n\")\n\n            if len(lines) &gt;= 1000:\n                self._handle.write(b\"\".join(lines))\n                lines = []\n        if lines:\n            self._handle.write(b\"\".join(lines))\n\n    elif isinstance(batch, RecordBatch):\n        # Vectorized write for Records (Segments)\n        ids = batch.ids\n        seqs = batch.seqs\n        decoded = seqs.alphabet.decode(seqs.encoded)\n        starts = seqs.starts\n        lengths = seqs.lengths\n\n        lines = []\n        for i in range(len(batch)):\n            s = starts[i]\n            l = lengths[i]\n            lines.append(b\"S\\t\" + ids[i] + b\"\\t\" + decoded[s : s+l] + b\"\\n\")\n\n            if len(lines) &gt;= 1000:\n                self._handle.write(b\"\".join(lines))\n                lines = []\n        if lines:\n            self._handle.write(b\"\".join(lines))\n    else:\n        super().write_batch(batch)\n</code></pre>"},{"location":"reference/baclib/io/seq/#baclib.io.seq.GfaWriter.write_one","title":"<code>write_one(item)</code>","text":"<p>Writes a Record (Segment) or Edge (Link).</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[Record, Edge]</code> <p>Record or Edge object.</p> required Source code in <code>baclib/io/seq.py</code> <pre><code>def write_one(self, item: Union[Record, Edge]):\n    \"\"\"\n    Writes a Record (Segment) or Edge (Link).\n\n    Args:\n        item: Record or Edge object.\n    \"\"\"\n    if isinstance(item, Record):\n        # S name seq\n        self._handle.write(b\"S\\t\" + item.id + b\"\\t\" + bytes(item.seq) + b\"\\n\")\n    elif isinstance(item, Edge):\n        cigar = item.attributes.get(b'cigar', b'0M')\n        us = item.u_strand.bytes\n        vs = item.v_strand.bytes\n        self._handle.write(b\"L\\t\" + item.u + b\"\\t\" + us + b\"\\t\" +\n                           item.v + b\"\\t\" + vs + b\"\\t\" + cigar + b\"\\n\")\n</code></pre>"},{"location":"reference/baclib/io/tabular/","title":"tabular","text":""},{"location":"reference/baclib/io/tabular/#baclib.io.tabular","title":"<code>baclib.io.tabular</code>","text":"<p>Readers and writers for tabular bioinformatics formats: BED, GFF3, PAF, and VCF.</p>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.BedReader","title":"<code>BedReader</code>","text":"<p>               Bases: <code>TabularReader</code></p> <p>Reader for BED format files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with open(\"features.bed\", \"rb\") as f:\n...     reader = BedReader(f)\n...     for feature in reader:\n...         print(feature.key)\n</code></pre> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.BED, extensions=['.bed'])\nclass BedReader(TabularReader):\n    \"\"\"\n    Reader for BED format files.\n\n    Examples:\n        &gt;&gt;&gt; with open(\"features.bed\", \"rb\") as f:\n        ...     reader = BedReader(f)\n        ...     for feature in reader:\n        ...         print(feature.key)\n    \"\"\"\n    _min_cols = 3\n\n    def parse_row(self, parts: list[bytes]) -&gt; Feature:\n        \"\"\"\n        Parses a BED row.\n\n        Args:\n            parts: List of column strings.\n\n        Returns:\n            A Feature object.\n        \"\"\"\n        start, end = int(parts[1]), int(parts[2])\n        n_cols = len(parts)\n        kind = parts[3] if n_cols &gt; 3 else b'feature'\n        score = float(parts[4]) if n_cols &gt; 4 and parts[4] != b'.' else 0.0\n        strand = parts[5] if n_cols &gt; 5 else b'.'\n        quals = [(b'source', parts[0])]\n        if score: quals.append((b'score', score))\n        if n_cols &gt; 9: quals.append((b'blocks', b','.join(parts[9:])))\n        return Feature(Interval(start, end, strand), kind, qualifiers=quals)\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if input bytes look like BED format.\"\"\"\n        try:\n            for line in s.splitlines():\n                if not line.strip() or line.startswith((b'track', b'browser', b'#')): continue\n                parts = line.split(b'\\t')\n                return len(parts) &gt;= 3 and parts[1].isdigit() and parts[2].isdigit()\n            return False\n        except Exception: return False\n\n    def _make_batch(self, items: list) -&gt; FeatureBatch:\n        return FeatureBatch.from_features(items)\n\n    def _make_batch_from_parts(self, parts_list: list[list[bytes]]) -&gt; FeatureBatch:\n        n = len(parts_list)\n        if n == 0: \n            return FeatureBatch.empty()\n\n        # Vectorized parsing using list comprehensions (faster than loop assignment)\n        starts = np.array([int(p[1]) for p in parts_list], dtype=np.int32)\n        ends = np.array([int(p[2]) for p in parts_list], dtype=np.int32)\n\n        from_bytes = FeatureKey.from_bytes\n        keys = np.array([from_bytes(p[3] if len(p) &gt; 3 else b'feature').value for p in parts_list], dtype=np.int16)\n\n        # Strands\n        def _get_strand(p):\n            if len(p) &gt; 5:\n                s = p[5]\n                if s == b'+': return 1\n                if s == b'-': return -1\n            return 0\n        strands = np.array([_get_strand(p) for p in parts_list], dtype=np.int32)\n\n        qualifiers_collection = []\n        for parts in parts_list:\n            quals = [(b'source', parts[0])]\n            n_cols = len(parts)\n            score = float(parts[4]) if n_cols &gt; 4 and parts[4] != b'.' else 0.0\n            if score: quals.append((b'score', score))\n            if n_cols &gt; 9: quals.append((b'blocks', b','.join(parts[9:])))\n            qualifiers_collection.append(quals)\n\n        intervals = IntervalBatch(starts, ends, strands, sort=False)\n        qualifiers = QualifierBatch.build(qualifiers_collection)\n\n        return FeatureBatch(intervals, keys, qualifiers)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.BedReader.parse_row","title":"<code>parse_row(parts)</code>","text":"<p>Parses a BED row.</p> <p>Parameters:</p> Name Type Description Default <code>parts</code> <code>list[bytes]</code> <p>List of column strings.</p> required <p>Returns:</p> Type Description <code>Feature</code> <p>A Feature object.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def parse_row(self, parts: list[bytes]) -&gt; Feature:\n    \"\"\"\n    Parses a BED row.\n\n    Args:\n        parts: List of column strings.\n\n    Returns:\n        A Feature object.\n    \"\"\"\n    start, end = int(parts[1]), int(parts[2])\n    n_cols = len(parts)\n    kind = parts[3] if n_cols &gt; 3 else b'feature'\n    score = float(parts[4]) if n_cols &gt; 4 and parts[4] != b'.' else 0.0\n    strand = parts[5] if n_cols &gt; 5 else b'.'\n    quals = [(b'source', parts[0])]\n    if score: quals.append((b'score', score))\n    if n_cols &gt; 9: quals.append((b'blocks', b','.join(parts[9:])))\n    return Feature(Interval(start, end, strand), kind, qualifiers=quals)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.BedReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if input bytes look like BED format.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if input bytes look like BED format.\"\"\"\n    try:\n        for line in s.splitlines():\n            if not line.strip() or line.startswith((b'track', b'browser', b'#')): continue\n            parts = line.split(b'\\t')\n            return len(parts) &gt;= 3 and parts[1].isdigit() and parts[2].isdigit()\n        return False\n    except Exception: return False\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.BedWriter","title":"<code>BedWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for BED format files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with BedWriter(\"output.bed\") as w:\n...     w.write_one(record)\n</code></pre> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.BED)\nclass BedWriter(BaseWriter):\n    \"\"\"\n    Writer for BED format files.\n\n    Examples:\n        &gt;&gt;&gt; with BedWriter(\"output.bed\") as w:\n        ...     w.write_one(record)\n    \"\"\"\n\n    def write_one(self, record: Record):\n        \"\"\"\n        Writes features of a Record in BED format.\n\n        Args:\n            record: The Record object.\n        \"\"\"\n        if not isinstance(record, Record): raise TypeError(\"BedWriter expects Record objects\")\n        for feature in record.features:\n            # Resolve Name/ID/Gene/Key for BED column 4 (Name)\n            name = feature.get(b'Name', feature.get(b'ID', feature.get(\n                b'gene', feature.key.bytes if isinstance(feature.key, FeatureKey) else feature.key)))\n            line = b\"\\t\".join([\n                record.id,\n                str(feature.interval.start).encode('ascii'),\n                str(feature.interval.end).encode('ascii'),\n                name,\n                str(feature.get(b'score', 0)).encode('ascii'), feature.interval.strand.bytes\n            ]) + b\"\\n\"\n            self._handle.write(line)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.BedWriter.write_one","title":"<code>write_one(record)</code>","text":"<p>Writes features of a Record in BED format.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>The Record object.</p> required Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_one(self, record: Record):\n    \"\"\"\n    Writes features of a Record in BED format.\n\n    Args:\n        record: The Record object.\n    \"\"\"\n    if not isinstance(record, Record): raise TypeError(\"BedWriter expects Record objects\")\n    for feature in record.features:\n        # Resolve Name/ID/Gene/Key for BED column 4 (Name)\n        name = feature.get(b'Name', feature.get(b'ID', feature.get(\n            b'gene', feature.key.bytes if isinstance(feature.key, FeatureKey) else feature.key)))\n        line = b\"\\t\".join([\n            record.id,\n            str(feature.interval.start).encode('ascii'),\n            str(feature.interval.end).encode('ascii'),\n            name,\n            str(feature.get(b'score', 0)).encode('ascii'), feature.interval.strand.bytes\n        ]) + b\"\\n\"\n        self._handle.write(line)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.GffReader","title":"<code>GffReader</code>","text":"<p>               Bases: <code>TabularReader</code></p> <p>Reader for GFF3 format files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with open(\"features.gff\", \"rb\") as f:\n...     reader = GffReader(f)\n...     for feature in reader:\n...         print(feature.key)\n</code></pre> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.GFF, extensions=['.gff', '.gff3'])\nclass GffReader(TabularReader):\n    \"\"\"\n    Reader for GFF3 format files.\n\n    Examples:\n        &gt;&gt;&gt; with open(\"features.gff\", \"rb\") as f:\n        ...     reader = GffReader(f)\n        ...     for feature in reader:\n        ...         print(feature.key)\n    \"\"\"\n    _min_cols = 9\n\n    def parse_row(self, parts: list[bytes]) -&gt; Feature:\n        \"\"\"\n        Parses a GFF3 row.\n\n        Args:\n            parts: List of column strings.\n\n        Returns:\n            A Feature object.\n        \"\"\"\n        start, end = int(parts[3]) - 1, int(parts[4])\n        quals = Qualifier.parse_gff_attributes(parts[8])\n        quals.append((b'source', parts[0]))\n        if parts[1] != b'.': quals.append((b'tool', parts[1]))\n        if parts[5] != b'.': quals.append((b'score', float(parts[5])))\n        if parts[7] != b'.': quals.append((b'phase', int(parts[7])))\n        return Feature(Interval(start, end, parts[6]), parts[2], qualifiers=quals)\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if input bytes look like GFF3 format.\"\"\"\n        return s.startswith(b'##gff')\n\n    def _make_batch(self, items: list) -&gt; FeatureBatch:\n        return FeatureBatch.from_features(items)\n\n    def _make_batch_from_parts(self, parts_list: list[list[bytes]]) -&gt; FeatureBatch:\n        n = len(parts_list)\n        if n == 0: \n            return FeatureBatch.empty()\n\n        # GFF is 1-based inclusive -&gt; 0-based half-open\n        starts = np.array([int(p[3]) - 1 for p in parts_list], dtype=np.int32)\n        ends = np.array([int(p[4]) for p in parts_list], dtype=np.int32)\n\n        from_bytes = FeatureKey.from_bytes\n        keys = np.array([from_bytes(p[2]).value for p in parts_list], dtype=np.int16)\n\n        # Strands\n        def _get_strand(p):\n            s = p[6]\n            if s == b'+': return 1\n            if s == b'-': return -1\n            return 0\n        strands = np.array([_get_strand(p) for p in parts_list], dtype=np.int32)\n\n        parse_attrs = Qualifier.parse_gff_attributes\n        qualifiers_collection = []\n        for parts in parts_list:\n            quals = parse_attrs(parts[8])\n            quals.append((b'source', parts[0]))\n            if parts[1] != b'.': quals.append((b'tool', parts[1]))\n            if parts[5] != b'.': quals.append((b'score', float(parts[5])))\n            if parts[7] != b'.': quals.append((b'phase', int(parts[7])))\n            qualifiers_collection.append(quals)\n\n        intervals = IntervalBatch(starts, ends, strands, sort=False)\n        qualifiers = QualifierBatch.build(qualifiers_collection)\n\n        return FeatureBatch(intervals, keys, qualifiers)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.GffReader.parse_row","title":"<code>parse_row(parts)</code>","text":"<p>Parses a GFF3 row.</p> <p>Parameters:</p> Name Type Description Default <code>parts</code> <code>list[bytes]</code> <p>List of column strings.</p> required <p>Returns:</p> Type Description <code>Feature</code> <p>A Feature object.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def parse_row(self, parts: list[bytes]) -&gt; Feature:\n    \"\"\"\n    Parses a GFF3 row.\n\n    Args:\n        parts: List of column strings.\n\n    Returns:\n        A Feature object.\n    \"\"\"\n    start, end = int(parts[3]) - 1, int(parts[4])\n    quals = Qualifier.parse_gff_attributes(parts[8])\n    quals.append((b'source', parts[0]))\n    if parts[1] != b'.': quals.append((b'tool', parts[1]))\n    if parts[5] != b'.': quals.append((b'score', float(parts[5])))\n    if parts[7] != b'.': quals.append((b'phase', int(parts[7])))\n    return Feature(Interval(start, end, parts[6]), parts[2], qualifiers=quals)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.GffReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if input bytes look like GFF3 format.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if input bytes look like GFF3 format.\"\"\"\n    return s.startswith(b'##gff')\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.GffWriter","title":"<code>GffWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for GFF3 format files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with GffWriter(\"output.gff\") as w:\n...     w.write_header()\n...     w.write_one(record)\n</code></pre> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.GFF)\nclass GffWriter(BaseWriter):\n    \"\"\"\n    Writer for GFF3 format files.\n\n    Examples:\n        &gt;&gt;&gt; with GffWriter(\"output.gff\") as w:\n        ...     w.write_header()\n        ...     w.write_one(record)\n    \"\"\"\n\n    def write_header(self):\n        \"\"\"Writes the GFF3 header.\"\"\"\n        self._handle.write(b\"##gff-version 3\\n\")\n\n    def write_one(self, record: Record):\n        \"\"\"\n        Writes a Record and its features in GFF3 format.\n\n        Args:\n            record: The Record object.\n        \"\"\"\n        if not isinstance(record, Record): raise TypeError(\"GffWriter expects Record objects\")\n        self._handle.write(b\"##sequence-region \" + record.id + b\" 1 %b\" % len(record) + b\"\\n\")\n        for feature in record.features: self._write_feature(record.id, feature)\n\n    def _write_feature(self, seq_id: bytes, feature: Feature):\n        \"\"\"\n        Writes a single feature.\n\n        Args:\n            seq_id: ID of the sequence containing the feature.\n            feature: The Feature object.\n        \"\"\"\n        source = feature.get(b'source', b'baclib')\n        start = feature.interval.start + 1\n        end = feature.interval.end\n        score = feature.get(b'score', b'.')\n\n        strand = feature.interval.strand.bytes\n\n        phase = feature.get(b'phase', b'.')\n        kind_bytes = feature.key.bytes if isinstance(feature.key, FeatureKey) else feature.key\n\n        attr_strings = []\n        if val := feature.get(b'ID'): attr_strings.append(b\"ID=\" + val)\n        if val := feature.get(b'Name'): attr_strings.append(b\"Name=\" + val)\n        for key, value in feature.qualifiers:\n            if key in {b'source', b'score', b'phase', b'ID', b'Name'}: continue\n            safe_val = val.replace(b';', b'%3B').replace(b'=', b'%3D').replace(b'&amp;', b'%26')\n            attr_strings.append(key + b\"=\" + safe_val)\n        attr_block = b\";\".join(attr_strings) if attr_strings else b\".\"\n\n        self._handle.write(\n            b\"\\t\".join([seq_id, source, kind_bytes, str(start).encode('ascii'), str(end).encode('ascii'), str(score).encode('ascii'), strand, str(phase).encode('ascii'), attr_block]) + b\"\\n\")\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.GffWriter.write_header","title":"<code>write_header()</code>","text":"<p>Writes the GFF3 header.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_header(self):\n    \"\"\"Writes the GFF3 header.\"\"\"\n    self._handle.write(b\"##gff-version 3\\n\")\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.GffWriter.write_one","title":"<code>write_one(record)</code>","text":"<p>Writes a Record and its features in GFF3 format.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Record</code> <p>The Record object.</p> required Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_one(self, record: Record):\n    \"\"\"\n    Writes a Record and its features in GFF3 format.\n\n    Args:\n        record: The Record object.\n    \"\"\"\n    if not isinstance(record, Record): raise TypeError(\"GffWriter expects Record objects\")\n    self._handle.write(b\"##sequence-region \" + record.id + b\" 1 %b\" % len(record) + b\"\\n\")\n    for feature in record.features: self._write_feature(record.id, feature)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.PafReader","title":"<code>PafReader</code>","text":"<p>               Bases: <code>TabularReader</code></p> <p>Reader for PAF (Pairwise mApping Format) files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with open(\"alignments.paf\", \"rb\") as f:\n...     reader = PafReader(f)\n...     for aln in reader:\n...         print(aln.score)\n</code></pre> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.PAF, extensions=['.paf'])\nclass PafReader(TabularReader):\n    \"\"\"\n    Reader for PAF (Pairwise mApping Format) files.\n\n    Examples:\n        &gt;&gt;&gt; with open(\"alignments.paf\", \"rb\") as f:\n        ...     reader = PafReader(f)\n        ...     for aln in reader:\n        ...         print(aln.score)\n    \"\"\"\n    _min_cols = 12\n\n    def parse_row(self, parts: list[bytes]) -&gt; Alignment:\n        \"\"\"\n        Parses a PAF row.\n\n        Args:\n            parts: List of column strings.\n\n        Returns:\n            An Alignment object.\n        \"\"\"\n        q_len, t_len, block_len = int(parts[1]), int(parts[6]), int(parts[10])\n        n_matches = int(parts[9])\n        cigar, score, quals = None, None, []\n        for key, value in Qualifier.parse_tags(parts[12:]):\n            if key == b'cg': cigar = value\n            elif key == b'AS': score = value\n            else: quals.append((key, value))\n\n        return Alignment(\n            query=parts[0], query_interval=Interval(int(parts[2]), int(parts[3]), 1),\n            target=parts[5], interval=Interval(int(parts[7]), int(parts[8]), parts[4].decode('ascii')),\n            query_length=q_len, target_length=t_len, length=block_len, score=score,\n            cigar=cigar, n_matches=n_matches, quality=int(parts[11]), qualifiers=quals\n        )\n\n    def _make_batch(self, items: list) -&gt; AlignmentBatch:\n        return AlignmentBatch.from_alignments(items)\n\n    def _make_batch_from_parts(self, parts_list: list[list[bytes]]) -&gt; AlignmentBatch:\n        n = len(parts_list)\n        if n == 0: return AlignmentBatch.empty()\n\n        # PAF cols: \n        # 0:q, 1:q_len, 2:q_s, 3:q_e, 4:strand, 5:t, 6:t_len, 7:t_s, 8:t_e, 9:match, 10:aln_len, 11:qual\n\n        # 1. IDs\n        q_names = [p[0] for p in parts_list]\n        t_names = [p[5] for p in parts_list]\n        q_ids, q_indices = np.unique(q_names, return_inverse=True)\n        t_ids, t_indices = np.unique(t_names, return_inverse=True)\n\n        # 2. Numeric Columns\n        # Use list comprehensions for speed on small batches\n        q_lens = np.array([int(p[1]) for p in parts_list], dtype=np.int32)\n        q_starts = np.array([int(p[2]) for p in parts_list], dtype=np.int32)\n        q_ends = np.array([int(p[3]) for p in parts_list], dtype=np.int32)\n\n        t_lens = np.array([int(p[6]) for p in parts_list], dtype=np.int32)\n        t_starts = np.array([int(p[7]) for p in parts_list], dtype=np.int32)\n        t_ends = np.array([int(p[8]) for p in parts_list], dtype=np.int32)\n\n        matches = np.array([int(p[9]) for p in parts_list], dtype=np.int32)\n        aln_lens = np.array([int(p[10]) for p in parts_list], dtype=np.int32)\n        qualities = np.array([int(p[11]) for p in parts_list], dtype=np.uint8)\n\n        # Strands: PAF uses '+' and '-'\n        strands = np.array([1 if p[4] == b'+' else -1 for p in parts_list], dtype=np.int8)\n\n        # 3. Tags (Score, Cigar)\n        scores = np.zeros(n, dtype=np.float32)\n        cigars = np.full(n, None, dtype=object)\n        quals = np.full(n, None, dtype=object)\n\n        parse_tags = Qualifier.parse_tags\n        for i, parts in enumerate(parts_list):\n            if len(parts) &gt; 12:\n                row_quals = []\n                for key, value in parse_tags(parts[12:]):\n                    if key == b'cg': cigars[i] = value\n                    elif key == b'AS': scores[i] = value\n                    else: row_quals.append((key, value))\n                if row_quals: quals[i] = row_quals\n\n        # 4. Construct Batch\n        data = np.zeros(n, dtype=AlignmentBatch._DTYPE)\n        data['q_idx'] = q_indices; data['t_idx'] = t_indices; data['score'] = scores\n        data['q_start'] = q_starts; data['q_end'] = q_ends; data['q_len'] = q_lens; data['q_strand'] = 1\n        data['t_start'] = t_starts; data['t_end'] = t_ends; data['t_len'] = t_lens; data['t_strand'] = strands\n        data['matches'] = matches; data['quality'] = qualities; data['aln_len'] = aln_lens\n\n        return AlignmentBatch(data=data, cigars=cigars, qualifiers=quals, query_ids=q_ids, target_ids=t_ids)\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if input bytes look like PAF format.\"\"\"\n        try:\n            line = s.split(b'\\n', 1)[0]\n            parts = line.split(b'\\t')\n            return (len(parts) &gt;= 12 and parts[1].isdigit() and parts[2].isdigit() and parts[3].isdigit() and\n                    parts[6].isdigit() and parts[7].isdigit() and parts[8].isdigit())\n        except Exception:\n            return False\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.PafReader.parse_row","title":"<code>parse_row(parts)</code>","text":"<p>Parses a PAF row.</p> <p>Parameters:</p> Name Type Description Default <code>parts</code> <code>list[bytes]</code> <p>List of column strings.</p> required <p>Returns:</p> Type Description <code>Alignment</code> <p>An Alignment object.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def parse_row(self, parts: list[bytes]) -&gt; Alignment:\n    \"\"\"\n    Parses a PAF row.\n\n    Args:\n        parts: List of column strings.\n\n    Returns:\n        An Alignment object.\n    \"\"\"\n    q_len, t_len, block_len = int(parts[1]), int(parts[6]), int(parts[10])\n    n_matches = int(parts[9])\n    cigar, score, quals = None, None, []\n    for key, value in Qualifier.parse_tags(parts[12:]):\n        if key == b'cg': cigar = value\n        elif key == b'AS': score = value\n        else: quals.append((key, value))\n\n    return Alignment(\n        query=parts[0], query_interval=Interval(int(parts[2]), int(parts[3]), 1),\n        target=parts[5], interval=Interval(int(parts[7]), int(parts[8]), parts[4].decode('ascii')),\n        query_length=q_len, target_length=t_len, length=block_len, score=score,\n        cigar=cigar, n_matches=n_matches, quality=int(parts[11]), qualifiers=quals\n    )\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.PafReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if input bytes look like PAF format.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if input bytes look like PAF format.\"\"\"\n    try:\n        line = s.split(b'\\n', 1)[0]\n        parts = line.split(b'\\t')\n        return (len(parts) &gt;= 12 and parts[1].isdigit() and parts[2].isdigit() and parts[3].isdigit() and\n                parts[6].isdigit() and parts[7].isdigit() and parts[8].isdigit())\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.PafWriter","title":"<code>PafWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for PAF (Pairwise mApping Format) files.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; with PafWriter(\"alignments.paf\") as w:\n...     w.write(alignment_batch)\n</code></pre> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.PAF)\nclass PafWriter(BaseWriter):\n    \"\"\"\n    Writer for PAF (Pairwise mApping Format) files.\n\n    Examples:\n        &gt;&gt;&gt; with PafWriter(\"alignments.paf\") as w:\n        ...     w.write(alignment_batch)\n    \"\"\"\n    def write_one(self, item: Union[Alignment, AlignmentBatch]):\n        \"\"\"Writes a single Alignment or a Batch.\n\n        Args:\n            item: ``Alignment`` or ``AlignmentBatch``.\n        \"\"\"\n        if isinstance(item, AlignmentBatch):\n            self.write_batch(item)\n        elif isinstance(item, Alignment):\n            self._write_alignment(item)\n        else:\n            raise TypeError(f\"PafWriter expects Alignment or AlignmentBatch objects, got {type(item)}\")\n\n    def _write_alignment(self, aln: Alignment):\n        q_name = aln.query\n        if isinstance(q_name, (int, np.integer)): q_name = str(q_name).encode('ascii')\n        elif isinstance(q_name, str): q_name = q_name.encode('ascii')\n\n        t_name = aln.target\n        if isinstance(t_name, (int, np.integer)): t_name = str(t_name).encode('ascii')\n        elif isinstance(t_name, str): t_name = t_name.encode('ascii')\n\n        strand = b'+' if aln.query_interval.strand == aln.interval.strand else b'-'\n\n        parts = [\n            q_name,\n            b\"%d\" % aln.query_length,\n            b\"%d\" % aln.query_interval.start,\n            b\"%d\" % aln.query_interval.end,\n            strand,\n            t_name,\n            b\"%d\" % aln.target_length,\n            b\"%d\" % aln.interval.start,\n            b\"%d\" % aln.interval.end,\n            b\"%d\" % aln.n_matches,\n            b\"%d\" % aln.length,\n            b\"%d\" % aln.quality\n        ]\n\n        self._append_tags(parts, aln.score, aln.cigar, aln.qualifiers)\n        self._handle.write(b\"\\t\".join(parts) + b\"\\n\")\n\n    def write_batch(self, batch: AlignmentBatch):\n        \"\"\"Writes an AlignmentBatch efficiently.\n\n        Args:\n           batch: ``AlignmentBatch`` to write.\n        \"\"\"\n        n = len(batch)\n        if n == 0: return\n\n        q_ids = batch.query.ids\n        t_ids = batch.target.ids\n        is_same_strand = (batch.q_strands == batch.t_strands)\n        qualities = batch._data['quality']\n\n        lines = []\n        for i in range(n):\n            q_name = q_ids[i]\n            if isinstance(q_name, str): q_name = q_name.encode('ascii')\n            elif isinstance(q_name, (int, np.integer)): q_name = str(q_name).encode('ascii')\n\n            t_name = t_ids[i]\n            if isinstance(t_name, str): t_name = t_name.encode('ascii')\n            elif isinstance(t_name, (int, np.integer)): t_name = str(t_name).encode('ascii')\n\n            parts = [\n                q_name,\n                b\"%d\" % batch.q_lens[i],\n                b\"%d\" % batch.q_starts[i],\n                b\"%d\" % batch.q_ends[i],\n                b'+' if is_same_strand[i] else b'-',\n                t_name,\n                b\"%d\" % batch.t_lens[i],\n                b\"%d\" % batch.t_starts[i],\n                b\"%d\" % batch.t_ends[i],\n                b\"%d\" % batch.matches[i],\n                b\"%d\" % batch.aln_lens[i],\n                b\"%d\" % qualities[i]\n            ]\n\n            self._append_tags(parts, batch.scores[i], batch.cigars[i], batch._qualifiers[i])\n            lines.append(b\"\\t\".join(parts) + b\"\\n\")\n\n            if len(lines) &gt;= 1000:\n                self._handle.write(b\"\".join(lines))\n                lines = []\n\n        if lines:\n            self._handle.write(b\"\".join(lines))\n\n    def _append_tags(self, parts: list, score, cigar, qualifiers):\n        # Score\n        if score is not None:\n            if isinstance(score, (int, np.integer)) or score.is_integer():\n                parts.append(b\"AS:i:%d\" % int(score))\n            else:\n                parts.append(b\"AS:f:%.4f\" % score)\n\n        # Cigar\n        if cigar:\n            parts.append(b\"cg:Z:\" + cigar)\n\n        # Qualifiers\n        if qualifiers:\n            for k, v in qualifiers:\n                if k in (b'AS', b'cg'): continue\n\n                val_bytes = b\"\"\n                type_char = b\"Z\"\n\n                if isinstance(v, (int, np.integer)):\n                    type_char = b\"i\"\n                    val_bytes = b\"%d\" % v\n                elif isinstance(v, (float, np.floating)):\n                    type_char = b\"f\"\n                    val_bytes = b\"%.4f\" % v\n                elif isinstance(v, bytes):\n                    type_char = b\"Z\"\n                    val_bytes = v\n                elif isinstance(v, str):\n                    type_char = b\"Z\"\n                    val_bytes = v.encode('ascii')\n                else:\n                    continue\n\n                parts.append(k + b\":\" + type_char + b\":\" + val_bytes)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.PafWriter.write_batch","title":"<code>write_batch(batch)</code>","text":"<p>Writes an AlignmentBatch efficiently.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>AlignmentBatch</code> <p><code>AlignmentBatch</code> to write.</p> required Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_batch(self, batch: AlignmentBatch):\n    \"\"\"Writes an AlignmentBatch efficiently.\n\n    Args:\n       batch: ``AlignmentBatch`` to write.\n    \"\"\"\n    n = len(batch)\n    if n == 0: return\n\n    q_ids = batch.query.ids\n    t_ids = batch.target.ids\n    is_same_strand = (batch.q_strands == batch.t_strands)\n    qualities = batch._data['quality']\n\n    lines = []\n    for i in range(n):\n        q_name = q_ids[i]\n        if isinstance(q_name, str): q_name = q_name.encode('ascii')\n        elif isinstance(q_name, (int, np.integer)): q_name = str(q_name).encode('ascii')\n\n        t_name = t_ids[i]\n        if isinstance(t_name, str): t_name = t_name.encode('ascii')\n        elif isinstance(t_name, (int, np.integer)): t_name = str(t_name).encode('ascii')\n\n        parts = [\n            q_name,\n            b\"%d\" % batch.q_lens[i],\n            b\"%d\" % batch.q_starts[i],\n            b\"%d\" % batch.q_ends[i],\n            b'+' if is_same_strand[i] else b'-',\n            t_name,\n            b\"%d\" % batch.t_lens[i],\n            b\"%d\" % batch.t_starts[i],\n            b\"%d\" % batch.t_ends[i],\n            b\"%d\" % batch.matches[i],\n            b\"%d\" % batch.aln_lens[i],\n            b\"%d\" % qualities[i]\n        ]\n\n        self._append_tags(parts, batch.scores[i], batch.cigars[i], batch._qualifiers[i])\n        lines.append(b\"\\t\".join(parts) + b\"\\n\")\n\n        if len(lines) &gt;= 1000:\n            self._handle.write(b\"\".join(lines))\n            lines = []\n\n    if lines:\n        self._handle.write(b\"\".join(lines))\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.PafWriter.write_one","title":"<code>write_one(item)</code>","text":"<p>Writes a single Alignment or a Batch.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[Alignment, AlignmentBatch]</code> <p><code>Alignment</code> or <code>AlignmentBatch</code>.</p> required Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_one(self, item: Union[Alignment, AlignmentBatch]):\n    \"\"\"Writes a single Alignment or a Batch.\n\n    Args:\n        item: ``Alignment`` or ``AlignmentBatch``.\n    \"\"\"\n    if isinstance(item, AlignmentBatch):\n        self.write_batch(item)\n    elif isinstance(item, Alignment):\n        self._write_alignment(item)\n    else:\n        raise TypeError(f\"PafWriter expects Alignment or AlignmentBatch objects, got {type(item)}\")\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.TabularReader","title":"<code>TabularReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Base class for readers of tabular formats (GFF, BED, PAF).</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>class TabularReader(BaseReader):\n    \"\"\"Base class for readers of tabular formats (GFF, BED, PAF).\"\"\"\n    _delim = b'\\t'\n    _min_cols: int = 1\n    __slots__ = ('_handle',)\n\n    def _read_parts(self) -&gt; Generator[list[bytes], None, None]:\n        \"\"\"Internal generator that yields split lines.\"\"\"\n        delim = self._delim\n        min_cols = self._min_cols\n\n        buf = bytearray()\n        for chunk in self.read_chunks(self._CHUNK_SIZE):\n            if not chunk:\n                if buf:\n                    line = bytes(buf).rstrip()\n                    if line and not line.startswith(b'#'):\n                        parts = line.split(delim)\n                        if len(parts) &gt;= min_cols:\n                            yield parts\n                break\n\n            buf.extend(chunk)\n            pos = 0\n\n            while True:\n                nl_pos = buf.find(b'\\n', pos)\n                if nl_pos == -1:\n                    del buf[:pos]\n                    break\n\n                line = bytes(buf[pos:nl_pos]).rstrip()\n                pos = nl_pos + 1\n\n                if not line or line.startswith(b'#'): continue\n\n                parts = line.split(delim)\n                if len(parts) &lt; min_cols: continue\n                yield parts\n\n    def __iter__(self) -&gt; Generator:\n        \"\"\"\n        Iterates over lines, parsing valid rows.\n\n        Yields:\n            Parsed Feature or Alignment objects.\n        \"\"\"\n        parse = self.parse_row\n        for parts in self._read_parts(): yield parse(parts)\n\n    def batches(self, size: int = 1024):\n        \"\"\"Yields batches of parsed items.\n\n        Args:\n           size: Maximum items per batch.\n        \"\"\"\n        batch_parts = []\n        for parts in self._read_parts():\n            batch_parts.append(parts)\n            if len(batch_parts) &gt;= size:\n                yield self._make_batch_from_parts(batch_parts)\n                batch_parts = []\n        if batch_parts: yield self._make_batch_from_parts(batch_parts)\n\n    def _make_batch(self, items: list):\n        \"\"\"Creates a batch from a list of items.\"\"\"\n        return items\n\n    def _make_batch_from_parts(self, parts_list: list[list[bytes]]):\n        \"\"\"Creates a batch directly from parsed columns.\"\"\"\n        items = [self.parse_row(p) for p in parts_list]\n        return self._make_batch(items)\n\n    @abstractmethod\n    def parse_row(self, parts: list[bytes]) -&gt; Union[Feature, Alignment]:\n        \"\"\"\n        Parses a single row split by delimiter.\n\n        Args:\n            parts: List of column bytes.\n\n        Returns:\n            A Feature or Alignment object.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.TabularReader.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterates over lines, parsing valid rows.</p> <p>Yields:</p> Type Description <code>Generator</code> <p>Parsed Feature or Alignment objects.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def __iter__(self) -&gt; Generator:\n    \"\"\"\n    Iterates over lines, parsing valid rows.\n\n    Yields:\n        Parsed Feature or Alignment objects.\n    \"\"\"\n    parse = self.parse_row\n    for parts in self._read_parts(): yield parse(parts)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.TabularReader.batches","title":"<code>batches(size=1024)</code>","text":"<p>Yields batches of parsed items.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>int</code> <p>Maximum items per batch.</p> <code>1024</code> Source code in <code>baclib/io/tabular.py</code> <pre><code>def batches(self, size: int = 1024):\n    \"\"\"Yields batches of parsed items.\n\n    Args:\n       size: Maximum items per batch.\n    \"\"\"\n    batch_parts = []\n    for parts in self._read_parts():\n        batch_parts.append(parts)\n        if len(batch_parts) &gt;= size:\n            yield self._make_batch_from_parts(batch_parts)\n            batch_parts = []\n    if batch_parts: yield self._make_batch_from_parts(batch_parts)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.TabularReader.parse_row","title":"<code>parse_row(parts)</code>  <code>abstractmethod</code>","text":"<p>Parses a single row split by delimiter.</p> <p>Parameters:</p> Name Type Description Default <code>parts</code> <code>list[bytes]</code> <p>List of column bytes.</p> required <p>Returns:</p> Type Description <code>Union[Feature, Alignment]</code> <p>A Feature or Alignment object.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>@abstractmethod\ndef parse_row(self, parts: list[bytes]) -&gt; Union[Feature, Alignment]:\n    \"\"\"\n    Parses a single row split by delimiter.\n\n    Args:\n        parts: List of column bytes.\n\n    Returns:\n        A Feature or Alignment object.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.VcfReader","title":"<code>VcfReader</code>","text":"<p>               Bases: <code>TabularReader</code></p> <p>Reader for VCF (Variant Call Format) files.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.VCF, extensions=['.vcf'])\nclass VcfReader(TabularReader):\n    \"\"\"\n    Reader for VCF (Variant Call Format) files.\n    \"\"\"\n    _min_cols = 8\n\n    def parse_row(self, parts: list[bytes]) -&gt; Mutation:\n        \"\"\"\n        Parses a VCF row into a Mutation.\n        CHROM POS ID REF ALT QUAL FILTER INFO\n        \"\"\"\n        chrom = parts[0]\n        pos = int(parts[1]) - 1  # VCF is 1-based\n        ref_bytes = parts[3]\n        alt_bytes = parts[4]\n\n        # Handle multiple ALTs? For now assume single or take first/split?\n        # Specification says comma separated. Mutation expects single Seq.\n        # We will take the first ALT for now or yield multiple?\n        # BaseReader model yields one item per row usually.\n        # Let's support single ALT for now or split if needed.\n        # Ideally we should yield multiple Mutations if multiple ALTs.\n        # But parse_row returns ONE object.\n        # So we treat complex ALTs as the literal bytes for now.\n\n        # Interval\n        # Ref length determines end\n        ref_len = len(ref_bytes)\n        interval = Interval(pos, pos + ref_len, 1)\n\n        ref_seq = Seq(ref_bytes)\n        alt_seq = Seq(alt_bytes)\n\n        quals = []\n        # QUAL\n        if parts[5] != b'.':\n            quals.append((b'quality', float(parts[5])))\n\n        # FILTER\n        if parts[6] != b'.':\n            quals.append((b'filter', parts[6]))\n\n        # INFO\n        if parts[7] != b'.':\n            for chunk in parts[7].split(b';'):\n                if b'=' in chunk:\n                    k, v = chunk.split(b'=', 1)\n                    quals.append((k, v))\n                else:\n                    quals.append((chunk, True))\n\n        # CHROM -&gt; source? or just a qualifier?\n        # Mutation doesn't have 'seq_id' field, it relies on context or qualifiers.\n        quals.append((b'source', chrom))\n\n        return Mutation(interval, ref_seq, alt_seq, qualifiers=quals)\n\n    @classmethod\n    def sniff(cls, s: bytes) -&gt; bool:\n        \"\"\"Checks if input bytes look like VCF format.\"\"\"\n        return s.startswith(b'##fileformat=VCF')\n\n    def _make_batch(self, items: list) -&gt; MutationBatch:\n        return MutationBatch.build(items)\n\n    def _make_batch_from_parts(self, parts_list: list[list[bytes]]) -&gt; MutationBatch:\n        n = len(parts_list)\n        if n == 0: return MutationBatch.empty()\n\n        # Vectorized parsing\n        # CHROM(0) POS(1) ID(2) REF(3) ALT(4) QUAL(5) FILTER(6) INFO(7)\n\n        starts = np.array([int(p[1]) - 1 for p in parts_list], dtype=np.int32)\n\n        ref_seqs_list = [Seq(p[3]) for p in parts_list]\n        alt_seqs_list = [Seq(p[4]) for p in parts_list]\n\n        ref_lens = np.array([len(s) for s in ref_seqs_list], dtype=np.int32)\n        ends = starts + ref_lens\n        strands = np.ones(n, dtype=np.int32)\n\n        intervals = IntervalBatch(starts, ends, strands, sort=False)\n        ref_seqs = SeqBatch.build(ref_seqs_list)\n        alt_seqs = SeqBatch.build(alt_seqs_list)\n\n        # Qualifiers parsing\n        qualifiers_collection = []\n        for parts in parts_list:\n            row_quals = []\n            if parts[5] != b'.': row_quals.append((b'quality', float(parts[5])))\n            if parts[6] != b'.': row_quals.append((b'filter', parts[6]))\n            if parts[7] != b'.':\n                for chunk in parts[7].split(b';'):\n                    if b'=' in chunk:\n                        k, v = chunk.split(b'=', 1)\n                        row_quals.append((k, v))\n                    else:\n                        row_quals.append((chunk, True))\n            row_quals.append((b'source', parts[0]))\n            qualifiers_collection.append(row_quals)\n\n        qualifiers = QualifierBatch.build(qualifiers_collection)\n\n        return MutationBatch(intervals, ref_seqs, alt_seqs, qualifiers=qualifiers)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.VcfReader.parse_row","title":"<code>parse_row(parts)</code>","text":"<p>Parses a VCF row into a Mutation. CHROM POS ID REF ALT QUAL FILTER INFO</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def parse_row(self, parts: list[bytes]) -&gt; Mutation:\n    \"\"\"\n    Parses a VCF row into a Mutation.\n    CHROM POS ID REF ALT QUAL FILTER INFO\n    \"\"\"\n    chrom = parts[0]\n    pos = int(parts[1]) - 1  # VCF is 1-based\n    ref_bytes = parts[3]\n    alt_bytes = parts[4]\n\n    # Handle multiple ALTs? For now assume single or take first/split?\n    # Specification says comma separated. Mutation expects single Seq.\n    # We will take the first ALT for now or yield multiple?\n    # BaseReader model yields one item per row usually.\n    # Let's support single ALT for now or split if needed.\n    # Ideally we should yield multiple Mutations if multiple ALTs.\n    # But parse_row returns ONE object.\n    # So we treat complex ALTs as the literal bytes for now.\n\n    # Interval\n    # Ref length determines end\n    ref_len = len(ref_bytes)\n    interval = Interval(pos, pos + ref_len, 1)\n\n    ref_seq = Seq(ref_bytes)\n    alt_seq = Seq(alt_bytes)\n\n    quals = []\n    # QUAL\n    if parts[5] != b'.':\n        quals.append((b'quality', float(parts[5])))\n\n    # FILTER\n    if parts[6] != b'.':\n        quals.append((b'filter', parts[6]))\n\n    # INFO\n    if parts[7] != b'.':\n        for chunk in parts[7].split(b';'):\n            if b'=' in chunk:\n                k, v = chunk.split(b'=', 1)\n                quals.append((k, v))\n            else:\n                quals.append((chunk, True))\n\n    # CHROM -&gt; source? or just a qualifier?\n    # Mutation doesn't have 'seq_id' field, it relies on context or qualifiers.\n    quals.append((b'source', chrom))\n\n    return Mutation(interval, ref_seq, alt_seq, qualifiers=quals)\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.VcfReader.sniff","title":"<code>sniff(s)</code>  <code>classmethod</code>","text":"<p>Checks if input bytes look like VCF format.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>@classmethod\ndef sniff(cls, s: bytes) -&gt; bool:\n    \"\"\"Checks if input bytes look like VCF format.\"\"\"\n    return s.startswith(b'##fileformat=VCF')\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.VcfWriter","title":"<code>VcfWriter</code>","text":"<p>               Bases: <code>BaseWriter</code></p> <p>Writer for VCF files.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>@SeqFile.register(SeqFileFormat.VCF)\nclass VcfWriter(BaseWriter):\n    \"\"\"\n    Writer for VCF files.\n    \"\"\"\n    def write_header(self):\n        \"\"\"Writes a minimal VCF header.\"\"\"\n        self._handle.write(b\"##fileformat=VCFv4.2\\n\")\n        self._handle.write(b\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\n\")\n\n    def write_one(self, item: Union[Mutation, MutationBatch]):\n        \"\"\"Writes a Mutation or MutationBatch.\n\n        Args:\n            item: ``Mutation`` or ``MutationBatch``.\n        \"\"\"\n        if isinstance(item, MutationBatch):\n            self.write_batch(item)\n        elif isinstance(item, Mutation):\n            self._write_mutation(item)\n        else:\n            raise TypeError(f\"VcfWriter expects Mutation or MutationBatch, got {type(item)}\")\n\n    def _write_mutation(self, m: Mutation):\n        chrom = m.qualifiers.get(b'source', b'.')\n        pos = m.interval.start + 1\n        ref = m.ref_seq.bytes\n        alt = m.alt_seq.bytes\n\n        qual = m.qualifiers.get(b'quality', b'.')\n        if isinstance(qual, float): qual = b\"%.2f\" % qual\n\n        filt = m.qualifiers.get(b'filter', b'.')\n\n        # Reconstruct INFO\n        info_parts = []\n        for k, v in m.qualifiers:\n            if k in (b'source', b'quality', b'filter'): continue\n            if v is True: info_parts.append(k)\n            else:\n                val_str = v.decode('ascii') if isinstance(v, bytes) else str(v)\n                info_parts.append(k + b\"=\" + val_str.encode('ascii'))\n\n        info = b\";\".join(info_parts) if info_parts else b'.'\n\n        row = b\"\\t\".join([\n            chrom,\n            b\"%d\" % pos,\n            b\".\",\n            ref,\n            alt,\n            qual if isinstance(qual, bytes) else str(qual).encode('ascii'),\n            filt,\n            info\n        ]) + b\"\\n\"\n        self._handle.write(row)\n\n    def write_batch(self, batch: MutationBatch):\n        \"\"\"Writes a MutationBatch.\"\"\"\n        for i in range(len(batch)):\n            # TODO: Optimize batch writing\n            self._write_mutation(batch[i])\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.VcfWriter.write_batch","title":"<code>write_batch(batch)</code>","text":"<p>Writes a MutationBatch.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_batch(self, batch: MutationBatch):\n    \"\"\"Writes a MutationBatch.\"\"\"\n    for i in range(len(batch)):\n        # TODO: Optimize batch writing\n        self._write_mutation(batch[i])\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.VcfWriter.write_header","title":"<code>write_header()</code>","text":"<p>Writes a minimal VCF header.</p> Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_header(self):\n    \"\"\"Writes a minimal VCF header.\"\"\"\n    self._handle.write(b\"##fileformat=VCFv4.2\\n\")\n    self._handle.write(b\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\n\")\n</code></pre>"},{"location":"reference/baclib/io/tabular/#baclib.io.tabular.VcfWriter.write_one","title":"<code>write_one(item)</code>","text":"<p>Writes a Mutation or MutationBatch.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Union[Mutation, MutationBatch]</code> <p><code>Mutation</code> or <code>MutationBatch</code>.</p> required Source code in <code>baclib/io/tabular.py</code> <pre><code>def write_one(self, item: Union[Mutation, MutationBatch]):\n    \"\"\"Writes a Mutation or MutationBatch.\n\n    Args:\n        item: ``Mutation`` or ``MutationBatch``.\n    \"\"\"\n    if isinstance(item, MutationBatch):\n        self.write_batch(item)\n    elif isinstance(item, Mutation):\n        self._write_mutation(item)\n    else:\n        raise TypeError(f\"VcfWriter expects Mutation or MutationBatch, got {type(item)}\")\n</code></pre>"}]}